CURRENT PROJECT STATE:

--- START FILE: .\debugsetupissues.py ---
import os
import sys

print(f"Current Working Directory: {os.getcwd()}")
print("-" * 30)

# Check 1: Does 'modules' folder exist?
if os.path.isdir('modules'):
    print("[PASS] 'modules' folder found.")
else:
    print("[FAIL] 'modules' folder NOT found in this directory.")

# Check 2: Does '__init__.py' exist?
if os.path.exists('modules/__init__.py'):
    print("[PASS] 'modules/__init__.py' found.")
else:
    print("[FAIL] 'modules/__init__.py' is MISSING. Please create this empty file.")

# Check 3: Check for the specific file
if os.path.exists('modules/config_loader.py'):
    print("[PASS] 'modules/config_loader.py' found.")
else:
    print("[FAIL] 'modules/config_loader.py' is MISSING.")

# Check 4: Check for Libraries
try:
    import pandas
    print("[PASS] pandas library is installed.")
except ImportError:
    print("[FAIL] pandas is NOT installed. Run: pip install pandas")

print("-" * 30)
print("Attempting import now...")
try:
    from modules.config_loader import ConfigLoader
    print("[SUCCESS] Import worked!")
except ImportError as e:
    print(f"[CRITICAL FAILURE] Import failed. Reason: {e}")
--- END FILE: .\debugsetupissues.py ---

--- START FILE: .\gui.py ---
from modules.gui.app import M3MigrationApp

if __name__ == "__main__":
    app = M3MigrationApp()
    app.mainloop()
--- END FILE: .\gui.py ---

--- START FILE: .\main.py ---
import pandas as pd
import os
import sys
import time
import tkinter as tk
import glob
from tkinter import filedialog
from colorama import init, Fore, Style
import datetime

# Initialize colorama
init(autoreset=True)

# --- IMPORTS ---
try:
    import modules.ui as ui
    from modules.audit_manager import AuditManager
    from modules.mco_importer import MCOImporter
    from modules.rule_manager import RuleManager
    from modules.batch_processor import BatchProcessor
    from modules.auto_detector import AutoDetector
    from modules.surgical_extractor import SurgicalExtractor
    from modules.migration_runner import MigrationRunner
    from modules.mco_checker import MCOChecker
    from modules.sdt_utils import SDTUtils
except ImportError as e:
    print(f"{Fore.RED}CRITICAL ERROR: Could not import modules.{Style.RESET_ALL}")
    print(f"Details: {e}")
    sys.exit(1)

# --- ACTION CONTROLLERS ---

def action_manual_rule_entry():
    ui.print_header("Interactive Rule Editor")
    RuleManager().interactive_manual_entry()

def action_snapshot_manager():
    ui.print_header("Snapshot / Restore Manager")
    rule_dir = 'config/rules'
    files = glob.glob(os.path.join(rule_dir, "*.xlsx"))
    if not files: print(f"{Fore.RED}No rule files found.{Style.RESET_ALL}"); return
    
    names = [os.path.basename(f) for f in files]
    choice = ui.interactive_list_picker(names, "Select Rule File")
    if not choice: return
    
    program_name = choice.replace('.xlsx', '')
    auditor = AuditManager()
    
    print("\n1. Create Snapshot (Checkpoint)")
    print("2. Restore from Snapshot (Undo)")
    print("0. Cancel")
    opt = input("Select Action: ").strip()
    
    if opt == '1':
        note = input("Enter note: ").strip()
        auditor.create_snapshot(program_name, note if note else "Manual")
    elif opt == '2':
        snapshots = auditor.list_snapshots(program_name)
        if not snapshots: print("No snapshots found."); return
        restore_choice = ui.interactive_list_picker(snapshots, "Select Snapshot to Restore")
        if restore_choice and input(f"{Fore.RED}Confirm overwrite? (y/n): {Style.RESET_ALL}").lower() == 'y':
            auditor.restore_snapshot(program_name, restore_choice)

def action_import_mco_interactive():
    ui.print_header("Import MCO (Create Master Rule Set)")
    mco_path = ui.select_file("SELECT MCO SPECIFICATION EXCEL", [("Excel files", "*.xlsx *.xlsm")])
    if not mco_path: return
    MCOImporter().interactive_import(mco_path)

def action_commit_audit():
    ui.print_header("Commit External Excel Edits")
    rule_dir = 'config/rules'; ui.ensure_folder(rule_dir)
    files = glob.glob(os.path.join(rule_dir, "*.xlsx"))
    if not files: return
    names = [os.path.basename(f) for f in files]
    choice = ui.interactive_list_picker(names, "Select Rule File to Commit")
    if not choice: return
    try:
        auditor = AuditManager()
        auditor.commit_changes(choice.replace('.xlsx', ''))
        print(f"{Fore.GREEN}Done.{Style.RESET_ALL}")
    except Exception as e: print(f"{Fore.RED}Error: {e}{Style.RESET_ALL}")

def action_view_history():
    ui.print_header("View Rule History")
    rule_dir = 'config/rules'
    files = glob.glob(os.path.join(rule_dir, "*.xlsx"))
    if not files: print(f"{Fore.RED}No rule files found.{Style.RESET_ALL}"); return
    names = [os.path.basename(f) for f in files]
    choice = ui.interactive_list_picker(names, "Select Rule File")
    if not choice: return
    AuditManager().view_history(choice.replace('.xlsx', ''))
    input(f"\n{Fore.CYAN}Press Enter to return to menu...{Style.RESET_ALL}")

def action_migrate_context_aware():
    ui.print_header("Run Migration")
    rule_dir = 'config/rules'; files = glob.glob(os.path.join(rule_dir, "*.xlsx"))
    if not files: print(f"{Fore.RED}No rule files found.{Style.RESET_ALL}"); return
    names = [os.path.basename(f) for f in files]
    
    rule_choice = ui.interactive_list_picker(names, "STEP 1: Select Master Rule Configuration")
    if not rule_choice: return
    program_name = rule_choice.replace('.xlsx', '')

    legacy_path = ui.select_file(f"STEP 2: SELECT LEGACY SOURCE DATA", [("Excel files", "*.xlsx")])
    if not legacy_path: return
    
    if "MI" in os.path.basename(legacy_path).upper():
        if input(f"{Fore.RED}File looks like SDT. Sure? (y/n): {Style.RESET_ALL}").lower() != 'y': return

    division = input(f"\n{Fore.CYAN}>> Enter Scope (e.g. DIV_US, default GLOBAL): {Style.RESET_ALL}").strip().upper()
    if not division: division = 'GLOBAL'

    runner = MigrationRunner()
    runner.execute_migration(program_name, legacy_path, division=division, silent=False)

def action_auto_detect():
    ui.print_header("Auto-Detect File Load")
    print(f"{Fore.YELLOW}Note: We need the MCO file to learn signatures.{Style.RESET_ALL}")
    mco_path = ui.select_file("Select MCO Specification", [("Excel files", "*.xlsx *.xlsm")])
    if not mco_path: return

    detector = AutoDetector(mco_path)
    detector.learn_signatures()
    runner = MigrationRunner()

    while True:
        print(f"\n{Fore.CYAN}--- READY FOR NEXT FILE ---{Style.RESET_ALL}")
        legacy_path = ui.select_file("Select ANY Movex Data File", [("Excel files", "*.xlsx")])
        if not legacy_path: break

        prefix, mco_sheet, detected_api = detector.identify_file(legacy_path)
        if not prefix:
            print(f"{Fore.RED}Could not identify file signatures.{Style.RESET_ALL}")
            if input("Try another file? (y/n): ").lower() == 'y': continue
            else: break

        print(f"\n{Fore.GREEN}>>> DETECTION SUCCESSFUL!{Style.RESET_ALL}")
        print(f"    Prefix:      {prefix}")
        print(f"    MCO Sheet:   {mco_sheet}")
        
        map_api, map_sdt_file, _ = runner.resolve_from_map_public(mco_sheet, 'MCO_SHEET')
        final_api = map_api if map_api else detected_api
        final_sdt = None
        
        if map_sdt_file:
            full_sdt_path = os.path.join('config/sdt_templates', map_sdt_file)
            if os.path.exists(full_sdt_path): final_sdt = full_sdt_path

        print(f"    Target API:  {final_api}")
        if not final_api or final_api == "Unknown":
            final_api = input(f"{Fore.CYAN}>> Please confirm API Name: {Style.RESET_ALL}").strip().upper()
        
        rule_path = f"config/rules/{final_api}.xlsx"
        if not os.path.exists(rule_path):
            print(f"{Fore.YELLOW}Warning: Rule config {final_api}.xlsx not found. Skipping.{Style.RESET_ALL}")
            continue

        runner.execute_migration(final_api, legacy_path, auto_sdt=final_sdt, silent=True)
        print(f"\n{Fore.YELLOW}--------------------------------------------------{Style.RESET_ALL}")
        if input(f"{Fore.YELLOW}Process another Movex file? (y/n): {Style.RESET_ALL}").lower() != 'y': break

def action_load_by_id():
    ui.print_header("Load by ID (Delta Load)")
    extractor = SurgicalExtractor()
    objects = extractor.get_available_objects()
    if not objects:
        print(f"{Fore.RED}Configuration missing (surgical_def.csv).{Style.RESET_ALL}")
        return

    obj_type = ui.interactive_list_picker(objects, "Select Business Object")
    if not obj_type: return

    ids_str = input(f"\n{Fore.CYAN}>> Enter IDs (comma separated): {Style.RESET_ALL}").strip()
    if not ids_str: return
    id_list = [x.strip() for x in ids_str.split(',')]

    tasks = extractor.perform_extraction(obj_type, id_list)
    if not tasks:
        print(f"{Fore.YELLOW}No tasks generated. Check IDs or Source Files.{Style.RESET_ALL}"); return

    scope = input(f"\n{Fore.CYAN}>> Enter Scope (default GLOBAL): {Style.RESET_ALL}").strip().upper()
    if not scope: scope = 'GLOBAL'

    runner = MigrationRunner()
    print(f"\n{Fore.GREEN}Starting Execution of {len(tasks)} Surgical Tasks...{Style.RESET_ALL}")
    
    if tasks:
        base_prog = tasks[0]['program_name']
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        master_output_name = f"LOAD_{base_prog}_SURGICAL_{ts}.xlsx"
        print(f"Target Output File: {master_output_name}")
    else:
        master_output_name = None
    
    for i, task in enumerate(tasks):
        print(f"\n{Fore.YELLOW}>>> TASK {i+1}/{len(tasks)}: {task['program_name']} ({task['mco_sheet']}){Style.RESET_ALL}")
        
        _, _, trans_sheet_list = runner.resolve_from_map_public(task['mco_sheet'], 'MCO_SHEET')
        
        targets = None
        if isinstance(trans_sheet_list, list):
            targets = [x.strip() for x in trans_sheet_list if x]
        elif isinstance(trans_sheet_list, str):
            targets = [x.strip() for x in trans_sheet_list.split(',') if x.strip()]
        
        if targets:
            print(f"    Target Transaction(s): {targets}")
        else:
            print(f"{Fore.RED}    Warning: No TRANSACTION_SHEET defined for {task['mco_sheet']}. Skipping.{Style.RESET_ALL}")
            continue

        runner.execute_migration(
            task['program_name'], 
            task['legacy_path'], 
            division=scope, 
            target_sheets=targets, 
            silent=True, 
            output_name_override=master_output_name
        )

def action_batch_migration():
    ui.print_header("Run Batch Migration")
    batch_file = ui.select_file("Select Batch Job Definition Excel", [("Excel files", "*.xlsx")])
    if not batch_file: return
    processor = BatchProcessor()
    df_batch = processor.load_batch_file(batch_file)
    if df_batch is None: return
    print(f"\n{Fore.GREEN}Found {len(df_batch)} jobs.{Style.RESET_ALL}")
    print("1. Run All Enabled  2. Select Specific Jobs  0. Cancel")
    choice = input(f"\n{Fore.CYAN}Select Action: {Style.RESET_ALL}").strip()
    if choice == '2':
        options = [f"{row.get('JOB_ID', idx)} | {row.get('RULE_CONFIG')}" for idx, row in df_batch.iterrows()]
        selected_strings = ui.interactive_list_picker(options, "Select Jobs", multi=True)
        if selected_strings:
            indices = [i for i, opt in enumerate(options) if opt in selected_strings]
            processor.run_batch_execution(df_batch.iloc[indices], force_run=True)
    elif choice == '1':
        processor.run_batch_execution(df_batch, force_run=False)

def action_check_mco():
    ui.print_header("MCO Health Checker")
    mco_path = ui.select_file("Select MCO File to Validate", [("Excel files", "*.xlsx *.xlsm")])
    if not mco_path: return
    
    checker = MCOChecker()
    checker.check_file(mco_path)
    input(f"\n{Fore.CYAN}Press Enter to return to menu...{Style.RESET_ALL}")

def action_utilities():
    ui.print_header("Utilities")
    print("1. Copy SDT Sheet Data")
    print("2. Merge SDT Files") # <--- NEW
    print("0. Back")
    
    choice = input(f"\n{Fore.CYAN}>> Selection: {Style.RESET_ALL}").strip()
    
    if choice == '1':
        SDTUtils().copy_sdt_sheet_interactive()
    elif choice == '2':
        SDTUtils().merge_sdt_interactive()

def action_maintenance():
    ui.print_header("System Maintenance")
    print(f"{Fore.RED}1. Hard Reset History{Style.RESET_ALL}"); print("0. Cancel")
    if input(f"\n{Fore.CYAN}>> Selection: {Style.RESET_ALL}").strip() == '1':
        if input(f"{Fore.RED}Type 'DELETE': {Style.RESET_ALL}") == 'DELETE':
            AuditManager().hard_reset(); print(f"\n{Fore.GREEN}Reset Complete.{Style.RESET_ALL}")

# --- MAIN LOOP ---
def main_menu():
    while True:
        print("\n")
        print(f"{Fore.BLUE}=== M3 DATA MIGRATION PLATFORM ==={Style.RESET_ALL}")
        print("1. Interactive Rule Editor")
        print("2. Commit External Excel Edits")
        print("3. Import MCO (Master Init)")
        print("4. Run Migration (Interactive)")
        print("5. Run Batch Migration (Automated)")
        print("6. Auto-detect File Load")
        print("7. Load by ID (Delta Load)")
        print("8. Snapshot / Restore Manager")
        print("9. View Rule History")
        print("10. Check MCO Health")
        print("11. Utilities")
        print("12. System Maintenance")
        print("0. Exit")
        
        choice = input(f"\n{Fore.CYAN}Select an option (0-12): {Style.RESET_ALL}")
        
        options = {
            '1': action_manual_rule_entry,
            '2': action_commit_audit,
            '3': action_import_mco_interactive,
            '4': action_migrate_context_aware,
            '5': action_batch_migration,
            '6': action_auto_detect,
            '7': action_load_by_id,
            '8': action_snapshot_manager,
            '9': action_view_history,
            '10': action_check_mco,
            '11': action_utilities,
            '12': action_maintenance
        }
        
        if choice in options: options[choice]()
        elif choice in ['0', 'q']: sys.exit(0)
        else: print(f"{Fore.RED}Invalid option.{Style.RESET_ALL}")

if __name__ == "__main__":
    root = tk.Tk(); root.withdraw()
    try: main_menu()
    except KeyboardInterrupt: print("\nOperation cancelled."); sys.exit(0)
--- END FILE: .\main.py ---

--- START FILE: .\pack_project.py ---
import os

# Files to ignore
IGNORE = ['__pycache__', '.git', '.history', '.snapshots', 'output', 'raw_data', 'venv']

with open("FULL_PROJECT_CONTEXT.txt", "w", encoding="utf-8") as outfile:
    outfile.write("CURRENT PROJECT STATE:\n\n")
    
    # Walk through all files
    for root, dirs, files in os.walk("."):
        # Filter ignored directories
        dirs[:] = [d for d in dirs if d not in IGNORE]
        
        for file in files:
            if file.endswith(".py") or file.endswith(".csv"):
                path = os.path.join(root, file)
                outfile.write(f"--- START FILE: {path} ---\n")
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        outfile.write(f.read())
                except: outfile.write("[Error reading file]")
                outfile.write(f"\n--- END FILE: {path} ---\n\n")

print("Done. Upload 'FULL_PROJECT_CONTEXT.txt' to the AI chat.")
--- END FILE: .\pack_project.py ---

--- START FILE: .\backup\debugsetupissues.py ---
import os
import sys

print(f"Current Working Directory: {os.getcwd()}")
print("-" * 30)

# Check 1: Does 'modules' folder exist?
if os.path.isdir('modules'):
    print("[PASS] 'modules' folder found.")
else:
    print("[FAIL] 'modules' folder NOT found in this directory.")

# Check 2: Does '__init__.py' exist?
if os.path.exists('modules/__init__.py'):
    print("[PASS] 'modules/__init__.py' found.")
else:
    print("[FAIL] 'modules/__init__.py' is MISSING. Please create this empty file.")

# Check 3: Check for the specific file
if os.path.exists('modules/config_loader.py'):
    print("[PASS] 'modules/config_loader.py' found.")
else:
    print("[FAIL] 'modules/config_loader.py' is MISSING.")

# Check 4: Check for Libraries
try:
    import pandas
    print("[PASS] pandas library is installed.")
except ImportError:
    print("[FAIL] pandas is NOT installed. Run: pip install pandas")

print("-" * 30)
print("Attempting import now...")
try:
    from modules.config_loader import ConfigLoader
    print("[SUCCESS] Import worked!")
except ImportError as e:
    print(f"[CRITICAL FAILURE] Import failed. Reason: {e}")
--- END FILE: .\backup\debugsetupissues.py ---

--- START FILE: .\backup\main.py ---
import pandas as pd
import os
import sys
import time
import tkinter as tk
import glob
from tkinter import filedialog
from colorama import init, Fore, Style

# Initialize colorama
init(autoreset=True)

# --- IMPORTS ---
try:
    from modules.config_loader import ConfigLoader
    from modules.extractor import DataExtractor
    from modules.sdt_writer import SDTWriter
    from modules.validator_analyzer import ValidatorAnalyzer
    from modules.rule_manager import RuleManager
    from modules.rule_promoter import RulePromoter
    from modules.audit_manager import AuditManager
    from modules.mco_importer import MCOImporter
    from modules.transform_engine import TransformEngine
    from modules.batch_processor import BatchProcessor
    # NEW MODULE
    from modules.auto_detector import AutoDetector
except ImportError as e:
    print(f"{Fore.RED}CRITICAL ERROR: Could not import modules.{Style.RESET_ALL}")
    print(f"Details: {e}")
    sys.exit(1)

# --- UI HELPERS ---
def interactive_list_picker(options, title_prompt, multi=False):
    filtered_indices = list(range(len(options)))
    filter_text = ""
    while True:
        print(f"\n{Fore.CYAN}--- {title_prompt} ---{Style.RESET_ALL}")
        if filter_text: print(f"{Fore.YELLOW}[Filter: '{filter_text}'] (Type 'all' to clear){Style.RESET_ALL}")
        
        limit = 20
        count = 0
        display_map = {}
        for i in filtered_indices:
            count += 1
            if count > limit:
                print(f"   ... ({len(filtered_indices) - limit} more matches)")
                break
            print(f"   {count}. {options[i]}")
            display_map[count] = i
        
        if multi: print(f"\n{Fore.GREEN}Type numbers separated by commas (e.g. 1, 3) or Filter Text.{Style.RESET_ALL}")
        else: print(f"\n{Fore.GREEN}Type a Number to select, or Text to filter.{Style.RESET_ALL}")
            
        user_input = input(f"{Fore.CYAN}>> Selection: {Style.RESET_ALL}").strip()
        if not user_input: continue
        
        if multi and ',' in user_input:
            try:
                parts = [int(x.strip()) for x in user_input.split(',')]
                selected = [options[display_map[p]] for p in parts if p in display_map]
                if selected: return selected
            except ValueError: pass

        if user_input.isdigit():
            choice = int(user_input)
            if choice in display_map:
                res = options[display_map[choice]]
                return [res] if multi else res
        
        elif user_input.lower() in ['all', 'clear']:
            filter_text = ""; filtered_indices = list(range(len(options)))
        else:
            filter_text = user_input
            filtered_indices = [i for i, opt in enumerate(options) if filter_text.lower() in str(opt).lower()]
            if not filtered_indices: print("No matches.")

def select_file(prompt, filetypes):
    print(f"\n{Fore.GREEN}--> ACTION REQUIRED: {prompt}{Style.RESET_ALL}")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    f = filedialog.askopenfilename(title=prompt, filetypes=filetypes)
    root.destroy()
    return f

def select_folder(prompt):
    print(f"\n{Fore.GREEN}--> ACTION REQUIRED: {prompt}{Style.RESET_ALL}")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    f = filedialog.askdirectory(title=prompt)
    root.destroy()
    return f

def ensure_folder(path):
    if not os.path.exists(path): os.makedirs(path)

def print_header(title):
    print("\n" + "="*60); print(f"   {title.upper()}"); print("="*60)

# --- ACTIONS ---

def action_manual_rule_entry():
    print_header("Interactive Rule Editor")
    manager = RuleManager()
    manager.interactive_manual_entry()

def action_snapshot_manager():
    print_header("Snapshot / Restore Manager")
    rule_dir = 'config/rules'
    files = glob.glob(os.path.join(rule_dir, "*.xlsx"))
    if not files: print(f"{Fore.RED}No rule files found.{Style.RESET_ALL}"); return
    names = [os.path.basename(f) for f in files]
    choice = interactive_list_picker(names, "Select Rule File")
    if not choice: return
    program_name = choice.replace('.xlsx', '')
    auditor = AuditManager()
    print("\n1. Create Snapshot (Checkpoint)")
    print("2. Restore from Snapshot (Undo)")
    print("0. Cancel")
    opt = input("Select Action: ").strip()
    if opt == '1':
        note = input("Enter note: ").strip()
        auditor.create_snapshot(program_name, note if note else "Manual")
    elif opt == '2':
        snapshots = auditor.list_snapshots(program_name)
        if not snapshots: print("No snapshots found."); return
        restore_choice = interactive_list_picker(snapshots, "Select Snapshot to Restore")
        if restore_choice and input(f"{Fore.RED}Confirm overwrite? (y/n): {Style.RESET_ALL}").lower() == 'y':
            auditor.restore_snapshot(program_name, restore_choice)

def action_import_mco_interactive():
    print_header("Import MCO (Create Master Rule Set)")
    mco_path = select_file("SELECT MCO SPECIFICATION EXCEL", [("Excel files", "*.xlsx *.xlsm")])
    if not mco_path: return
    importer = MCOImporter()
    importer.interactive_import(mco_path)

def action_commit_audit():
    print_header("Commit External Excel Edits")
    rule_dir = 'config/rules'; ensure_folder(rule_dir)
    files = glob.glob(os.path.join(rule_dir, "*.xlsx"))
    if not files: return
    names = [os.path.basename(f) for f in files]
    choice = interactive_list_picker(names, "Select Rule File to Commit")
    if not choice: return
    try:
        auditor = AuditManager()
        auditor.commit_changes(choice.replace('.xlsx', ''))
        print(f"{Fore.GREEN}Done.{Style.RESET_ALL}")
    except Exception as e: print(f"{Fore.RED}Error: {e}{Style.RESET_ALL}")

def action_view_history():
    print_header("View Rule History")
    rule_dir = 'config/rules'
    files = glob.glob(os.path.join(rule_dir, "*.xlsx"))
    if not files: print(f"{Fore.RED}No rule files found.{Style.RESET_ALL}"); return
    names = [os.path.basename(f) for f in files]
    choice = interactive_list_picker(names, "Select Rule File")
    if not choice: return
    auditor = AuditManager()
    auditor.view_history(choice.replace('.xlsx', ''))
    input(f"\n{Fore.CYAN}Press Enter to return to menu...{Style.RESET_ALL}")

def action_migrate_context_aware():
    print_header("Run Migration (Interactive)")
    rule_dir = 'config/rules'; files = glob.glob(os.path.join(rule_dir, "*.xlsx"))
    if not files: print(f"{Fore.RED}No rule files found.{Style.RESET_ALL}"); return
    names = [os.path.basename(f) for f in files]
    rule_choice = interactive_list_picker(names, "STEP 1: Select Master Rule Configuration")
    if not rule_choice: return
    program_name = rule_choice.replace('.xlsx', '')

    legacy_path = select_file(f"STEP 2: SELECT LEGACY SOURCE DATA", [("Excel files", "*.xlsx")])
    if not legacy_path: return
    
    _execute_migration_logic(program_name, legacy_path)

def action_magic_drop():
    print_header("Magic Movex Drop (Auto-Detect)")
    
    # 1. Ask for MCO (Only once per session ideally, but for now we ask)
    # Improvement: Store MCO path in config file so we don't ask every time
    print(f"{Fore.YELLOW}Note: We need the MCO file to learn the signatures.{Style.RESET_ALL}")
    mco_path = select_file("Select MCO Specification", [("Excel files", "*.xlsx *.xlsm")])
    if not mco_path: return

    # 2. Select Source File
    legacy_path = select_file("Select ANY Movex Data File", [("Excel files", "*.xlsx")])
    if not legacy_path: return

    # 3. Detect
    detector = AutoDetector(mco_path)
    prefix, mco_sheet, api_name = detector.identify_file(legacy_path)

    if not prefix:
        print(f"{Fore.RED}Could not identify file. Headers do not match known MCO prefixes.{Style.RESET_ALL}")
        return

    print(f"\n{Fore.GREEN}>>> DETECTION SUCCESSFUL!{Style.RESET_ALL}")
    print(f"    Prefix:      {prefix}")
    print(f"    MCO Sheet:   {mco_sheet}")
    print(f"    Inferred API: {api_name}")

    # 4. Confirmation / Override
    if api_name == "Unknown" or not api_name:
        api_name = input(f"{Fore.CYAN}>> Please confirm API Name (e.g. MMS200MI): {Style.RESET_ALL}").strip().upper()
    
    # Check if Rule Config exists
    rule_path = f"config/rules/{api_name}.xlsx"
    if not os.path.exists(rule_path):
        print(f"{Fore.YELLOW}Warning: Rule config {api_name}.xlsx not found.{Style.RESET_ALL}")
        print("You may need to run 'Import MCO' for this sheet first.")
        if input("Continue anyway? (y/n): ").lower() != 'y': return

    # 5. Hand off to Migration Logic
    _execute_migration_logic(api_name, legacy_path)

def _execute_migration_logic(program_name, legacy_path):
    """
    Shared logic for Manual Migration and Magic Drop.
    """
    clean_name = program_name.replace('API_', '')
    potential_sdt = os.path.join('config/sdt_templates', f"{clean_name}_API.xlsx")
    sdt_path = None
    if os.path.exists(potential_sdt):
        print(f"\n{Fore.GREEN}Found matching SDT Template: {potential_sdt}{Style.RESET_ALL}")
        # Auto-accept in magic mode? Let's stick to confirmation for safety
        if input("Use this template? (y/n): ").lower() == 'y': sdt_path = potential_sdt

    if not sdt_path:
        sdt_path = select_file("STEP 3: SELECT M3 SDT TEMPLATE", [("Excel files", "*.xlsx")])
        if not sdt_path: return
    
    xls = pd.ExcelFile(sdt_path, engine='openpyxl')
    sheets = [s for s in xls.sheet_names if "MI" in s and "LST" not in s.upper() and "GET" not in s.upper()]
    target_sheets = interactive_list_picker(sheets, "STEP 4: Select Transactions", multi=True)
    if not target_sheets: return

    division = input(f"\n{Fore.CYAN}>> Enter Scope (e.g. DIV_US, default GLOBAL): {Style.RESET_ALL}").strip().upper()
    if not division: division = 'GLOBAL'

    try:
        print(f"\n{Fore.CYAN}--- STARTING MIGRATION ---{Style.RESET_ALL}")
        config_loader = ConfigLoader(program_name)
        rules, lookups = config_loader.load_config(division_code=division)
        
        extractor = DataExtractor()
        df_legacy = extractor.load_data(legacy_path, format_type='MOVEX', sheet_name=0)

        output_dir = "output"; ensure_folder(output_dir)
        writer = SDTWriter(output_dir)
        out_name = f"LOAD_{program_name}.xlsx"
        writer.generate_from_template(sdt_path, df_legacy, rules, target_sheets, out_name)
        
    except Exception as e:
        print(f"{Fore.RED}FATAL ERROR: {e}{Style.RESET_ALL}")
        import traceback
        traceback.print_exc()

def action_batch_migration():
    print_header("Run Batch Migration")
    batch_file = select_file("Select Batch Job Definition Excel", [("Excel files", "*.xlsx")])
    if not batch_file: return
    processor = BatchProcessor()
    df_batch = processor.load_batch_file(batch_file)
    if df_batch is None: return
    print(f"\n{Fore.GREEN}Found {len(df_batch)} jobs.{Style.RESET_ALL}")
    print("1. Run All Enabled")
    print("2. Select Specific Jobs")
    print("0. Cancel")
    choice = input(f"\n{Fore.CYAN}Select Action: {Style.RESET_ALL}").strip()
    if choice == '2':
        options = []
        for idx, row in df_batch.iterrows():
            options.append(f"{row.get('JOB_ID', idx)} | {row.get('RULE_CONFIG')}")
        selected_strings = interactive_list_picker(options, "Select Jobs", multi=True)
        if selected_strings:
            indices = [i for i, opt in enumerate(options) if opt in selected_strings]
            processor.run_batch_execution(df_batch.iloc[indices], force_run=True)
    elif choice == '1':
        processor.run_batch_execution(df_batch, force_run=False)

def action_maintenance():
    print_header("System Maintenance")
    print(f"{Fore.RED}1. Hard Reset History{Style.RESET_ALL}"); print("0. Cancel")
    if input(f"\n{Fore.CYAN}>> Selection: {Style.RESET_ALL}").strip() == '1':
        if input(f"{Fore.RED}Type 'DELETE': {Style.RESET_ALL}") == 'DELETE':
            AuditManager().hard_reset(); print(f"\n{Fore.GREEN}Reset Complete.{Style.RESET_ALL}")

# --- MAIN LOOP ---
def main_menu():
    while True:
        print("\n")
        print(f"{Fore.BLUE}=== M3 DATA MIGRATION PLATFORM ==={Style.RESET_ALL}")
        print("1. Interactive Rule Editor")
        print("2. Commit External Excel Edits")
        print("3. Import MCO (Master Init)")
        print("4. Run Migration (Interactive)")
        print("5. Run Batch Migration (Automated)")
        print("6. Magic Movex Drop (Auto-Detect)")
        print("7. Snapshot / Restore Manager")
        print("8. View Rule History")
        print("9. System Maintenance")
        print("0. Exit")
        
        choice = input(f"\n{Fore.CYAN}Select an option (0-9): {Style.RESET_ALL}")
        
        if choice == '1': action_manual_rule_entry()
        elif choice == '2': action_commit_audit()
        elif choice == '3': action_import_mco_interactive()
        elif choice == '4': action_migrate_context_aware()
        elif choice == '5': action_batch_migration()
        elif choice == '6': action_magic_drop()
        elif choice == '7': action_snapshot_manager()
        elif choice == '8': action_view_history()
        elif choice == '9': action_maintenance()
        elif choice == '0': sys.exit(0)
        else: print(f"{Fore.RED}Invalid option.{Style.RESET_ALL}")

if __name__ == "__main__":
    root = tk.Tk(); root.withdraw()
    try: main_menu()
    except KeyboardInterrupt: print("\nOperation cancelled."); sys.exit(0)
--- END FILE: .\backup\main.py ---

--- START FILE: .\backup\modules\audit_manager.py ---
import pandas as pd
import os
import getpass
import datetime
import shutil
import glob
from colorama import Fore, Style

class AuditManager:
    def __init__(self, rule_dir='config/rules'):
        self.rule_dir = rule_dir
        self.history_dir = f"{rule_dir}/.history"
        self.snapshot_dir = f"{rule_dir}/.snapshots"
        
        for d in [self.history_dir, self.snapshot_dir]:
            if not os.path.exists(d): os.makedirs(d)

    def create_snapshot(self, program_name, note="Manual"):
        src = f"{self.rule_dir}/{program_name}.xlsx"
        if not os.path.exists(src): return
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{program_name}_{timestamp}_{note}.xlsx"
        dst = f"{self.snapshot_dir}/{filename}"
        shutil.copy2(src, dst)
        print(f"{Fore.GREEN}   -> Snapshot created: {filename}{Style.RESET_ALL}")

    def list_snapshots(self, program_name):
        pattern = f"{self.snapshot_dir}/{program_name}_*.xlsx"
        files = glob.glob(pattern)
        files.sort(key=os.path.getmtime, reverse=True)
        return [os.path.basename(f) for f in files]

    def restore_snapshot(self, program_name, snapshot_filename):
        src = f"{self.snapshot_dir}/{snapshot_filename}"
        dst = f"{self.rule_dir}/{program_name}.xlsx"
        if not os.path.exists(src): return
        
        self.create_snapshot(program_name, "PRE_RESTORE_BACKUP")
        shutil.copy2(src, dst)
        print(f"{Fore.GREEN}   -> Restored {program_name} from {snapshot_filename}{Style.RESET_ALL}")

    def view_history(self, program_name):
        path = f"{self.rule_dir}/{program_name}.xlsx"
        if not os.path.exists(path):
            print(f"{Fore.RED}Rule file not found.{Style.RESET_ALL}"); return

        try:
            df = pd.read_excel(path, sheet_name='_Audit_Log')
            if df.empty:
                print(f"{Fore.YELLOW}No history found.{Style.RESET_ALL}"); return

            print(f"\n{Fore.CYAN}--- RULE HISTORY: {program_name} ---{Style.RESET_ALL}")
            print(f"{'TIMESTAMP':<20} | {'USER':<10} | {'ACTION':<6} | {'TARGET':<10} | {'DETAILS'}")
            print("-" * 90)

            for _, row in df.iterrows():
                ts = str(row.get('TIMESTAMP', ''))[:19]
                user = str(row.get('USER', ''))[:10]
                action = str(row.get('ACTION', ''))[:6]
                target = str(row.get('TARGET_FIELD', ''))[:10]
                
                old_v = str(row.get('OLD_VALUE', '')).strip()
                new_v = str(row.get('NEW_VALUE', '')).strip()
                
                if action == 'EDIT': details = f"{old_v} -> {new_v}"
                elif action == 'ADD': details = "New Rule"
                else: details = new_v

                print(f"{ts:<20} | {user:<10} | {action:<6} | {target:<10} | {details[:40]}")
            print("-" * 90)
        except Exception as e:
            print(f"{Fore.RED}Error reading history: {e}{Style.RESET_ALL}")

    def commit_changes(self, program_name):
        self.create_snapshot(program_name, "Auto_Commit")
        current_path = f"{self.rule_dir}/{program_name}.xlsx"
        history_path = f"{self.history_dir}/{program_name}.xlsx"
        if not os.path.exists(current_path): return

        print(f"Scanning for changes in {program_name}...")
        try: df_curr = pd.read_excel(current_path, sheet_name='Rules').fillna('').astype(str)
        except: return

        # Deduplicate to prevent false flags if MCO had dupes
        df_curr = df_curr.drop_duplicates(subset=['TARGET_FIELD'], keep='last')

        changes = []
        if os.path.exists(history_path):
            df_prev = pd.read_excel(history_path, sheet_name='Rules').fillna('').astype(str)
            df_prev = df_prev.drop_duplicates(subset=['TARGET_FIELD'], keep='last')
            
            merged = pd.merge(df_curr, df_prev, on='TARGET_FIELD', how='outer', suffixes=('_NEW', '_OLD'), indicator=True)
            user = getpass.getuser()
            ts = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            for idx, row in merged.iterrows():
                target = row['TARGET_FIELD']
                if row['_merge'] == 'left_only': changes.append([ts, user, 'ADD', target, '', 'Created'])
                elif row['_merge'] == 'right_only': changes.append([ts, user, 'DELETE', target, 'Existed', 'Deleted'])
                elif row['_merge'] == 'both':
                    for col in ['SOURCE_FIELD', 'RULE_TYPE', 'RULE_VALUE', 'SCOPE']:
                        c_new, c_old = f"{col}_NEW", f"{col}_OLD"
                        if row[c_new] != row[c_old]: changes.append([ts, user, 'EDIT', target, row[c_old], row[c_new]])
        else:
            changes.append([datetime.datetime.now(), getpass.getuser(), 'INIT', 'ALL', '', 'Initial Commit'])

        if changes:
            print(f"   -> Detected {len(changes)} changes.")
            with pd.ExcelWriter(current_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:
                try: df_old = pd.read_excel(current_path, sheet_name='_Audit_Log')
                except: df_old = pd.DataFrame(columns=['TIMESTAMP', 'USER', 'ACTION', 'TARGET_FIELD', 'OLD_VALUE', 'NEW_VALUE'])
                
                df_new = pd.DataFrame(changes, columns=['TIMESTAMP', 'USER', 'ACTION', 'TARGET_FIELD', 'OLD_VALUE', 'NEW_VALUE'])
                if not df_new.empty:
                    pd.concat([df_old, df_new], ignore_index=True).to_excel(writer, sheet_name='_Audit_Log', index=False)
        else:
            print("   -> No changes detected.")
        shutil.copy2(current_path, history_path)

    def hard_reset(self):
        """
        DANGER: Deletes all snapshots, history, and clears Audit Logs from Excel files.
        """
        print(f"{Fore.YELLOW}   Cleaning .history and .snapshots folders...{Style.RESET_ALL}")
        for folder in [self.history_dir, self.snapshot_dir]:
            files = glob.glob(os.path.join(folder, "*"))
            for f in files:
                try: os.remove(f)
                except: pass
        
        print(f"{Fore.YELLOW}   Resetting Audit Log sheets in Excel rules...{Style.RESET_ALL}")
        rule_files = glob.glob(os.path.join(self.rule_dir, "*.xlsx"))
        
        empty_log = pd.DataFrame(columns=['TIMESTAMP', 'USER', 'ACTION', 'TARGET_FIELD', 'OLD_VALUE', 'NEW_VALUE'])
        
        for f in rule_files:
            try:
                # Use openpyxl replace mode to overwrite just the specific sheet
                with pd.ExcelWriter(f, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
                    empty_log.to_excel(writer, sheet_name='_Audit_Log', index=False)
                print(f"   -> Cleaned: {os.path.basename(f)}")
            except Exception as e:
                print(f"{Fore.RED}   -> Failed to clean {os.path.basename(f)}: {e}{Style.RESET_ALL}")
--- END FILE: .\backup\modules\audit_manager.py ---

--- START FILE: .\backup\modules\auto_detector.py ---
import pandas as pd
import re
import os
from colorama import Fore, Style

class AutoDetector:
    def __init__(self, mco_path):
        self.mco_path = mco_path
        self.prefix_map = {} # {'MM': {'sheet': 'Item Master', 'api': 'MMS200MI'}}
        self.is_learned = False

    def learn_signatures(self):
        """
        Scans the MCO file to map 2-char Prefixes -> MCO Sheets -> API Names.
        """
        print(f"{Fore.CYAN}   Analyzing MCO for Field Signatures...{Style.RESET_ALL}")
        
        try:
            xls = pd.ExcelFile(self.mco_path)
            
            for sheet in xls.sheet_names:
                # 1. READ HEADER (Try Row 3 first, then scan)
                try:
                    df = pd.read_excel(xls, sheet_name=sheet, header=2, nrows=5)
                except:
                    continue

                # 2. VALIDATE IF MCO SHEET
                # Must have 'Field name' (or M3 Field) AND 'Data Converison Source' (Legacy)
                cols_upper = [str(c).upper() for c in df.columns]
                
                has_target = any('FIELD NAME' in c or 'M3 FIELD' in c for c in cols_upper)
                # Handle the specific typo in your MCO ("Converison")
                has_source = any('SOURCE' in c for c in cols_upper)
                
                if not (has_target and has_source):
                    continue

                # 3. EXTRACT PREFIX
                # Find the source column index
                src_col_idx = next((i for i, c in enumerate(cols_upper) if 'SOURCE' in c), None)
                if src_col_idx is None: continue
                
                # Read full column to find a non-empty value
                df_full = pd.read_excel(xls, sheet_name=sheet, header=2, usecols=[src_col_idx])
                sample_values = df_full.iloc[:, 0].dropna().astype(str).tolist()
                
                prefix = None
                for val in sample_values:
                    clean_val = val.strip().upper()
                    # Movex fields are typically 6 chars (MMITNO). We take first 2.
                    if len(clean_val) >= 2 and clean_val.isalpha():
                        prefix = clean_val[:2]
                        break
                
                if not prefix: continue

                # 4. GUESS API NAME
                # Look for 'API' column or scan first few rows for pattern "MMS200MI"
                api_name = "Unknown"
                
                # Method A: Check Column Header
                api_col_idx = next((i for i, c in enumerate(cols_upper) if 'API' in c), None)
                if api_col_idx is not None:
                    df_api = pd.read_excel(xls, sheet_name=sheet, header=2, usecols=[api_col_idx], nrows=10)
                    for val in df_api.iloc[:, 0].dropna().astype(str):
                        # Regex for XXX###MI (e.g. MMS200MI)
                        match = re.search(r'([A-Z]{3}\d{3}MI)', val, re.IGNORECASE)
                        if match:
                            api_name = match.group(1).upper()
                            break
                
                # Store Logic
                self.prefix_map[prefix] = {
                    'sheet': sheet,
                    'api': api_name
                }
                # print(f"      [LEARNED] {prefix} -> {sheet} ({api_name})")

            self.is_learned = True
            print(f"{Fore.GREEN}   -> Learned signatures for {len(self.prefix_map)} Business Objects.{Style.RESET_ALL}")

        except Exception as e:
            print(f"{Fore.RED}Error learning MCO signatures: {e}{Style.RESET_ALL}")

    def identify_file(self, file_path):
        """
        Reads a Movex Excel file and returns the matching MCO info.
        Returns: (prefix, mco_sheet, api_name) or None
        """
        if not self.is_learned:
            self.learn_signatures()

        try:
            # Read first row to get headers
            df = pd.read_excel(file_path, nrows=5)
            
            # Find the header row (using Extractor logic essentially)
            header_row = None
            for idx, row in df.iterrows():
                # Look for common fields
                row_str = " ".join([str(x).upper() for x in row.values])
                if "ITNO" in row_str or "CUNO" in row_str or "SUNO" in row_str:
                    header_row = row
                    break
            
            if header_row is None:
                # Fallback to row 0
                header_row = df.iloc[0]

            # Analyze Headers for Known Prefixes
            for col in header_row.values:
                val = str(col).strip().upper()
                if len(val) >= 2:
                    p = val[:2]
                    if p in self.prefix_map:
                        info = self.prefix_map[p]
                        return p, info['sheet'], info['api']
            
            return None, None, None

        except Exception as e:
            print(f"{Fore.RED}Error analyzing file: {e}{Style.RESET_ALL}")
            return None, None, None
--- END FILE: .\backup\modules\auto_detector.py ---

--- START FILE: .\backup\modules\batch_processor.py ---
import pandas as pd
import os
import datetime
from colorama import Fore, Style
from modules.config_loader import ConfigLoader
from modules.extractor import DataExtractor
from modules.sdt_writer import SDTWriter

class BatchProcessor:
    def __init__(self):
        self.output_dir = 'output'
        if not os.path.exists(self.output_dir): os.makedirs(self.output_dir)

    def load_batch_file(self, batch_file_path):
        """
        Reads the batch Excel file and returns a clean DataFrame.
        """
        try:
            df_batch = pd.read_excel(batch_file_path)
            # Clean headers
            df_batch.columns = [str(c).strip().upper() for c in df_batch.columns]
            return df_batch
        except Exception as e:
            print(f"{Fore.RED}Critical Error reading batch file: {e}{Style.RESET_ALL}")
            return None

    def _resolve_path(self, user_path, default_folder):
        """
        Smartly tries to find a file. 
        1. Checks exact path provided.
        2. Checks inside default_folder.
        """
        # Case 1: Exact Path
        if os.path.exists(user_path):
            return user_path
        
        # Case 2: Inside Default Folder
        name_only = os.path.basename(user_path)
        common_path = os.path.join(default_folder, name_only)
        if os.path.exists(common_path):
            return common_path
            
        return None # Not found

    def run_batch_execution(self, df_batch, force_run=False):
        """
        Executes the jobs in the provided DataFrame.
        """
        success_count = 0
        fail_count = 0
        log_report = []

        print(f"\n{Fore.CYAN}--- EXECUTING {len(df_batch)} BATCH JOBS ---{Style.RESET_ALL}")

        for idx, row in df_batch.iterrows():
            job_id = str(row.get('JOB_ID', f'ROW_{idx+1}'))
            enabled = str(row.get('ENABLED', 'N')).strip().upper()
            
            if not force_run and enabled != 'Y':
                print(f"   {Fore.CYAN}[SKIP]{Style.RESET_ALL} Job {job_id} (Disabled)")
                continue

            print(f"\n{Fore.YELLOW}>>> JOB {job_id}{Style.RESET_ALL}")
            
            try:
                # 1. Parse Parameters
                rule_config = str(row.get('RULE_CONFIG', '')).strip().replace('.xlsx', '')
                raw_source = str(row.get('SOURCE_PATH', '')).strip()
                raw_sdt    = str(row.get('SDT_TEMPLATE', '')).strip()
                scope       = str(row.get('SCOPE', 'GLOBAL')).strip()
                out_prefix  = str(row.get('OUTPUT_PREFIX', 'BATCH')).strip()
                
                raw_trans = str(row.get('TRANSACTIONS', '')).strip()
                target_sheets = [t.strip() for t in raw_trans.split(',') if t.strip()]

                # 2. Smart Path Validation
                source_path = self._resolve_path(raw_source, 'raw_data')
                sdt_path = self._resolve_path(raw_sdt, 'config/sdt_templates')

                if not source_path: 
                    raise FileNotFoundError(f"Source file not found: {raw_source} (Checked root and raw_data/)")
                if not sdt_path: 
                    raise FileNotFoundError(f"Template file not found: {raw_sdt} (Checked root and config/sdt_templates/)")
                if not target_sheets: 
                    raise ValueError("No Transactions listed.")

                # 3. Execute Pipeline
                print(f"      Rule Config: {rule_config} | Scope: {scope}")
                print(f"      Template: {os.path.basename(sdt_path)}")
                
                # A. Load Rules
                config_loader = ConfigLoader(rule_config)
                rules, lookups = config_loader.load_config(division_code=scope)
                if rules.empty: raise ValueError(f"Rules for {rule_config} are empty.")

                # B. Load Source
                print(f"      Source Data: {os.path.basename(source_path)}")
                extractor = DataExtractor()
                df_legacy = extractor.load_data(source_path, format_type='MOVEX', sheet_name=0)

                # C. Generate Output
                print(f"      Generating SDT...")
                writer = SDTWriter(self.output_dir)
                out_filename = f"{out_prefix}_{rule_config}.xlsx"
                
                writer.generate_from_template(sdt_path, df_legacy, rules, target_sheets, out_filename)
                
                print(f"{Fore.GREEN}      [SUCCESS] Job {job_id} Finished.{Style.RESET_ALL}")
                success_count += 1
                log_report.append({'Job': job_id, 'Status': 'SUCCESS', 'File': out_filename})

            except Exception as e:
                print(f"{Fore.RED}      [FAILED] Job {job_id}: {e}{Style.RESET_ALL}")
                fail_count += 1
                log_report.append({'Job': job_id, 'Status': 'FAILED', 'Error': str(e)})

        # Summary
        print(f"\n{Fore.CYAN}--- BATCH RUN COMPLETE ---{Style.RESET_ALL}")
        print(f"Success: {Fore.GREEN}{success_count}{Style.RESET_ALL} | Failed: {Fore.RED}{fail_count}{Style.RESET_ALL}")
        
        # Save Log
        log_path = f"{self.output_dir}/Batch_Log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        pd.DataFrame(log_report).to_csv(log_path, index=False)
        print(f"Log saved to: {log_path}")
--- END FILE: .\backup\modules\batch_processor.py ---

--- START FILE: .\backup\modules\config_loader.py ---
import pandas as pd
import os

class ConfigLoader:
    def __init__(self, program_name, rule_dir='config/rules'):
        self.program_name = program_name
        self.rule_dir = rule_dir
        self.file_path = f"{rule_dir}/{program_name}.xlsx"
        self.rules_raw = None
        self.lookups = {}

    def load_config(self, division_code='GLOBAL'):
        if not os.path.exists(self.file_path):
             return pd.DataFrame(), {}
        
        print(f"Loading configuration for {self.program_name} (Scope: {division_code})...")
        
        try:
            df = pd.read_excel(self.file_path, sheet_name='Rules')
            df.columns = [c.upper().strip() for c in df.columns]
            
            if 'SCOPE' not in df.columns:
                df['SCOPE'] = 'GLOBAL'
            
            mask = (df['SCOPE'] == 'GLOBAL') | (df['SCOPE'] == division_code)
            df_filtered = df[mask].copy()
            
            df_filtered['SCOPE_SCORE'] = df_filtered['SCOPE'].apply(lambda x: 1 if x == 'GLOBAL' else 2)
            df_filtered = df_filtered.sort_values('SCOPE_SCORE', ascending=False)
            
            self.rules_raw = df_filtered.drop_duplicates(subset=['TARGET_FIELD'], keep='first').drop(columns=['SCOPE_SCORE'])

            xls = pd.ExcelFile(self.file_path)
            for sheet in xls.sheet_names:
                if sheet not in ['Rules', '_Audit_Log']:
                    df_lookup = pd.read_excel(xls, sheet_name=sheet, dtype=str)
                    if len(df_lookup.columns) >= 2:
                        key_col = df_lookup.columns[0]
                        val_col = df_lookup.columns[1]
                        self.lookups[sheet] = dict(zip(df_lookup[key_col], df_lookup[val_col]))

            return self.rules_raw, self.lookups

        except Exception as e:
            raise ValueError(f"Failed to load config: {e}")

    def get_existing_targets(self):
        if os.path.exists(self.file_path):
            try:
                df = pd.read_excel(self.file_path, sheet_name='Rules')
                return df['TARGET_FIELD'].unique().tolist()
            except:
                return []
        return []

    def get_program_rules(self, program_name=None):
        return self.rules_raw
--- END FILE: .\backup\modules\config_loader.py ---

--- START FILE: .\backup\modules\extractor.py ---
import pandas as pd
import os

class DataExtractor:
    def __init__(self):
        pass

    def load_data(self, file_path, format_type='MOVEX', sheet_name=0):
        """
        Reads Excel files based on specific ERP formats.
        
        Args:
            file_path (str): Path to the Excel file.
            format_type (str): 'MOVEX' or 'M3_SDT'.
            sheet_name (str/int): The sheet to read. Default is first sheet (0).
        
        Returns:
            pd.DataFrame: Cleaned dataframe with headers normalized.
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")

        print(f"Reading {format_type} file: {file_path}...")

        try:
            if format_type == 'MOVEX':
                # Movex: Headers on Row 1 (Index 0), Data starts Row 2
                df = pd.read_excel(file_path, sheet_name=sheet_name, header=0)
                
            elif format_type == 'M3_SDT':
                # M3 SDT: Headers on Row 1 (Index 0).
                # Rows 2 & 3 are blank/descriptions. Data starts Row 4 (Index 3).
                # We read the header separately, then the data, skipping the gap.
                
                # 1. Get Headers from Row 1
                df_headers = pd.read_excel(file_path, sheet_name=sheet_name, header=0, nrows=0)
                columns = df_headers.columns.tolist()
                
                # 2. Read Data starting at Row 4 (skipping rows 1, 2, 3)
                # Note: 'header=None' because we supplied cols manually, 'skiprows=3' skips the gap
                df = pd.read_excel(file_path, sheet_name=sheet_name, header=None, skiprows=3)
                
                # 3. Apply columns (Ensure count matches)
                if len(df.columns) == len(columns):
                    df.columns = columns
                else:
                    # Fallback if columns don't align (common in dirty SDT files)
                    print("Warning: Column count mismatch. Using first N columns.")
                    df = df.iloc[:, :len(columns)]
                    df.columns = columns

            # Standardization: Strip whitespace from column names and cast to string
            df.columns = [str(c).strip().upper() for c in df.columns]
            
            print(f"   -> Loaded {len(df)} rows.")
            return df

        except Exception as e:
            raise ValueError(f"Error extracting data: {e}")

# Usage Example:
# extractor = DataExtractor()
# df_legacy = extractor.load_data('raw_data/MMS001_Movex.xlsx', 'MOVEX')
# df_gold = extractor.load_data('raw_data/MMS001_Gold_Copy.xlsx', 'M3_SDT', sheet_name='MMS001MI')
--- END FILE: .\backup\modules\extractor.py ---

--- START FILE: .\backup\modules\hooks.py ---
import pandas as pd

def clean_description(row, source_col):
    """Example: Title Case and strip whitespace."""
    val = str(row.get(source_col, '')).strip()
    return val.title()

def logic_item_type(row, source_col):
    """Example: Derive M3 Item Type based on Movex Group."""
    # Assuming source_col is 'MMITGR'
    val = str(row.get(source_col, '')).strip()
    if val.startswith('9'):
        return '900' # Finished Good
    elif val.startswith('1'):
        return '100' # Raw Material
    else:
        return '999' # Misc

def combine_desc(row, source_col):
    """Example: Combine Name + Name2."""
    # Logic ignores source_col, uses specific columns
    d1 = str(row.get('MMITDS', '')).strip()
    d2 = str(row.get('MMFUDS', '')).strip()
    return f"{d1} {d2}".strip()[:40] # Truncate to 40 chars
--- END FILE: .\backup\modules\hooks.py ---

--- START FILE: .\backup\modules\mco_importer.py ---
import pandas as pd
import os
import glob
from colorama import Fore, Style
import time
from modules.audit_manager import AuditManager

class MCOImporter:
    def __init__(self, sdt_folder='config/sdt_templates'):
        self.sdt_folder = sdt_folder

    def _smart_pick(self, options, title_prompt):
        filtered_indices = list(range(len(options)))
        filter_text = ""
        while True:
            print(f"\n{Fore.CYAN}--- {title_prompt} ---{Style.RESET_ALL}")
            if filter_text: print(f"{Fore.YELLOW}[Filter: '{filter_text}'] (Type 'all' to clear){Style.RESET_ALL}")
            limit = 20; count = 0; display_map = {}
            for i in filtered_indices:
                count += 1
                if count > limit:
                    print(f"   ... ({len(filtered_indices) - limit} more matches)")
                    break
                print(f"   {count}. {options[i]}")
                display_map[count] = i
            print(f"\n{Fore.GREEN}Type a Number to select, or Text to filter.{Style.RESET_ALL}")
            user_input = input(f"{Fore.CYAN}>> Selection: {Style.RESET_ALL}").strip()
            if not user_input: continue
            if user_input.isdigit():
                choice = int(user_input)
                if choice in display_map: return options[display_map[choice]]
            elif user_input.lower() in ['all', 'clear']: filter_text = ""; filtered_indices = list(range(len(options)))
            else:
                filter_text = user_input
                filtered_indices = [i for i, opt in enumerate(options) if filter_text.lower() in str(opt).lower()]
                if not filtered_indices: print("No matches."); time.sleep(0.5); filter_text = ""; filtered_indices = list(range(len(options)))

    def _find_header_row(self, file_path, sheet_name):
        df_raw = pd.read_excel(file_path, sheet_name=sheet_name, header=None, nrows=15)
        header_idx = -1
        keywords = ['FIELD NAME', 'M3 FIELD', 'TECHNICAL NAME']
        for idx, row in df_raw.iterrows():
            row_str = " ".join([str(x).upper() for x in row.values])
            if any(k in row_str for k in keywords): header_idx = idx; break
        if header_idx == -1: header_idx = 2 
        print(f"      -> Detected headers on Row {header_idx + 1}")
        df = pd.read_excel(file_path, sheet_name=sheet_name, header=header_idx)
        df.columns = [str(c).strip().replace('\n', ' ').replace('_', ' ').upper() for c in df.columns]
        return df, header_idx

    def interactive_import(self, mco_path, output_dir='config/rules'):
        try:
            xls_mco = pd.ExcelFile(mco_path)
            mco_sheets = xls_mco.sheet_names
            selected_mco_sheet = self._smart_pick(mco_sheets, "Select MCO Sheet (Business Object)")
            
            # Load Data and keep header index to report accurate Excel row numbers
            df_mco, header_idx = self._find_header_row(mco_path, selected_mco_sheet)
            
            default_name = selected_mco_sheet.split(' ')[0] + "MI"
            api_name = input(f"{Fore.CYAN}   >> Name this Rule Set (Default: {default_name}): {Style.RESET_ALL}").strip().upper()
            if not api_name: api_name = default_name.upper()
            
            self._generate_master_rule_file(df_mco, api_name, output_dir, excel_offset=header_idx+2)
        except Exception as e:
            print(f"{Fore.RED}Error during import: {e}{Style.RESET_ALL}")
            import traceback
            traceback.print_exc()

    def _generate_master_rule_file(self, df_mco, api_name, output_dir, excel_offset):
        print(f"\n   -> Validating MCO content for {api_name}...")
        
        cols = df_mco.columns
        target_aliases = ['FIELD NAME', 'M3 FIELD', 'TECHNICAL NAME']
        col_target = next((c for c in cols if any(a in c for a in target_aliases)), None)
        col_req    = next((c for c in cols if 'CUSTOMER REQUIRED' in c or 'REQUIRED' in c), None)
        col_source = next((c for c in cols if 'CONVERSION SOURCE' in c or 'SOURCE' in c or 'LEGACY' in c), None)
        col_logic  = next((c for c in cols if 'TRANSFORMATION RULE' in c or 'LOGIC' in c or 'RULE' in c), None)

        if not col_target: print(f"{Fore.RED}      CRITICAL: Could not find Target Column.{Style.RESET_ALL}"); return

        rules_data = []
        
        # --- 1. DUPLICATE CHECKER ---
        seen_map = {} # { 'ITNO': {row_idx, source_val} }
        duplicates = []

        for idx, row in df_mco.iterrows():
            tgt = str(row.get(col_target, '')).strip().upper()
            if not tgt or tgt == 'NAN': continue
            if len(tgt) == 6: tgt = tgt[2:] # Strip prefix

            raw_src = str(row.get(col_source, '')).strip().replace('nan', '')
            excel_row = idx + excel_offset

            if tgt in seen_map:
                prev = seen_map[tgt]
                duplicates.append({
                    'Field': tgt,
                    'Entry 1': f"Row {prev['row']} (Src: {prev['src']})",
                    'Entry 2': f"Row {excel_row} (Src: {raw_src})"
                })
            else:
                seen_map[tgt] = {'row': excel_row, 'src': raw_src}

        # --- 2. REPORT & HALT ---
        if duplicates:
            print(f"\n{Fore.RED}" + "="*60)
            print(f"   CRITICAL ERROR: DUPLICATE RULES DETECTED")
            print(f"   The MCO contains multiple rules for the same Target Field.")
            print(f"   Please fix the Excel file and try again.")
            print("="*60 + f"{Style.RESET_ALL}")
            
            print(f"{'FIELD':<15} | {'ENTRY 1':<30} | {'ENTRY 2'}")
            print("-" * 80)
            for d in duplicates:
                print(f"{d['Field']:<15} | {d['Entry 1']:<30} | {d['Entry 2']}")
            print("-" * 80)
            return # HALT GENERATION

        # --- 3. GENERATION PHASE ---
        print(f"   -> No duplicates found. Proceeding with generation...")
        
        target_path = f"{output_dir}/{api_name}.xlsx"
        if os.path.exists(target_path):
            print(f"{Fore.YELLOW}   Creating safety snapshot before overwrite...{Style.RESET_ALL}")
            auditor = AuditManager(output_dir) 
            auditor.create_snapshot(api_name, "AUTO_PRE_MCO_OVERWRITE")

        for idx, row in df_mco.iterrows():
            tgt = str(row.get(col_target, '')).strip().upper()
            if not tgt or tgt == 'NAN': continue
            if len(tgt) == 6: tgt = tgt[2:]

            raw_src = str(row.get(col_source, '')).strip().replace('nan', '').upper()
            raw_req = str(row.get(col_req, '0')).strip().replace('nan', '0')
            raw_logic = str(row.get(col_logic, '')).strip().replace('nan', '')
            
            r_type, r_val, r_src, desc = 'IGNORE', '', '', 'Imported from MCO'
            if raw_src:
                r_type = 'DIRECT'; r_src = raw_src; desc = f"Mapped from {raw_src}"
            elif raw_req == '1' or raw_req.upper().startswith('Y'):
                if 'CONST' in raw_logic.upper() or 'FIXED' in raw_logic.upper():
                        r_type = 'CONST'; desc = f"Required Constant: {raw_logic}"
                else:
                    r_type = 'TODO'; desc = f"Required! Logic: {raw_logic}"
            else:
                r_type = 'IGNORE'; desc = "MCO listed but not required"
            
            rules_data.append({'TARGET_API': api_name, 'TARGET_FIELD': tgt, 'SOURCE_FIELD': r_src, 'RULE_TYPE': r_type, 'RULE_VALUE': r_val, 'SCOPE': 'GLOBAL', 'DESCRIPTION': desc})

        if not os.path.exists(output_dir): os.makedirs(output_dir)
        path = f"{output_dir}/{api_name}.xlsx"
        df_rules = pd.DataFrame(rules_data)
        
        def sorter(x):
            if x == 'TODO': return 0
            if x in ['DIRECT', 'CONST', 'MAP']: return 1
            return 2
        df_rules['Sort'] = df_rules['RULE_TYPE'].apply(sorter)
        df_rules = df_rules.sort_values(['Sort', 'TARGET_FIELD']).drop(columns=['Sort'])

        with pd.ExcelWriter(path, engine='xlsxwriter') as writer:
            df_rules.to_excel(writer, sheet_name='Rules', index=False)
            pd.DataFrame(columns=['TIMESTAMP']).to_excel(writer, sheet_name='_Audit_Log', index=False)
        print(f"      -> Created Master Config: {path} ({len(rules_data)} fields)")
--- END FILE: .\backup\modules\mco_importer.py ---

--- START FILE: .\backup\modules\rule_manager.py ---
import pandas as pd
import os
import glob
from colorama import Fore, Style
from modules.audit_manager import AuditManager

class RuleManager:
    def __init__(self, rule_dir='config/rules'):
        self.rule_dir = rule_dir
        if not os.path.exists(self.rule_dir):
            os.makedirs(self.rule_dir)

    def interactive_manual_entry(self):
        # 1. Select Rule File
        files = glob.glob(os.path.join(self.rule_dir, "*.xlsx"))
        if not files:
            print(f"{Fore.RED}No rule files found. Please import MCO first.{Style.RESET_ALL}")
            return

        print(f"\n{Fore.CYAN}--- Select Rule File to Edit ---{Style.RESET_ALL}")
        for i, f in enumerate(files):
            print(f"   {i+1}. {os.path.basename(f)}")
        
        try:
            f_idx = int(input(f"{Fore.CYAN}   >> Select File (Number): {Style.RESET_ALL}").strip()) - 1
            selected_path = files[f_idx]
            program_name = os.path.basename(selected_path).replace('.xlsx', '')
        except:
            return

        # 2. Load Rules
        try:
            df = pd.read_excel(selected_path, sheet_name='Rules')
        except Exception as e:
            print(f"{Fore.RED}Error reading file: {e}{Style.RESET_ALL}")
            return

        # Fix types
        for col in ['RULE_VALUE', 'SOURCE_FIELD', 'RULE_TYPE', 'DESCRIPTION']:
            if col in df.columns:
                df[col] = df[col].astype(object)
        
        # Track if changes were made
        changes_made = False

        while True:
            print(f"\n{Fore.YELLOW}--- Editing {program_name} ---{Style.RESET_ALL}")
            print("1. Show All Fields")
            print("2. Show Only 'TODO' (Unmapped)")
            print("3. Show Only 'DIRECT/CONST' (Mapped)")
            print("4. Save & Exit")
            print("0. Cancel / Exit without Saving")
            
            view_opt = input("Select View: ").strip()
            
            if view_opt == '4': 
                if changes_made:
                    print(f"   Saving changes to disk...")
                    # 1. Save the Excel File
                    with pd.ExcelWriter(selected_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
                        df.to_excel(writer, sheet_name='Rules', index=False)
                    
                    # 2. Auto-Commit (Generate History)
                    print(f"   Updating Audit Log...")
                    auditor = AuditManager(self.rule_dir)
                    auditor.commit_changes(program_name)
                    
                    print(f"{Fore.GREEN}   -> Saved and Audited.{Style.RESET_ALL}")
                else:
                    print(f"{Fore.YELLOW}   No changes to save.{Style.RESET_ALL}")
                break

            if view_opt in ['0', 'q']:
                print(f"{Fore.YELLOW}   Exiting without saving.{Style.RESET_ALL}")
                break
            
            # View Logic
            if view_opt == '2':
                df_view = df[df['RULE_TYPE'] == 'TODO'].reset_index()
            elif view_opt == '3':
                df_view = df[df['RULE_TYPE'].isin(['DIRECT', 'CONST', 'MAP'])].reset_index()
            else:
                df_view = df.reset_index()

            if df_view.empty:
                print("   (No fields found for this view)")
                continue

            # Display
            print(f"\n{Fore.CYAN}Available Targets:{Style.RESET_ALL}")
            limit = 20
            for idx, row in df_view.head(limit).iterrows():
                # Safe string conversion
                val_disp = str(row['RULE_VALUE'])[:15] if pd.notna(row['RULE_VALUE']) else ""
                src_disp = str(row['SOURCE_FIELD'])[:15] if pd.notna(row['SOURCE_FIELD']) else ""
                print(f"   {idx+1}. {row['TARGET_FIELD']:<10} | Type: {row['RULE_TYPE']:<6} | Src: {src_disp:<10} | Val: {val_disp}")
            
            if len(df_view) > limit:
                print(f"   ... ({len(df_view) - limit} more fields)")

            # Edit Logic
            sel_input = input(f"\n{Fore.CYAN}>> Enter Number to Edit (or 'b' to back): {Style.RESET_ALL}").strip()
            if not sel_input.isdigit(): continue
            sel_idx = int(sel_input) - 1
            
            if 0 <= sel_idx < len(df_view):
                real_index = df_view.iloc[sel_idx]['index']
                target_field = df.at[real_index, 'TARGET_FIELD']
                current_type = df.at[real_index, 'RULE_TYPE']
                
                print(f"\n   Editing: {target_field} (Current: {current_type})")
                print("   Types: 1. CONST  2. DIRECT  3. MAP  4. IGNORE  5. TODO")
                t_choice = input("   Select Type: ").strip()
                
                new_type, new_val, new_src = current_type, '', ''
                
                if t_choice == '1':
                    new_type = 'CONST'
                    new_val = input("   Value: ").strip()
                elif t_choice == '2':
                    new_type = 'DIRECT'
                    new_src = input("   Source Field: ").strip().upper()
                elif t_choice == '3':
                    new_type = 'MAP'
                    new_src = input("   Source Field: ").strip().upper()
                    new_val = f"MAP_{new_src}_TO_{target_field}"
                elif t_choice == '4':
                    new_type = 'IGNORE'
                elif t_choice == '5':
                    new_type = 'TODO'
                
                # Apply update
                df.at[real_index, 'RULE_TYPE'] = new_type
                df.at[real_index, 'RULE_VALUE'] = new_val
                df.at[real_index, 'SOURCE_FIELD'] = new_src
                df.at[real_index, 'DESCRIPTION'] = 'Manually Updated'
                
                changes_made = True
                print(f"   -> Updated in memory.")
--- END FILE: .\backup\modules\rule_manager.py ---

--- START FILE: .\backup\modules\rule_promoter.py ---
import pandas as pd
import os
import datetime

class RulePromoter:
    def __init__(self, output_dir='config/rules'):
        self.output_dir = output_dir
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def generate_production_rules(self, program_name, df_draft, df_legacy, df_gold, join_key_legacy, join_key_m3):
        print(f"Promoting Draft to Production for {program_name}...")
        
        production_rules = []
        lookup_sheets = {}
        
        # Handle list keys for merge
        key_leg = join_key_legacy if isinstance(join_key_legacy, list) else [join_key_legacy]
        key_m3 = join_key_m3 if isinstance(join_key_m3, list) else [join_key_m3]
        
        merged = pd.merge(df_legacy, df_gold, left_on=key_leg, right_on=key_m3, how='inner', suffixes=('_SRC', '_TGT'))
        
        for idx, row in df_draft.iterrows():
            target_col = row['TARGET']
            source_col = row['SOURCE']
            rule_type  = row['TYPE']
            logic_val  = row['LOGIC']
            
            rule_entry = {
                'TARGET_API': program_name,
                'TARGET_FIELD': target_col,
                'SOURCE_FIELD': '',
                'RULE_TYPE': rule_type,
                'RULE_VALUE': '',
                'SCOPE': 'GLOBAL',
                'DESCRIPTION': f"Auto-generated. Confidence: {row.get('CONFIDENCE', 'N/A')}"
            }

            if rule_type == 'CONST':
                rule_entry['RULE_VALUE'] = logic_val
            elif rule_type == 'DIRECT':
                rule_entry['SOURCE_FIELD'] = source_col
            elif rule_type == 'MAP':
                rule_entry['SOURCE_FIELD'] = source_col
                lookup_name = f"MAP_{source_col}_TO_{target_col}"[:30]
                rule_entry['RULE_VALUE'] = lookup_name
                
                try:
                    src_lookup_col = source_col if source_col in merged.columns else f"{source_col}_SRC"
                    tgt_lookup_col = target_col if target_col in merged.columns else f"{target_col}_TGT"
                    
                    lookup_df = merged[[src_lookup_col, tgt_lookup_col]].dropna().drop_duplicates(subset=[src_lookup_col])
                    lookup_df.columns = ['SOURCE_KEY', 'TARGET_VALUE']
                    lookup_sheets[lookup_name] = lookup_df.sort_values('SOURCE_KEY')
                except KeyError:
                    print(f"   Warning: Could not auto-generate lookup for {lookup_name}.")
            else:
                rule_entry['RULE_TYPE'] = 'TODO'
                rule_entry['DESCRIPTION'] = "Logic unknown. Requires manual intervention."

            production_rules.append(rule_entry)

        df_prod = pd.DataFrame(production_rules)
        file_path = f"{self.output_dir}/{program_name}.xlsx"
        
        with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:
            df_prod.to_excel(writer, sheet_name='Rules', index=False)
            for sheet_name, df_table in lookup_sheets.items():
                df_table.to_excel(writer, sheet_name=sheet_name, index=False)
            pd.DataFrame(columns=['TIMESTAMP', 'USER', 'ACTION', 'TARGET_FIELD', 'OLD_VALUE', 'NEW_VALUE']).to_excel(writer, sheet_name='_Audit_Log', index=False)
            
        print(f"   -> Production Rule File Created: {file_path}")
        return file_path
--- END FILE: .\backup\modules\rule_promoter.py ---

--- START FILE: .\backup\modules\sdt_writer.py ---
import pandas as pd
import shutil
import os
import glob
import datetime
from openpyxl import load_workbook
from colorama import Fore, Style

class SDTWriter:
    def __init__(self, output_dir='output'):
        self.output_dir = output_dir
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def _get_versioned_filename(self, base_name):
        """
        Generates a filename with date and revision number.
        Format: BASE_NAME_YYYYMMDD_revN.xlsx
        """
        # Ensure extension is handled cleanly
        base_clean = base_name.replace('.xlsx', '')
        today = datetime.datetime.now().strftime("%Y%m%d")
        
        # Pattern to find existing files for today
        search_pattern = os.path.join(self.output_dir, f"{base_clean}_{today}_rev*.xlsx")
        existing_files = glob.glob(search_pattern)
        
        rev = 1
        if existing_files:
            try:
                # Extract revisions: ..._rev1.xlsx -> 1
                revs = []
                for f in existing_files:
                    # Split by '_rev' and take the part before '.xlsx'
                    part = f.split('_rev')[-1].split('.')[0]
                    if part.isdigit():
                        revs.append(int(part))
                
                if revs:
                    rev = max(revs) + 1
            except Exception:
                pass # Default to 1 if parsing fails
        
        return f"{base_clean}_{today}_rev{rev}.xlsx"

    def generate_from_template(self, template_path, legacy_data, rules_df, target_sheets, output_name):
        # 1. Calculate Versioned Filename
        versioned_name = self._get_versioned_filename(output_name)
        out_path = os.path.join(self.output_dir, versioned_name)
        
        print(f"   -> Creating new version: {versioned_name}")
        shutil.copyfile(template_path, out_path)
        
        wb = load_workbook(out_path)
        
        for sheet_name in target_sheets:
            if sheet_name not in wb.sheetnames:
                print(f"   Warning: Sheet '{sheet_name}' not found in template.")
                continue
                
            print(f"   -> Populating Sheet: {sheet_name}...")
            ws = wb[sheet_name]
            
            # Read Headers
            headers = [cell.value for cell in ws[1]]
            valid_fields = [str(h).strip().upper() for h in headers if h is not None]
            
            # Transform
            df_transformed = self._transform_data(legacy_data, rules_df, valid_fields, sheet_name)
            
            # Write Data
            start_row = 4
            data_rows = df_transformed.values.tolist()
            
            for i, row in enumerate(data_rows):
                for j, val in enumerate(row):
                    cell = ws.cell(row=start_row + i, column=j + 1)
                    cell.value = str(val) if pd.notna(val) else ""

        wb.save(out_path)
        print(f"   -> SDT Generation Complete: {out_path}")

    def _transform_data(self, df_source, rules_df, valid_fields, sheet_name):
        df_out = pd.DataFrame('', index=df_source.index, columns=valid_fields)
        rule_map = rules_df.set_index('TARGET_FIELD').to_dict('index')
        
        mapped_count = 0
        
        for field in valid_fields:
            if field in rule_map:
                rule = rule_map[field]
                r_type = rule['RULE_TYPE']
                # Ensure we strip whitespace from rule source
                r_src = str(rule['SOURCE_FIELD']).strip().upper()
                r_val = rule['RULE_VALUE']
                
                try:
                    if r_type == 'DIRECT':
                        # Check if source exists (Case Insensitive Check)
                        source_cols_upper = {c.upper(): c for c in df_source.columns}
                        
                        if r_src in source_cols_upper:
                            real_col_name = source_cols_upper[r_src]
                            df_out[field] = df_source[real_col_name]
                            mapped_count += 1
                        else:
                            # CRITICAL DEBUG: Missing Source
                            print(f"{Fore.RED}      [MISSING SOURCE] Rule for '{field}' expects '{r_src}'.{Style.RESET_ALL}")
                            matches = [c for c in df_source.columns if r_src in c.upper()]
                            if matches:
                                print(f"{Fore.YELLOW}         Did you mean: {matches}?{Style.RESET_ALL}")
                    
                    elif r_type == 'CONST':
                        df_out[field] = r_val
                        mapped_count += 1
                        
                    elif r_type == 'MAP':
                        source_cols_upper = {c.upper(): c for c in df_source.columns}
                        if r_src in source_cols_upper:
                            real_col_name = source_cols_upper[r_src]
                            df_out[field] = df_source[real_col_name]
                            mapped_count += 1
                            
                except Exception as e:
                    print(f"Error mapping {field}: {e}")
        
        # --- DIAGNOSTIC BLOCK ---
        if mapped_count == 0:
            print(f"\n{Fore.YELLOW}   [DIAGNOSTIC REPORT for {sheet_name}]{Style.RESET_ALL}")
            print(f"   The tool found 0 matching rules. This usually means the Target Field names in your Rules don't match the SDT.")
            
            print(f"   1. SDT Expects these fields (Sample): {valid_fields[:5]}")
            available_rules = list(rule_map.keys())
            print(f"   2. Your Rules Config has these fields (Sample): {available_rules[:5]}")
            overlap = set(valid_fields).intersection(set(available_rules))
            print(f"   3. Overlap Count: {len(overlap)} fields match.")
            
            if len(overlap) == 0:
                print(f"{Fore.RED}      CRITICAL: Zero overlap. Your MCO Importer likely picked the wrong column for 'M3 Field'.")
        
        else:
            print(f"      -> Successfully mapped {mapped_count} columns.")
            
        return df_out
--- END FILE: .\backup\modules\sdt_writer.py ---

--- START FILE: .\backup\modules\transform_engine.py ---
import pandas as pd
import numpy as np
# Import the hooks module
try:
    import modules.hooks as hooks
except ImportError:
    hooks = None

class TransformEngine:
    def __init__(self, rules_df, lookups):
        self.rules = rules_df
        self.lookups = lookups

    def process(self, df_source):
        # 1. Identify Target Fields from Rules
        target_fields = self.rules['TARGET_FIELD'].unique()
        df_target = pd.DataFrame(index=df_source.index)

        # 2. Iterate Rules
        print("   Applying Transformation Rules...")
        
        # Optimize: Loop through rules and apply vector operations where possible
        for index, rule in self.rules.iterrows():
            target_col = rule['TARGET_FIELD']
            source_col = str(rule['SOURCE_FIELD']).strip().upper()
            rule_type = rule['RULE_TYPE']
            rule_value = rule['RULE_VALUE']

            try:
                # --- RULE: DIRECT ---
                if rule_type == 'DIRECT':
                    # Find case-insensitive match for source column
                    # (We assume extractor made df_source columns upper, but being safe)
                    if source_col in df_source.columns:
                        df_target[target_col] = df_source[source_col]
                
                # --- RULE: CONSTANT ---
                elif rule_type == 'CONST':
                    df_target[target_col] = rule_value

                # --- RULE: MAP (Lookup) ---
                elif rule_type == 'MAP':
                    if source_col in df_source.columns:
                        # Logic: rule_value is the sheet name of the lookup (e.g. MAP_ITGR_TO_ITCL)
                        # self.lookups is a dict of dicts: { 'MAP_NAME': { 'OLD': 'NEW' } }
                        if rule_value in self.lookups:
                            lookup_dict = self.lookups[rule_value]
                            # Map and fillna with original or default? Usually keep original or error.
                            # Here we leave as NaN if not found, or use fillna(rule_value) if you want a default
                            df_target[target_col] = df_source[source_col].astype(str).map(lookup_dict)
                        else:
                            # If no lookup table exists yet, just copy direct (fallback)
                            df_target[target_col] = df_source[source_col]

                # --- RULE: PYTHON (The New Feature) ---
                elif rule_type == 'PYTHON':
                    if hooks and hasattr(hooks, rule_value):
                        func = getattr(hooks, rule_value)
                        # Apply function row-by-row (slower but flexible)
                        df_target[target_col] = df_source.apply(lambda row: func(row, source_col), axis=1)
                    else:
                        print(f"      [WARNING] Python hook '{rule_value}' not found in modules/hooks.py")

            except Exception as e:
                # Log error but don't crash everything
                # print(f"Error processing {target_col}: {e}")
                pass
        
        return df_target
--- END FILE: .\backup\modules\transform_engine.py ---

--- START FILE: .\backup\modules\validator_analyzer.py ---
import pandas as pd
import numpy as np

class ValidatorAnalyzer:
    def __init__(self, ignore_cols=None):
        if ignore_cols is None:
            self.ignore_cols = ['CONO', 'MESSAGE', 'RGDT', 'LMDT', 'RGTM', 'CHID']
        else:
            self.ignore_cols = ignore_cols

    def _prepare_df(self, df):
        df = df.copy()
        cols_to_drop = [c for c in self.ignore_cols if c in df.columns]
        if cols_to_drop:
            df.drop(columns=cols_to_drop, inplace=True)
        return df

    def _is_valid_predictor(self, series, total_rows):
        unique_count = series.nunique()
        if unique_count <= 1: return False
        if unique_count == total_rows: return False
        if unique_count > 100: return False 
        if unique_count > (total_rows * 0.2): return False
        return True

    def _get_col_safe(self, df, col_name, suffix):
        if col_name in df.columns: return df[col_name]
        suffixed_name = f"{col_name}{suffix}"
        if suffixed_name in df.columns: return df[suffixed_name]
        return pd.Series([np.nan] * len(df))

    def generate_diff_report(self, df_generated, df_gold, join_key, output_path):
        keys = join_key if isinstance(join_key, list) else [join_key]
        print(f"Running Validation vs Gold Standard on keys: {keys}")
        
        df_gen = self._prepare_df(df_generated).add_suffix('_GEN')
        df_gold = self._prepare_df(df_gold).add_suffix('_GOLD')
        
        keys_gen = [f"{k}_GEN" for k in keys]
        keys_gold = [f"{k}_GOLD" for k in keys]
        
        merged = pd.merge(df_gen, df_gold, left_on=keys_gen, right_on=keys_gold, how='outer', indicator=True)
        
        missing = merged[merged['_merge'] == 'left_only']
        extra = merged[merged['_merge'] == 'right_only']
        intersect = merged[merged['_merge'] == 'both']
        
        diff_rows = []
        base_cols = [c.replace('_GEN', '') for c in df_gen.columns if c not in keys_gen]
        
        for idx, row in intersect.iterrows():
            diffs = []
            for col in base_cols:
                v_gen = str(row.get(f"{col}_GEN", "")).strip().replace('nan', '')
                v_gold = str(row.get(f"{col}_GOLD", "")).strip().replace('nan', '')
                if v_gen != v_gold:
                    diffs.append(f"{col}: '{v_gen}'!='{v_gold}'")
            
            if diffs:
                key_val_str = "-".join([str(row.get(k, '')) for k in keys_gen])
                entry = {'KEY': key_val_str, 'DIFFS': " | ".join(diffs)}
                for col in base_cols:
                    entry[f"{col}_GEN"] = row.get(f"{col}_GEN")
                    entry[f"{col}_GOLD"] = row.get(f"{col}_GOLD")
                diff_rows.append(entry)
                
        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:
            if diff_rows: pd.DataFrame(diff_rows).to_excel(writer, sheet_name='Diffs', index=False)
            else: pd.DataFrame({'Info': ['No Differences Found']}).to_excel(writer, sheet_name='Diffs')
            missing.to_excel(writer, sheet_name='Missing_In_M3', index=False)
            extra.to_excel(writer, sheet_name='Extra_In_M3', index=False)
        print(f"   -> Validation Report: {output_path}")

    def reverse_engineer_rules(self, df_legacy, df_gold, leg_key, m3_key, existing_targets=None):
        if existing_targets is None: existing_targets = []
        print(f"Starting Analysis (Skipping {len(existing_targets)} pre-defined fields)...")
        
        leg_keys = leg_key if isinstance(leg_key, list) else [leg_key]
        m3_keys = m3_key if isinstance(m3_key, list) else [m3_key]
        
        df_gold = self._prepare_df(df_gold)
        merged = pd.merge(df_legacy, df_gold, left_on=leg_keys, right_on=m3_keys, how='inner', suffixes=('_SRC', '_TGT'))
        total_rows = len(merged)
        print(f"   -> Analyzing {total_rows} matched rows.")
        
        valid_predictors = []
        for col in df_legacy.columns:
            if self._is_valid_predictor(df_legacy[col], total_rows):
                valid_predictors.append(col)
        
        suggestions = []

        for m3_col in df_gold.columns:
            if m3_col in m3_keys: continue
            if m3_col in existing_targets: continue
            
            series_m3 = self._get_col_safe(merged, m3_col, '_TGT')
            unique_vals = series_m3.dropna().unique()
            
            # CHECK 1: CONSTANTS
            if len(unique_vals) == 1:
                suggestions.append({
                    'TARGET': m3_col, 'SOURCE': 'ALL_ROWS_IDENTICAL', 'TYPE': 'CONST',
                    'LOGIC': str(unique_vals[0]), 'CONFIDENCE': 'High'
                })
                continue
            
            # CHECK 2: DIRECT MAPPING
            match_found = False
            for leg_col in df_legacy.columns:
                series_leg = self._get_col_safe(merged, leg_col, '_SRC')
                s_m3 = series_m3.astype(str).str.strip().replace({'nan':'', '0':'', '0.0':''})
                s_leg = series_leg.astype(str).str.strip().replace({'nan':'', '0':'', '0.0':''})
                
                if (s_m3 != '').sum() < (total_rows * 0.05): continue
                
                if s_m3.equals(s_leg):
                    suggestions.append({
                        'TARGET': m3_col, 'SOURCE': leg_col, 'TYPE': 'DIRECT',
                        'LOGIC': 'Direct Copy', 'CONFIDENCE': 'High'
                    })
                    match_found = True
                    break
            if match_found: continue

            # CHECK 3: CONDITIONAL LOGIC
            if len(unique_vals) > 50:
                 suggestions.append({
                    'TARGET': m3_col, 'SOURCE': 'NO_OBVIOUS_PATTERN_FOUND', 'TYPE': 'UNKNOWN',
                    'LOGIC': 'High Variance / ID field', 'CONFIDENCE': 'N/A'
                })
                 continue

            best_predictor = None
            for pred_col in valid_predictors:
                series_pred = self._get_col_safe(merged, pred_col, '_SRC')
                temp_df = pd.DataFrame({'PRED': series_pred, 'TARGET': series_m3})
                if temp_df.groupby('PRED')['TARGET'].nunique().mean() == 1.0:
                    best_predictor = pred_col
                    break
            
            if best_predictor:
                series_pred = self._get_col_safe(merged, best_predictor, '_SRC')
                temp_df = pd.DataFrame({'PRED': series_pred, 'TARGET': series_m3})
                example = temp_df.groupby('PRED')['TARGET'].first().head(3).to_dict()
                suggestions.append({
                    'TARGET': m3_col, 'SOURCE': best_predictor, 'TYPE': 'MAP',
                    'LOGIC': f"Based on {best_predictor} (e.g. {str(example)})", 
                    'CONFIDENCE': 'Medium'
                })
            else:
                suggestions.append({
                    'TARGET': m3_col, 'SOURCE': 'NO_OBVIOUS_PATTERN_FOUND', 'TYPE': 'UNKNOWN',
                    'LOGIC': 'Complex or Manual', 'CONFIDENCE': 'Low'
                })

        return pd.DataFrame(suggestions)
--- END FILE: .\backup\modules\validator_analyzer.py ---

--- START FILE: .\backup\modules\__init__.py ---

--- END FILE: .\backup\modules\__init__.py ---

--- START FILE: .\config\business_units.csv ---
UNIT,DESCRIPTION
WPP,Portion Packaging
WD,Division
ABI,ABI
WFI,Films
WEM,Mexico
WHS,Heat seal

--- END FILE: .\config\business_units.csv ---

--- START FILE: .\config\migration_map.csv ---
MCO_SHEET,API_NAME,SDT_TEMPLATE,TRANSACTION_SHEET
Item Master,MMS200MI,MMS200MI_API.xlsx,"API_MMS200MI_AddItmBasic,API_MMS200MI_UpdItmBasic,API_MMS200MI_DltItm"
Item Warehouse,MMS200MI,MMS200MI_API.xlsx,"API_MMS200MI_AddItmWhs,API_MMS200MI_UpdItmWhs,API_MMS200MI_DltItmWhs"
Warehouse Locations,MMS010MI,MMS010MI_API.xlsx,
Customers,CRS610MI,CRS610MI_API.xlsx,"API_CRS610MI_Copy,API_CRS610MI_ChgBasicData,API_CRS610MI_ChgFinancial,API_CRS610MI_ChgOrderInfo,API_CRS610MI_Delete"
Customer - CDF,CRS610MI,CRS610MI_API.xlsx,
Customer Addresses,CRS610MI,CRS610MI_API.xlsx,API_CRS610MI_AddAddress
ListSupplierAddresses,CRS620MI,CRS620MI_API.xlsx,
Product Header,PDS001MI,PDS001MI_API.xlsx,
Product Header Features,PDS001MI,PDS001MI_API.xlsx,
Product Header Draw Measuremnts,PDS001MI,PDS001MI_API.xlsx,
Product Variant Seq-1,PDS001MI,PDS001MI_API.xlsx,
Product Variant Seq-2,PDS001MI,PDS001MI_API.xlsx,
Product Matrixes Header,PDS001MI,PDS001MI_API.xlsx,
Product Matrixes Lines,PDS001MI,PDS001MI_API.xlsx,
Product Options - PDS050MI,PDS050MI,PDS050MI_API.xlsx,
Product Features - PDS055MI,PDS055MI,PDS055MI_API.xlsx,
ConnectOptionsFeatures PDS056MI,PDS056MI,PDS056MI_API.xlsx,
Work Centers - PDS010,PDS010MI,PDS010MI_API.xlsx,
Work Shifts - MPD_PDS030,PDS030MI,PDS030MI_API.xlsx,
Item Facility,MMS200MI,MMS200MI_API.xlsx,API_MMS200MI_UpdItmFac
Suppliers,CRS620MI,CRS620MI_API.xlsx,"API_CRS620MI_CopyTemplate,API_CRS620MI_UpdSupplier,API_CRS620MI_DelSupplier"

--- END FILE: .\config\migration_map.csv ---

--- START FILE: .\config\source_map.csv ---
MCO_SHEET,SOURCE_FILE,JOIN_KEY
Item Master,raw_data/MITMAS.xlsx,MMITNO
Customers,raw_data/OCUSMA.xlsx,OKCUNO
Customer Addresses,raw_data/OCUSAD.xlsx,OPCUNO
Item Warehouse,raw_data/MITBAL.xlsx,MBITNO
Item Facility,raw_data/MITFAC.xlsx,M9ITNO
Suppliers,raw_data/CIDMAS.xlsx,IDSUNO

--- END FILE: .\config\source_map.csv ---

--- START FILE: .\config\surgical_def.csv ---
OBJECT_TYPE,MCO_SHEET
CUSTOMER,Customers
CUSTOMER,Customer Addresses
ITEM,Item Master
ITEM,Item Warehouse
SUPPLIER,Suppliers
ITEM,Item Facility

--- END FILE: .\config\surgical_def.csv ---

--- START FILE: .\modules\audit_manager.py ---
import pandas as pd
import os
import getpass
import datetime
import shutil
import glob
from colorama import Fore, Style

class AuditManager:
    def __init__(self, rule_dir='config/rules'):
        self.rule_dir = rule_dir
        self.history_dir = f"{rule_dir}/.history"
        self.snapshot_dir = f"{rule_dir}/.snapshots"
        
        for d in [self.history_dir, self.snapshot_dir]:
            if not os.path.exists(d): os.makedirs(d)

    def create_snapshot(self, program_name, note="Manual"):
        src = f"{self.rule_dir}/{program_name}.xlsx"
        if not os.path.exists(src): return
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{program_name}_{timestamp}_{note}.xlsx"
        dst = f"{self.snapshot_dir}/{filename}"
        shutil.copy2(src, dst)
        print(f"{Fore.GREEN}   -> Snapshot created: {filename}{Style.RESET_ALL}")

    def list_snapshots(self, program_name):
        pattern = f"{self.snapshot_dir}/{program_name}_*.xlsx"
        files = glob.glob(pattern)
        files.sort(key=os.path.getmtime, reverse=True)
        return [os.path.basename(f) for f in files]

    def restore_snapshot(self, program_name, snapshot_filename):
        src = f"{self.snapshot_dir}/{snapshot_filename}"
        dst = f"{self.rule_dir}/{program_name}.xlsx"
        if not os.path.exists(src): return
        
        self.create_snapshot(program_name, "PRE_RESTORE_BACKUP")
        shutil.copy2(src, dst)
        print(f"{Fore.GREEN}   -> Restored {program_name} from {snapshot_filename}{Style.RESET_ALL}")

    def view_history(self, program_name):
        path = f"{self.rule_dir}/{program_name}.xlsx"
        if not os.path.exists(path):
            print(f"{Fore.RED}Rule file not found.{Style.RESET_ALL}"); return

        try:
            df = pd.read_excel(path, sheet_name='_Audit_Log')
            if df.empty:
                print(f"{Fore.YELLOW}No history found.{Style.RESET_ALL}"); return

            print(f"\n{Fore.CYAN}--- RULE HISTORY: {program_name} ---{Style.RESET_ALL}")
            print(f"{'TIMESTAMP':<20} | {'USER':<10} | {'ACTION':<6} | {'TARGET':<10} | {'DETAILS'}")
            print("-" * 90)

            for _, row in df.iterrows():
                ts = str(row.get('TIMESTAMP', ''))[:19]
                user = str(row.get('USER', ''))[:10]
                action = str(row.get('ACTION', ''))[:6]
                target = str(row.get('TARGET_FIELD', ''))[:10]
                
                old_v = str(row.get('OLD_VALUE', '')).strip()
                new_v = str(row.get('NEW_VALUE', '')).strip()
                
                if action == 'EDIT': details = f"{old_v} -> {new_v}"
                elif action == 'ADD': details = "New Rule"
                else: details = new_v

                print(f"{ts:<20} | {user:<10} | {action:<6} | {target:<10} | {details[:40]}")
            print("-" * 90)
        except Exception as e:
            print(f"{Fore.RED}Error reading history: {e}{Style.RESET_ALL}")

    def commit_changes(self, program_name):
        self.create_snapshot(program_name, "Auto_Commit")
        current_path = f"{self.rule_dir}/{program_name}.xlsx"
        history_path = f"{self.history_dir}/{program_name}.xlsx"
        if not os.path.exists(current_path): return

        print(f"Scanning for changes in {program_name}...")
        try: df_curr = pd.read_excel(current_path, sheet_name='Rules').fillna('').astype(str)
        except: return

        # Deduplicate to prevent false flags if MCO had dupes
        df_curr = df_curr.drop_duplicates(subset=['TARGET_FIELD'], keep='last')

        changes = []
        if os.path.exists(history_path):
            df_prev = pd.read_excel(history_path, sheet_name='Rules').fillna('').astype(str)
            df_prev = df_prev.drop_duplicates(subset=['TARGET_FIELD'], keep='last')
            
            merged = pd.merge(df_curr, df_prev, on='TARGET_FIELD', how='outer', suffixes=('_NEW', '_OLD'), indicator=True)
            user = getpass.getuser()
            ts = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            for idx, row in merged.iterrows():
                target = row['TARGET_FIELD']
                if row['_merge'] == 'left_only': changes.append([ts, user, 'ADD', target, '', 'Created'])
                elif row['_merge'] == 'right_only': changes.append([ts, user, 'DELETE', target, 'Existed', 'Deleted'])
                elif row['_merge'] == 'both':
                    for col in ['SOURCE_FIELD', 'RULE_TYPE', 'RULE_VALUE', 'SCOPE']:
                        c_new, c_old = f"{col}_NEW", f"{col}_OLD"
                        if row[c_new] != row[c_old]: changes.append([ts, user, 'EDIT', target, row[c_old], row[c_new]])
        else:
            changes.append([datetime.datetime.now(), getpass.getuser(), 'INIT', 'ALL', '', 'Initial Commit'])

        if changes:
            print(f"   -> Detected {len(changes)} changes.")
            with pd.ExcelWriter(current_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:
                try: df_old = pd.read_excel(current_path, sheet_name='_Audit_Log')
                except: df_old = pd.DataFrame(columns=['TIMESTAMP', 'USER', 'ACTION', 'TARGET_FIELD', 'OLD_VALUE', 'NEW_VALUE'])
                
                df_new = pd.DataFrame(changes, columns=['TIMESTAMP', 'USER', 'ACTION', 'TARGET_FIELD', 'OLD_VALUE', 'NEW_VALUE'])
                if not df_new.empty:
                    pd.concat([df_old, df_new], ignore_index=True).to_excel(writer, sheet_name='_Audit_Log', index=False)
        else:
            print("   -> No changes detected.")
        shutil.copy2(current_path, history_path)

    def hard_reset(self):
        """
        DANGER: Deletes all snapshots, history, and clears Audit Logs from Excel files.
        """
        print(f"{Fore.YELLOW}   Cleaning .history and .snapshots folders...{Style.RESET_ALL}")
        for folder in [self.history_dir, self.snapshot_dir]:
            files = glob.glob(os.path.join(folder, "*"))
            for f in files:
                try: os.remove(f)
                except: pass
        
        print(f"{Fore.YELLOW}   Resetting Audit Log sheets in Excel rules...{Style.RESET_ALL}")
        rule_files = glob.glob(os.path.join(self.rule_dir, "*.xlsx"))
        
        empty_log = pd.DataFrame(columns=['TIMESTAMP', 'USER', 'ACTION', 'TARGET_FIELD', 'OLD_VALUE', 'NEW_VALUE'])
        
        for f in rule_files:
            try:
                # Use openpyxl replace mode to overwrite just the specific sheet
                with pd.ExcelWriter(f, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
                    empty_log.to_excel(writer, sheet_name='_Audit_Log', index=False)
                print(f"   -> Cleaned: {os.path.basename(f)}")
            except Exception as e:
                print(f"{Fore.RED}   -> Failed to clean {os.path.basename(f)}: {e}{Style.RESET_ALL}")
--- END FILE: .\modules\audit_manager.py ---

--- START FILE: .\modules\auto_detector.py ---
import pandas as pd
import re
import os
from colorama import Fore, Style

class AutoDetector:
    def __init__(self, mco_path):
        self.mco_path = mco_path
        self.prefix_map = {}  # {'MM': {'sheet': 'Item Master', 'api': 'MMS200MI'}}
        self.is_learned = False

    def learn_signatures(self):
        """
        Scans the MCO file to map 2-char Prefixes -> MCO Sheets -> API Names.
        """
        print(f"{Fore.CYAN}   Analyzing MCO for Field Signatures...{Style.RESET_ALL}")
        
        try:
            xls = pd.ExcelFile(self.mco_path)
            learned_list = []
            
            for sheet in xls.sheet_names:
                # 1. READ HEADER (Try Row 3 first, then scan)
                try:
                    df = pd.read_excel(xls, sheet_name=sheet, header=2, nrows=5)
                except Exception:
                    continue

                # 2. VALIDATE IF MCO SHEET
                cols_upper = [str(c).upper() for c in df.columns]
                
                # Check for Target and Source columns (handling typos)
                has_target = any('FIELD NAME' in c or 'M3 FIELD' in c for c in cols_upper)
                has_source = any('SOURCE' in c or 'CONVERISON' in c or 'CONVERSION' in c for c in cols_upper)
                
                if not (has_target and has_source):
                    continue

                # 3. EXTRACT PREFIX from Data Converison Source / Source column
                src_col_idx = next(
                    (i for i, c in enumerate(cols_upper) if 'SOURCE' in c or 'CONVERISON' in c or 'CONVERSION' in c),
                    None
                )
                if src_col_idx is None:
                    continue
                
                # Read full column
                df_full = pd.read_excel(xls, sheet_name=sheet, header=2, usecols=[src_col_idx])
                sample_values = df_full.iloc[:, 0].dropna().astype(str).tolist()
                
                prefix = None
                for val in sample_values:
                    clean_val = val.strip().upper()
                    # Accept the first 2 *alphanumeric* characters as the prefix
                    # This allows 'MMDIM1', 'OBV1', 'M9FACI', etc.
                    if len(clean_val) >= 2 and clean_val[:2].isalnum():
                        prefix = clean_val[:2]
                        break
                
                if not prefix:
                    # Optional debug: we had a source column but no usable prefix
                    print(
                        f"{Fore.YELLOW}      [AutoDetector] No usable 2-char prefix found in "
                        f"source column for sheet '{sheet}'. Sample values: "
                        f"{sample_values[:5]}{Style.RESET_ALL}"
                    )
                    continue

                # 4. GUESS API NAME
                api_name = "Unknown"
                
                # Try to find API Name in columns
                api_col_idx = next((i for i, c in enumerate(cols_upper) if 'API' in c), None)
                if api_col_idx is not None:
                    df_api = pd.read_excel(xls, sheet_name=sheet, header=2, usecols=[api_col_idx], nrows=10)
                    for val in df_api.iloc[:, 0].dropna().astype(str):
                        match = re.search(r'([A-Z]{3}\d{3}MI)', val, re.IGNORECASE)
                        if match:
                            api_name = match.group(1).upper()
                            break
                
                # Store Logic
                self.prefix_map[prefix] = {
                    'sheet': sheet,
                    'api': api_name
                }
                learned_list.append(f"{prefix} -> {sheet}")

            self.is_learned = True
            
            # VERBOSE OUTPUT
            print(f"{Fore.GREEN}   -> Learned signatures for {len(self.prefix_map)} Business Objects:{Style.RESET_ALL}")
            # Print in 3 columns to save space
            for i in range(0, len(learned_list), 3):
                print("      " + "   |   ".join(learned_list[i:i+3]))

        except Exception as e:
            print(f"{Fore.RED}Error learning MCO signatures: {e}{Style.RESET_ALL}")

    def identify_file(self, file_path):
        """
        Reads a Movex Excel file and returns the matching MCO info.
        """
        if not self.is_learned:
            self.learn_signatures()

        try:
            # Read first few rows to find headers
            df = pd.read_excel(file_path, nrows=10, header=None)
            
            header_row = None
            header_idx = 0
            
            # Heuristic Hunt
            for idx, row in df.iterrows():
                row_str = " ".join([str(x).upper() for x in row.values])
                # Look for common Movex fields
                if "ITNO" in row_str or "CUNO" in row_str or "SUNO" in row_str or "CONO" in row_str:
                    header_row = row
                    header_idx = idx
                    break
            
            if header_row is None:
                print(f"{Fore.YELLOW}   Warning: Could not detect standard Movex headers (ITNO/CUNO). Using Row 1.{Style.RESET_ALL}")
                header_row = df.iloc[0]

            print(f"   -> Scanning File Headers (Row {header_idx+1})...")

            # Analyze Headers
            for col in header_row.values:
                val = str(col).strip().upper()
                if len(val) >= 2:
                    p = val[:2]  # Extract 'MM' from 'MMITNO', 'M9' from 'M9FACI', etc.
                    
                    if p in self.prefix_map:
                        info = self.prefix_map[p]
                        print(f"      Matched Prefix '{p}' -> {info['sheet']}")
                        return p, info['sheet'], info['api']
            
            # Debug: Show what we checked against
            print(f"{Fore.RED}   No matching prefix found in headers.{Style.RESET_ALL}")
            print(f"   Checked against: {list(self.prefix_map.keys())}")
            return None, None, None

        except Exception as e:
            print(f"{Fore.RED}Error analyzing file: {e}{Style.RESET_ALL}")
            return None, None, None

--- END FILE: .\modules\auto_detector.py ---

--- START FILE: .\modules\batch_processor.py ---
import pandas as pd
import os
import datetime
from colorama import Fore, Style
from modules.config_loader import ConfigLoader
from modules.extractor import DataExtractor
from modules.sdt_writer import SDTWriter

class BatchProcessor:
    def __init__(self):
        self.output_dir = 'output'
        if not os.path.exists(self.output_dir): os.makedirs(self.output_dir)

    def load_batch_file(self, batch_file_path):
        """
        Reads the batch Excel file and returns a clean DataFrame.
        """
        try:
            df_batch = pd.read_excel(batch_file_path)
            # Clean headers
            df_batch.columns = [str(c).strip().upper() for c in df_batch.columns]
            return df_batch
        except Exception as e:
            print(f"{Fore.RED}Critical Error reading batch file: {e}{Style.RESET_ALL}")
            return None

    def _resolve_path(self, user_path, default_folder):
        """
        Smartly tries to find a file. 
        1. Checks exact path provided.
        2. Checks inside default_folder.
        """
        # Case 1: Exact Path
        if os.path.exists(user_path):
            return user_path
        
        # Case 2: Inside Default Folder
        name_only = os.path.basename(user_path)
        common_path = os.path.join(default_folder, name_only)
        if os.path.exists(common_path):
            return common_path
            
        return None # Not found

    def run_batch_execution(self, df_batch, force_run=False):
        """
        Executes the jobs in the provided DataFrame.
        """
        success_count = 0
        fail_count = 0
        log_report = []

        print(f"\n{Fore.CYAN}--- EXECUTING {len(df_batch)} BATCH JOBS ---{Style.RESET_ALL}")

        for idx, row in df_batch.iterrows():
            job_id = str(row.get('JOB_ID', f'ROW_{idx+1}'))
            enabled = str(row.get('ENABLED', 'N')).strip().upper()
            
            if not force_run and enabled != 'Y':
                print(f"   {Fore.CYAN}[SKIP]{Style.RESET_ALL} Job {job_id} (Disabled)")
                continue

            print(f"\n{Fore.YELLOW}>>> JOB {job_id}{Style.RESET_ALL}")
            
            try:
                # 1. Parse Parameters
                rule_config = str(row.get('RULE_CONFIG', '')).strip().replace('.xlsx', '')
                raw_source = str(row.get('SOURCE_PATH', '')).strip()
                raw_sdt    = str(row.get('SDT_TEMPLATE', '')).strip()
                scope       = str(row.get('SCOPE', 'GLOBAL')).strip()
                out_prefix  = str(row.get('OUTPUT_PREFIX', 'BATCH')).strip()
                
                raw_trans = str(row.get('TRANSACTIONS', '')).strip()
                target_sheets = [t.strip() for t in raw_trans.split(',') if t.strip()]

                # 2. Smart Path Validation
                source_path = self._resolve_path(raw_source, 'raw_data')
                sdt_path = self._resolve_path(raw_sdt, 'config/sdt_templates')

                if not source_path: 
                    raise FileNotFoundError(f"Source file not found: {raw_source} (Checked root and raw_data/)")
                if not sdt_path: 
                    raise FileNotFoundError(f"Template file not found: {raw_sdt} (Checked root and config/sdt_templates/)")
                if not target_sheets: 
                    raise ValueError("No Transactions listed.")

                # 3. Execute Pipeline
                print(f"      Rule Config: {rule_config} | Scope: {scope}")
                print(f"      Template: {os.path.basename(sdt_path)}")
                
                # A. Load Rules
                config_loader = ConfigLoader(rule_config)
                rules, lookups = config_loader.load_config(division_code=scope)
                if rules.empty: raise ValueError(f"Rules for {rule_config} are empty.")

                # B. Load Source
                print(f"      Source Data: {os.path.basename(source_path)}")
                extractor = DataExtractor()
                df_legacy = extractor.load_data(source_path, format_type='MOVEX', sheet_name=0)

                # C. Generate Output
                print(f"      Generating SDT...")
                writer = SDTWriter(self.output_dir)
                out_filename = f"{out_prefix}_{rule_config}.xlsx"
                
                writer.generate_from_template(sdt_path, df_legacy, rules, target_sheets, out_filename)
                
                print(f"{Fore.GREEN}      [SUCCESS] Job {job_id} Finished.{Style.RESET_ALL}")
                success_count += 1
                log_report.append({'Job': job_id, 'Status': 'SUCCESS', 'File': out_filename})

            except Exception as e:
                print(f"{Fore.RED}      [FAILED] Job {job_id}: {e}{Style.RESET_ALL}")
                fail_count += 1
                log_report.append({'Job': job_id, 'Status': 'FAILED', 'Error': str(e)})

        # Summary
        print(f"\n{Fore.CYAN}--- BATCH RUN COMPLETE ---{Style.RESET_ALL}")
        print(f"Success: {Fore.GREEN}{success_count}{Style.RESET_ALL} | Failed: {Fore.RED}{fail_count}{Style.RESET_ALL}")
        
        # Save Log
        log_path = f"{self.output_dir}/Batch_Log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        pd.DataFrame(log_report).to_csv(log_path, index=False)
        print(f"Log saved to: {log_path}")
--- END FILE: .\modules\batch_processor.py ---

--- START FILE: .\modules\config_loader.py ---
import pandas as pd
import os

class ConfigLoader:
    def __init__(self, program_name, rule_dir='config/rules'):
        self.program_name = program_name
        self.rule_dir = rule_dir
        self.file_path = f"{rule_dir}/{program_name}.xlsx"
        self.rules_raw = None
        self.lookups = {}

    def load_config(self, division_code='GLOBAL'):
        if not os.path.exists(self.file_path):
             return pd.DataFrame(), {}
        
        print(f"Loading configuration for {self.program_name} (Scope: {division_code})...")
        
        try:
            df = pd.read_excel(self.file_path, sheet_name='Rules')
            df.columns = [c.upper().strip() for c in df.columns]
            
            if 'SCOPE' not in df.columns:
                df['SCOPE'] = 'GLOBAL'
            
            mask = (df['SCOPE'] == 'GLOBAL') | (df['SCOPE'] == division_code)
            df_filtered = df[mask].copy()
            
            df_filtered['SCOPE_SCORE'] = df_filtered['SCOPE'].apply(lambda x: 1 if x == 'GLOBAL' else 2)
            df_filtered = df_filtered.sort_values('SCOPE_SCORE', ascending=False)
            
            final_rules = df_filtered.drop_duplicates(subset=['TARGET_FIELD'], keep='first')
            self.rules_raw = final_rules.drop(columns=['SCOPE_SCORE'])
            
            # FIX: Ensure columns exist even if empty
            if self.rules_raw.empty:
                self.rules_raw = pd.DataFrame(columns=['TARGET_FIELD', 'SOURCE_FIELD', 'RULE_TYPE', 'RULE_VALUE', 'SCOPE'])

            xls = pd.ExcelFile(self.file_path)
            for sheet in xls.sheet_names:
                if sheet not in ['Rules', '_Audit_Log']:
                    df_lookup = pd.read_excel(xls, sheet_name=sheet, dtype=str)
                    if len(df_lookup.columns) >= 2:
                        key_col = df_lookup.columns[0]
                        val_col = df_lookup.columns[1]
                        self.lookups[sheet] = dict(zip(df_lookup[key_col], df_lookup[val_col]))

            return self.rules_raw, self.lookups

        except Exception as e:
            print(f"Config Load Error: {e}")
            return pd.DataFrame(), {}

    def get_existing_targets(self):
        if os.path.exists(self.file_path):
            try:
                df = pd.read_excel(self.file_path, sheet_name='Rules')
                return df['TARGET_FIELD'].unique().tolist()
            except: return []
        return []
--- END FILE: .\modules\config_loader.py ---

--- START FILE: .\modules\extractor.py ---
import pandas as pd
import os

class DataExtractor:
    def __init__(self):
        pass

    def load_data(self, file_path, format_type='MOVEX', sheet_name=0):
        """
        Reads Excel files based on specific ERP formats.
        
        Args:
            file_path (str): Path to the Excel file.
            format_type (str): 'MOVEX' or 'M3_SDT'.
            sheet_name (str/int): The sheet to read. Default is first sheet (0).
        
        Returns:
            pd.DataFrame: Cleaned dataframe with headers normalized.
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")

        print(f"Reading {format_type} file: {file_path}...")

        try:
            if format_type == 'MOVEX':
                # Movex: Headers on Row 1 (Index 0), Data starts Row 2
                df = pd.read_excel(file_path, sheet_name=sheet_name, header=0)
                
            elif format_type == 'M3_SDT':
                # M3 SDT: Headers on Row 1 (Index 0).
                # Rows 2 & 3 are blank/descriptions. Data starts Row 4 (Index 3).
                # We read the header separately, then the data, skipping the gap.
                
                # 1. Get Headers from Row 1
                df_headers = pd.read_excel(file_path, sheet_name=sheet_name, header=0, nrows=0)
                columns = df_headers.columns.tolist()
                
                # 2. Read Data starting at Row 4 (skipping rows 1, 2, 3)
                # Note: 'header=None' because we supplied cols manually, 'skiprows=3' skips the gap
                df = pd.read_excel(file_path, sheet_name=sheet_name, header=None, skiprows=3)
                
                # 3. Apply columns (Ensure count matches)
                if len(df.columns) == len(columns):
                    df.columns = columns
                else:
                    # Fallback if columns don't align (common in dirty SDT files)
                    print("Warning: Column count mismatch. Using first N columns.")
                    df = df.iloc[:, :len(columns)]
                    df.columns = columns

            # Standardization: Strip whitespace from column names and cast to string
            df.columns = [str(c).strip().upper() for c in df.columns]
            
            print(f"   -> Loaded {len(df)} rows.")
            return df

        except Exception as e:
            raise ValueError(f"Error extracting data: {e}")

# Usage Example:
# extractor = DataExtractor()
# df_legacy = extractor.load_data('raw_data/MMS001_Movex.xlsx', 'MOVEX')
# df_gold = extractor.load_data('raw_data/MMS001_Gold_Copy.xlsx', 'M3_SDT', sheet_name='MMS001MI')
--- END FILE: .\modules\extractor.py ---

--- START FILE: .\modules\hooks.py ---
import pandas as pd
import datetime

# --- HELPERS ---
def _get_val(row, col_name):
    """Safe value retrieval helper."""
    # Try exact match first
    if col_name in row: return row[col_name]
    # Try case insensitive
    for c in row.index:
        if c.upper() == col_name.upper(): return row[c]
    return None

# --- CUSTOM HOOKS ---

def format_date_yymmdd(row, source_col):
    """
    Converts YYYYMMDD or YYYY-MM-DD to YYMMDD.
    """
    val = str(_get_val(row, source_col))
    val = val.replace('-', '').replace('/', '').replace('.', '')
    if len(val) == 8:
        return val[2:] # 20231027 -> 231027
    return val

def math_calculate_volume(row, source_col):
    """
    Example Math: Calculates Vol = Height * Width * Depth
    Assumes columns MMHEIG, MMWIDT, MMDEPT exist in source.
    """
    try:
        h = float(_get_val(row, 'MMHEIG') or 0)
        w = float(_get_val(row, 'MMWIDT') or 0)
        d = float(_get_val(row, 'MMDEPT') or 0)
        return str(h * w * d)
    except:
        return "0"

def logic_item_status(row, source_col):
    """
    Example Case Statement:
    If Group is 900 -> Status 20
    If Group is 999 -> Status 50
    Else -> Status 10
    """
    group = str(_get_val(row, 'MMITGR'))
    
    if group == '900':
        return '20'
    elif group == '999':
        return '50'
    else:
        return '10'

def concat_description(row, source_col):
    """
    Combines Name 1 + Name 2
    """
    d1 = str(_get_val(row, 'MMITDS') or "").strip()
    d2 = str(_get_val(row, 'MMFUDS') or "").strip()
    return f"{d1} {d2}".strip()
--- END FILE: .\modules\hooks.py ---

--- START FILE: .\modules\mco_checker.py ---
import pandas as pd
import os
from colorama import Fore, Style

class MCOChecker:
    def __init__(self):
        pass

    def check_file(self, mco_path):
        if not os.path.exists(mco_path):
            print(f"{Fore.RED}File not found: {mco_path}{Style.RESET_ALL}")
            return

        print(f"\n{Fore.CYAN}--- ANALYZING MCO HEALTH: {os.path.basename(mco_path)} ---{Style.RESET_ALL}")
        
        try:
            xls = pd.ExcelFile(mco_path)
        except Exception as e:
            print(f"{Fore.RED}Critical Error: Cannot open Excel file. {e}{Style.RESET_ALL}")
            return

        total_issues = 0
        
        for sheet in xls.sheet_names:
            issues = self._analyze_sheet(mco_path, sheet)
            if issues:
                total_issues += len(issues)
                print(f"\n{Fore.YELLOW}SHEET: {sheet}{Style.RESET_ALL}")
                for issue in issues:
                    color = Fore.RED if "CRITICAL" in issue else Fore.YELLOW
                    print(f"   {color}{issue}{Style.RESET_ALL}")
        
        if total_issues == 0:
            print(f"\n{Fore.GREEN}RESULT: HEALTHY. No obvious structural issues found.{Style.RESET_ALL}")
        else:
            print(f"\n{Fore.RED}RESULT: {total_issues} ISSUES FOUND. Please review above.{Style.RESET_ALL}")

    def _analyze_sheet(self, path, sheet_name):
        issues = []
        
        try:
            # Read raw to find headers
            df_raw = pd.read_excel(path, sheet_name=sheet_name, header=None, nrows=15)
            
            header_idx = -1
            keywords = ['FIELD NAME', 'M3 FIELD', 'TECHNICAL NAME']
            for idx, row in df_raw.iterrows():
                row_str = " ".join([str(x).upper() for x in row.values])
                if any(k in row_str for k in keywords):
                    header_idx = idx
                    break
            
            if header_idx == -1:
                return [] # Skip non-spec sheets silently

            # Read Data
            df = pd.read_excel(path, sheet_name=sheet_name, header=header_idx)
            df.columns = [str(c).strip().replace('\n', ' ').replace('_', ' ').upper() for c in df.columns]

            # Columns
            target_aliases = ['FIELD NAME', 'M3 FIELD', 'TECHNICAL NAME']
            col_target = next((c for c in df.columns if any(a in c for a in target_aliases)), None)
            col_req = next((c for c in df.columns if 'CUSTOMER REQUIRED' in c or 'REQUIRED' in c), None)
            col_source = next((c for c in df.columns if 'CONVERSION SOURCE' in c or 'SOURCE' in c), None)
            col_logic = next((c for c in df.columns if 'TRANSFORMATION RULE' in c or 'LOGIC' in c), None)

            if not col_target: return ["CRITICAL: Missing 'M3 Field' column."]

            seen_targets = set()
            
            for idx, row in df.iterrows():
                excel_row = idx + header_idx + 2
                tgt = str(row.get(col_target, '')).strip().upper()
                
                if not tgt or tgt == 'NAN': continue
                if len(tgt) == 6: tgt = tgt[2:] # Normalize for dup check
                
                if tgt in seen_targets:
                    issues.append(f"Row {excel_row}: Duplicate Rule for Target '{tgt}'")
                seen_targets.add(tgt)

                if col_req:
                    req = str(row.get(col_req, '0')).strip().upper()
                    if req.startswith('1') or req.startswith('Y'):
                        src = str(row.get(col_source, '')).strip().replace('nan', '') if col_source else ""
                        logic = str(row.get(col_logic, '')).strip().replace('nan', '') if col_logic else ""
                        
                        if not src and not logic:
                             issues.append(f"Row {excel_row}: Field '{tgt}' is REQUIRED but has no Source/Logic.")

        except Exception as e:
            return [f"CRITICAL: Cannot read sheet. Error: {e}"]

        return issues
--- END FILE: .\modules\mco_checker.py ---

--- START FILE: .\modules\mco_importer.py ---
import pandas as pd
import os
import glob
from colorama import Fore, Style
import time
from modules.audit_manager import AuditManager

class MCOImporter:
    def __init__(self, sdt_folder='config/sdt_templates'):
        self.sdt_folder = sdt_folder

    # --- GUI HELPERS ---
    def get_sheet_names(self, mco_path):
        try: return pd.ExcelFile(mco_path).sheet_names
        except: return []

    def run_import_headless(self, mco_path, selected_sheet, api_name, output_dir='config/rules'):
        try:
            print(f"Loading MCO Sheet: {selected_sheet}...")
            df_mco = self._find_header_row(mco_path, selected_sheet)
            self._generate_master_rule_file(df_mco, api_name, output_dir)
            return True
        except Exception as e:
            print(f"{Fore.RED}Import Error: {e}{Style.RESET_ALL}")
            return False

    # --- CLI HELPERS ---
    def _smart_pick(self, options, title_prompt):
        filtered_indices = list(range(len(options)))
        filter_text = ""
        while True:
            print(f"\n{Fore.CYAN}--- {title_prompt} ---{Style.RESET_ALL}")
            if filter_text: print(f"{Fore.YELLOW}[Filter: '{filter_text}'] (Type 'all' to clear){Style.RESET_ALL}")
            limit = 20; count = 0; display_map = {}
            for i in filtered_indices:
                count += 1
                if count > limit:
                    print(f"   ... ({len(filtered_indices) - limit} more matches)")
                    break
                print(f"   {count}. {options[i]}")
                display_map[count] = i
            
            print(f"\n{Fore.GREEN}Type a Number to select, or Text to filter.{Style.RESET_ALL}")
            user_input = input(f"{Fore.CYAN}>> Selection: {Style.RESET_ALL}").strip()
            if not user_input: continue
            
            if user_input.isdigit():
                choice = int(user_input)
                if choice in display_map: return options[display_map[choice]]
            elif user_input.lower() in ['all', 'clear']: 
                filter_text = ""; filtered_indices = list(range(len(options)))
            else:
                filter_text = user_input
                filtered_indices = [i for i, opt in enumerate(options) if filter_text.lower() in str(opt).lower()]
                if not filtered_indices: 
                    print(f"{Fore.RED}No matches found.{Style.RESET_ALL}")
                    time.sleep(0.5); filter_text = ""; filtered_indices = list(range(len(options)))

    def interactive_import(self, mco_path, output_dir='config/rules'):
        try:
            xls = pd.ExcelFile(mco_path)
            mco_sheets = xls.sheet_names
            selected_mco_sheet = self._smart_pick(mco_sheets, "Select MCO Sheet")
            df_mco = self._find_header_row(mco_path, selected_mco_sheet)
            default_name = selected_mco_sheet.split(' ')[0] + "MI"
            api_name = input(f"{Fore.CYAN}   >> Name this Rule Set (Default: {default_name}): {Style.RESET_ALL}").strip().upper()
            if not api_name: api_name = default_name.upper()
            self._generate_master_rule_file(df_mco, api_name, output_dir)
        except Exception as e:
            print(f"{Fore.RED}Error during import: {e}{Style.RESET_ALL}")
            import traceback; traceback.print_exc()

    # --- CORE LOGIC ---
    def _find_header_row(self, file_path, sheet_name):
        df_raw = pd.read_excel(file_path, sheet_name=sheet_name, header=None, nrows=15)
        header_idx = -1
        keywords = ['FIELD NAME', 'M3 FIELD', 'TECHNICAL NAME']
        for idx, row in df_raw.iterrows():
            row_str = " ".join([str(x).upper() for x in row.values])
            if any(k in row_str for k in keywords): header_idx = idx; break
        if header_idx == -1: header_idx = 2 
        print(f"      -> Detected headers on Row {header_idx + 1}")
        df = pd.read_excel(file_path, sheet_name=sheet_name, header=header_idx)
        df.columns = [str(c).strip().replace('\n', ' ').replace('_', ' ').upper() for c in df.columns]
        return df

    def _generate_master_rule_file(self, df_mco, api_name, output_dir):
        if not os.path.exists(output_dir): os.makedirs(output_dir)
        target_path = f"{output_dir}/{api_name}.xlsx"
        
        existing_rules = pd.DataFrame()
        if os.path.exists(target_path):
            print(f"{Fore.YELLOW}   Merging into existing rule file...{Style.RESET_ALL}")
            audit_root = os.path.dirname(output_dir) if 'rules' in output_dir else output_dir
            auditor = AuditManager(output_dir) 
            auditor.create_snapshot(api_name, "AUTO_PRE_MERGE")
            try:
                existing_rules = pd.read_excel(target_path, sheet_name='Rules')
            except: pass

        print(f"   -> Parsing MCO content...")
        cols = df_mco.columns
        target_aliases = ['FIELD NAME', 'M3 FIELD', 'TECHNICAL NAME']
        col_target = next((c for c in cols if any(a in c for a in target_aliases)), None)
        col_req    = next((c for c in cols if 'CUSTOMER REQUIRED' in c or 'REQUIRED' in c), None)
        col_source = next((c for c in cols if 'CONVERSION SOURCE' in c or 'SOURCE' in c or 'LEGACY' in c), None)
        col_logic  = next((c for c in cols if 'TRANSFORMATION RULE' in c or 'LOGIC' in c or 'RULE' in c), None)

        if not col_target: print(f"{Fore.RED}      CRITICAL: Could not find Target Column.{Style.RESET_ALL}"); return

        new_rules = []
        for _, row in df_mco.iterrows():
            tgt = str(row.get(col_target, '')).strip().upper()
            if not tgt or tgt == 'NAN': continue
            if len(tgt) == 6: tgt = tgt[2:] # Prefix Strip

            raw_src = str(row.get(col_source, '')).strip().replace('nan', '').upper()
            if len(raw_src) == 6 and raw_src[:2].isalpha(): raw_src = raw_src[2:]

            raw_req = str(row.get(col_req, '0')).strip().replace('nan', '0')
            raw_logic = str(row.get(col_logic, '')).strip().replace('nan', '')
            
            r_type, r_val, r_src, desc = 'IGNORE', '', '', 'Imported'
            if raw_src:
                r_type = 'DIRECT'; r_src = raw_src; desc = f"Mapped from {raw_src}"
            elif raw_req == '1' or raw_req.upper().startswith('Y'):
                if 'CONST' in raw_logic.upper() or 'FIXED' in raw_logic.upper():
                        r_type = 'CONST'; desc = f"Required Constant: {raw_logic}"
                else:
                    r_type = 'TODO'; desc = f"Required! Logic: {raw_logic}"
            else:
                r_type = 'IGNORE'; desc = "MCO listed but not required"
            
            new_rules.append({'TARGET_API': api_name, 'TARGET_FIELD': tgt, 'SOURCE_FIELD': r_src, 'RULE_TYPE': r_type, 'RULE_VALUE': r_val, 'SCOPE': 'GLOBAL', 'DESCRIPTION': desc})

        df_new = pd.DataFrame(new_rules)

        # --- 4. MERGE & DEDUPLICATE (CRITICAL FIX) ---
        if not existing_rules.empty:
            combined = pd.concat([existing_rules, df_new])
            # Use Last to ensure new MCO overwrites old, or subsequent MCO rows overwrite previous rows
            final_rules = combined.drop_duplicates(subset=['TARGET_FIELD'], keep='last')
        else:
            # FIX: Deduplicate even if it's a new file! 
            final_rules = df_new.drop_duplicates(subset=['TARGET_FIELD'], keep='last')

        def sorter(x):
            if x == 'TODO': return 0
            if x in ['DIRECT', 'CONST', 'MAP']: return 1
            return 2
        final_rules['Sort'] = final_rules['RULE_TYPE'].apply(sorter)
        final_rules = final_rules.sort_values(['Sort', 'TARGET_FIELD']).drop(columns=['Sort'])

        with pd.ExcelWriter(target_path, engine='xlsxwriter') as writer:
            final_rules.to_excel(writer, sheet_name='Rules', index=False)
            pd.DataFrame(columns=['TIMESTAMP']).to_excel(writer, sheet_name='_Audit_Log', index=False)
            
        print(f"      -> Updated Config: {target_path} (Total Fields: {len(final_rules)})")
--- END FILE: .\modules\mco_importer.py ---

--- START FILE: .\modules\migration_runner.py ---
import pandas as pd
import os
import glob
from colorama import Fore, Style
from modules.config_loader import ConfigLoader
from modules.extractor import DataExtractor
from modules.sdt_writer import SDTWriter
import modules.ui as ui

class MigrationRunner:
    def __init__(self):
        self.output_dir = 'output'
        ui.ensure_folder(self.output_dir)

    def _resolve_from_map(self, lookup_val, lookup_col):
        """
        Looks up API and SDT info from config/migration_map.csv.
        Returns: (api_name, sdt_template, [list_of_sheets])
        """
        map_path = 'config/migration_map.csv'
        if not os.path.exists(map_path): return None, None, None
        
        try:
            df = pd.read_csv(map_path)
            # Normalize headers
            df.columns = [c.upper().strip() for c in df.columns]
            
            # Case-Insensitive Lookup
            lookup_val = str(lookup_val).strip().upper()
            # Create a normalized column for searching
            df['LOOKUP_NORM'] = df[lookup_col].astype(str).str.strip().str.upper()
            
            match = df[df['LOOKUP_NORM'] == lookup_val]
            if match.empty: return None, None, None
            
            row = match.iloc[0]
            api = row.get('API_NAME')
            sdt = row.get('SDT_TEMPLATE')
            
            # Parse Transaction Sheets (Handle commas)
            raw_trans = str(row.get('TRANSACTION_SHEET', ''))
            sheets = None
            if raw_trans and raw_trans.lower() != 'nan':
                # Split by comma and strip whitespace
                sheets = [t.strip() for t in raw_trans.split(',') if t.strip()]
                
            return api, sdt, sheets

        except Exception as e: 
            print(f"{Fore.RED}Map Error: {e}{Style.RESET_ALL}")
            return None, None, None

    def execute_migration(self, program_name, legacy_path, auto_sdt=None, division='GLOBAL', target_sheets=None, silent=False, output_name_override=None):
        """
        Core logic to run a migration task.
        """
        clean_name = program_name.replace('API_', '')
        sdt_path = auto_sdt

        # 1. Resolve SDT Template Path
        if not sdt_path:
            # Try Heuristic: config/sdt_templates/MMS200MI_API.xlsx
            potential = os.path.join('config/sdt_templates', f"{clean_name}_API.xlsx")
            if os.path.exists(potential): sdt_path = potential
        
        # If still not found and interactive, ask user
        if not sdt_path and not silent:
            sdt_path = ui.select_file("STEP 3: SELECT M3 SDT TEMPLATE", [("Excel files", "*.xlsx")])
        
        if not sdt_path: 
            if not silent: print(f"{Fore.RED}SDT Template not found.{Style.RESET_ALL}")
            return

        # 2. Select Transaction Sheets
        # If target_sheets passed (e.g. from Surgical Map), use them.
        if not target_sheets:
            try:
                xls = pd.ExcelFile(sdt_path, engine='openpyxl')
                sheets = [s for s in xls.sheet_names if "MI" in s and "LST" not in s.upper()]
                
                if silent:
                    # If silent and no target defined, default to ALL VALID SHEETS
                    target_sheets = sheets 
                else:
                    # Interactive Picker
                    target_sheets = ui.interactive_list_picker(sheets, f"Select Transactions", multi=True)
            except Exception as e:
                print(f"{Fore.RED}Error reading SDT template: {e}{Style.RESET_ALL}")
                return
        
        if not target_sheets: return

        # 3. Process Migration
        try:
            if not silent: print(f"\n{Fore.CYAN}--- STARTING MIGRATION ---{Style.RESET_ALL}")
            
            # Load Rules
            config_loader = ConfigLoader(program_name)
            rules, lookups = config_loader.load_config(division_code=division)
            
            # Load Source Data
            extractor = DataExtractor()
            # For Batch/Surgical, we assume data is on the first sheet (Sheet 0)
            df_legacy = extractor.load_data(legacy_path, format_type='MOVEX', sheet_name=0)

            writer = SDTWriter(self.output_dir)
            
            # FILENAME LOGIC
            if output_name_override:
                # Force specific name (for merging surgical tasks)
                out_name = output_name_override
            else:
                # Generate standard name
                base_src = os.path.basename(legacy_path).replace('.xlsx','')
                out_name = f"LOAD_{program_name}_{base_src}.xlsx"
            
            # Generate Output
            writer.generate_from_template(sdt_path, df_legacy, rules, target_sheets, out_name)
            
        except Exception as e:
            print(f"{Fore.RED}FATAL ERROR: {e}{Style.RESET_ALL}")
            import traceback
            traceback.print_exc()

    def resolve_from_map_public(self, val, col):
        """Helper wrapper to expose map resolution to other modules (like main.py/gui.py)."""
        return self._resolve_from_map(val, col)
--- END FILE: .\modules\migration_runner.py ---

--- START FILE: .\modules\rule_manager.py ---
import pandas as pd
import os
import glob
import math
from colorama import Fore, Style
from modules.audit_manager import AuditManager

class RuleManager:
    def __init__(self, rule_dir='config/rules'):
        self.rule_dir = rule_dir
        if not os.path.exists(self.rule_dir):
            os.makedirs(self.rule_dir)

    def _smart_pick(self, options, title_prompt):
        # ... (Same smart pick logic as before) ...
        filtered_indices = list(range(len(options)))
        filter_text = ""
        while True:
            print(f"\n{Fore.CYAN}--- {title_prompt} ---{Style.RESET_ALL}")
            if filter_text: print(f"{Fore.YELLOW}[Filter: '{filter_text}'] (Type 'all' to clear){Style.RESET_ALL}")
            limit = 20; count = 0; display_map = {}
            for i in filtered_indices:
                count += 1
                if count > limit:
                    print(f"   ... ({len(filtered_indices) - limit} more matches)")
                    break
                print(f"   {count}. {options[i]}")
                display_map[count] = i
            print(f"\n{Fore.GREEN}Type a Number to select, or Text to filter.{Style.RESET_ALL}")
            user_input = input(f"{Fore.CYAN}>> Selection: {Style.RESET_ALL}").strip()
            if not user_input: continue
            if user_input.isdigit():
                choice = int(user_input)
                if choice in display_map: return options[display_map[choice]]
            elif user_input.lower() in ['all', 'clear']: filter_text = ""; filtered_indices = list(range(len(options)))
            else:
                filter_text = user_input
                filtered_indices = [i for i, opt in enumerate(options) if filter_text.lower() in str(opt).lower()]
                if not filtered_indices: print(f"{Fore.RED}No matches found.{Style.RESET_ALL}")

    def interactive_manual_entry(self):
        # ... (Same setup logic) ...
        files = glob.glob(os.path.join(self.rule_dir, "*.xlsx"))
        if not files:
            print(f"{Fore.RED}No rule files found. Please import MCO first.{Style.RESET_ALL}")
            return

        filenames = [os.path.basename(f) for f in files]
        selected_name = self._smart_pick(filenames, "Select Rule File to Edit")
        selected_path = os.path.join(self.rule_dir, selected_name)
        program_name = selected_name.replace('.xlsx', '')

        try: 
            df = pd.read_excel(selected_path, sheet_name='Rules')
            df.fillna("", inplace=True)
        except: return

        for col in ['RULE_VALUE', 'SOURCE_FIELD', 'RULE_TYPE', 'DESCRIPTION']:
            if col in df.columns: df[col] = df[col].astype(object)
        
        changes_made = False; current_page = 0; PAGE_SIZE = 20; active_filter = ""
        while True:
            # ... (Same view logic) ...
            df_view = df.copy()
            if active_filter: df_view = df_view[df_view['TARGET_FIELD'].str.contains(active_filter.upper(), na=False)]
            total_rows = len(df_view); total_pages = math.ceil(total_rows / PAGE_SIZE)
            if total_pages == 0: total_pages = 1
            if current_page >= total_pages: current_page = 0
            start_idx = current_page * PAGE_SIZE; end_idx = start_idx + PAGE_SIZE
            df_page = df_view.iloc[start_idx:end_idx]

            print(f"\n{Fore.YELLOW}--- Editing {program_name} (Page {current_page+1}/{total_pages}) ---{Style.RESET_ALL}")
            if active_filter: print(f"{Fore.CYAN}[Filter Active: '{active_filter}']{Style.RESET_ALL}")
            print(f"{'#':<4} {'TARGET':<10} | {'TYPE':<8} | {'SOURCE':<15} | {'SCOPE':<10}")
            print("-" * 65)
            display_map = {}; row_counter = 1
            for idx, row in df_page.iterrows():
                scope_disp = str(row.get('SCOPE', 'GLOBAL'))[:10]
                src_disp = str(row['SOURCE_FIELD'])[:15]
                color = Style.RESET_ALL
                if row['RULE_TYPE'] == 'TODO': color = Fore.RED
                elif row['RULE_TYPE'] == 'IGNORE': color = Fore.LIGHTBLACK_EX
                print(f"{color}{row_counter:<4} {row['TARGET_FIELD']:<10} | {row['RULE_TYPE']:<8} | {src_disp:<15} | {scope_disp}{Style.RESET_ALL}")
                display_map[row_counter] = idx; row_counter += 1
            print("-" * 65)
            
            # ... (Same command logic) ...
            print("Commands: [N]ext Page, [P]revious Page, [F]ilter, [S]ave & Exit, [Q]uit")
            cmd = input(f"{Fore.CYAN}>> Enter # to Edit or Command: {Style.RESET_ALL}").strip().upper()
            if cmd == 'N': 
                if current_page < total_pages - 1: current_page += 1
            elif cmd == 'P': 
                if current_page > 0: current_page -= 1
            elif cmd == 'F': active_filter = input("Enter Filter Text (Enter to clear): ").strip(); current_page = 0
            elif cmd == 'S':
                if changes_made:
                    with pd.ExcelWriter(selected_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as w:
                        df.to_excel(w, sheet_name='Rules', index=False)
                    AuditManager(self.rule_dir).commit_changes(program_name)
                    print(f"{Fore.GREEN}   -> Saved and Audited.{Style.RESET_ALL}")
                return
            elif cmd == 'Q': return
            elif cmd.isdigit() and int(cmd) in display_map:
                self._edit_row(df, display_map[int(cmd)])
                changes_made = True

    def _edit_row(self, df, idx):
        target = df.at[idx, 'TARGET_FIELD']
        curr_type = str(df.at[idx, 'RULE_TYPE']).strip()
        curr_src = str(df.at[idx, 'SOURCE_FIELD']).strip()
        curr_val = str(df.at[idx, 'RULE_VALUE']).strip()
        curr_scope = str(df.at[idx, 'SCOPE']).strip() if 'SCOPE' in df.columns else 'GLOBAL'

        print(f"\n{Fore.GREEN}EDITING: {target}{Style.RESET_ALL}")
        print(f"Current: Type={curr_type}, Src={curr_src}, Val={curr_val}, Scope={curr_scope}")
        print(f"{Fore.CYAN}Types: CONST, DIRECT, MAP, IGNORE, TODO, PYTHON{Style.RESET_ALL}")
        
        new_type = input(f"New Type ({curr_type}): ").strip().upper()
        if not new_type: new_type = curr_type
        if new_type == 'CONSTANT': new_type = 'CONST'
        
        new_src = curr_src; new_val = curr_val

        if new_type not in ['CONST', 'IGNORE', 'TODO']:
            new_src = input(f"New Source ({curr_src}): ").strip().upper()
            if not new_src: new_src = curr_src
        else: new_src = ""

        if new_type not in ['DIRECT', 'IGNORE', 'TODO']:
            new_val = input(f"New Value ({curr_val}): ").strip()
            if not new_val: new_val = curr_val
        else: new_val = "" 
        
        # SCOPE INPUT
        new_scope = input(f"Scope ({curr_scope}): ").strip().upper()
        if not new_scope: new_scope = curr_scope

        df.at[idx, 'RULE_TYPE'] = new_type
        df.at[idx, 'SOURCE_FIELD'] = new_src
        df.at[idx, 'RULE_VALUE'] = new_val
        df.at[idx, 'SCOPE'] = new_scope
        df.at[idx, 'DESCRIPTION'] = 'Manual Edit'
        print("Updated in memory.")

    def merge_draft_file(self, draft_path, program_name, scope='GLOBAL'):
        # (Same merge logic as previous)
        path = f"{self.rule_dir}/{program_name}.xlsx"
        if not os.path.exists(path): return
        try:
            df_draft = pd.read_excel(draft_path)
            self.merge_draft_to_production(program_name, df_draft, scope)
        except Exception as e: print(f"Error: {e}")

    def merge_draft_to_production(self, program_name, df_draft, scope='GLOBAL'):
        path = f"{self.rule_dir}/{program_name}.xlsx"
        if not os.path.exists(path): return
        try:
            df_existing = pd.read_excel(path, sheet_name='Rules')
            df_existing.fillna("", inplace=True)
            target_map = {tgt: idx for idx, tgt in enumerate(df_existing['TARGET_FIELD'])}
            updates = 0
            for _, row in df_draft.iterrows():
                tgt = row['TARGET']
                if tgt in target_map:
                    idx = target_map[tgt]
                    if str(df_existing.at[idx, 'RULE_TYPE']).upper() in ['TODO', 'IGNORE', '', 'NAN']:
                        df_existing.at[idx, 'RULE_TYPE'] = row['TYPE']
                        df_existing.at[idx, 'SOURCE_FIELD'] = row['SOURCE']
                        if row['TYPE'] == 'CONST': df_existing.at[idx, 'RULE_VALUE'] = row['LOGIC']
                        elif row['TYPE'] == 'MAP': df_existing.at[idx, 'RULE_VALUE'] = f"MAP_{row['SOURCE']}_TO_{tgt}"
                        df_existing.at[idx, 'SCOPE'] = scope
                        updates += 1
            with pd.ExcelWriter(path, engine='openpyxl', mode='a', if_sheet_exists='replace') as w:
                df_existing.to_excel(w, sheet_name='Rules', index=False)
            print(f"{Fore.GREEN}Merged {updates} rules.{Style.RESET_ALL}")
            AuditManager(self.rule_dir).commit_changes(program_name)
        except Exception as e: print(f"Merge failed: {e}")
--- END FILE: .\modules\rule_manager.py ---

--- START FILE: .\modules\rule_promoter.py ---
import pandas as pd
import os
import datetime

class RulePromoter:
    def __init__(self, output_dir='config/rules'):
        self.output_dir = output_dir
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def generate_production_rules(self, program_name, df_draft, df_legacy, df_gold, join_key_legacy, join_key_m3):
        print(f"Promoting Draft to Production for {program_name}...")
        
        production_rules = []
        lookup_sheets = {}
        
        # Handle list keys for merge
        key_leg = join_key_legacy if isinstance(join_key_legacy, list) else [join_key_legacy]
        key_m3 = join_key_m3 if isinstance(join_key_m3, list) else [join_key_m3]
        
        merged = pd.merge(df_legacy, df_gold, left_on=key_leg, right_on=key_m3, how='inner', suffixes=('_SRC', '_TGT'))
        
        for idx, row in df_draft.iterrows():
            target_col = row['TARGET']
            source_col = row['SOURCE']
            rule_type  = row['TYPE']
            logic_val  = row['LOGIC']
            
            rule_entry = {
                'TARGET_API': program_name,
                'TARGET_FIELD': target_col,
                'SOURCE_FIELD': '',
                'RULE_TYPE': rule_type,
                'RULE_VALUE': '',
                'SCOPE': 'GLOBAL',
                'DESCRIPTION': f"Auto-generated. Confidence: {row.get('CONFIDENCE', 'N/A')}"
            }

            if rule_type == 'CONST':
                rule_entry['RULE_VALUE'] = logic_val
            elif rule_type == 'DIRECT':
                rule_entry['SOURCE_FIELD'] = source_col
            elif rule_type == 'MAP':
                rule_entry['SOURCE_FIELD'] = source_col
                lookup_name = f"MAP_{source_col}_TO_{target_col}"[:30]
                rule_entry['RULE_VALUE'] = lookup_name
                
                try:
                    src_lookup_col = source_col if source_col in merged.columns else f"{source_col}_SRC"
                    tgt_lookup_col = target_col if target_col in merged.columns else f"{target_col}_TGT"
                    
                    lookup_df = merged[[src_lookup_col, tgt_lookup_col]].dropna().drop_duplicates(subset=[src_lookup_col])
                    lookup_df.columns = ['SOURCE_KEY', 'TARGET_VALUE']
                    lookup_sheets[lookup_name] = lookup_df.sort_values('SOURCE_KEY')
                except KeyError:
                    print(f"   Warning: Could not auto-generate lookup for {lookup_name}.")
            else:
                rule_entry['RULE_TYPE'] = 'TODO'
                rule_entry['DESCRIPTION'] = "Logic unknown. Requires manual intervention."

            production_rules.append(rule_entry)

        df_prod = pd.DataFrame(production_rules)
        file_path = f"{self.output_dir}/{program_name}.xlsx"
        
        with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:
            df_prod.to_excel(writer, sheet_name='Rules', index=False)
            for sheet_name, df_table in lookup_sheets.items():
                df_table.to_excel(writer, sheet_name=sheet_name, index=False)
            pd.DataFrame(columns=['TIMESTAMP', 'USER', 'ACTION', 'TARGET_FIELD', 'OLD_VALUE', 'NEW_VALUE']).to_excel(writer, sheet_name='_Audit_Log', index=False)
            
        print(f"   -> Production Rule File Created: {file_path}")
        return file_path
--- END FILE: .\modules\rule_promoter.py ---

--- START FILE: .\modules\sdt_utils.py ---
import openpyxl
import os
from colorama import Fore, Style
import modules.ui as ui  # Reuse your existing UI helper

class SDTUtils:
    def __init__(self):
        pass

    def copy_sdt_sheet_interactive(self):
        """
        Interactive utility to copy data between sheets in an SDT file.
        """
        print(f"\n{Fore.CYAN}--- COPY SDT SHEET UTILITY ---{Style.RESET_ALL}")
        
        excel_file = ui.select_file("Select SDT Excel File", [("Excel files", "*.xlsx")])
        if not excel_file: return

        while True:
            try:
                wb = openpyxl.load_workbook(excel_file)
                break
            except PermissionError:
                print(f"{Fore.RED} File is open. Close '{os.path.basename(excel_file)}' and press Enter.{Style.RESET_ALL}")
                input()
            except Exception as e:
                print(f"{Fore.RED}Error opening file: {e}{Style.RESET_ALL}")
                return

        made_changes = False

        while True:
            sheet_names = wb.sheetnames
            
            print(f"\n{Fore.YELLOW}Select SOURCE Sheet:{Style.RESET_ALL}")
            source_sheet = ui.interactive_list_picker(sheet_names, "Source Sheet")
            if not source_sheet: break

            print(f"\n{Fore.YELLOW}Select DESTINATION Sheet:{Style.RESET_ALL}")
            dest_options = [s for s in sheet_names if s != source_sheet]
            dest_sheet = ui.interactive_list_picker(dest_options, "Destination Sheet")
            if not dest_sheet: break

            print(f"\nCopying from '{source_sheet}' -> '{dest_sheet}'...")
            
            matched, copied = self._map_and_copy_data(wb[source_sheet], wb[dest_sheet])
            
            if matched > 0:
                made_changes = True
                print(f"{Fore.GREEN} Copied {copied} rows across {matched} columns.{Style.RESET_ALL}")
            else:
                print(f"{Fore.RED} No matching columns found.{Style.RESET_ALL}")

            if input("\nProcess another sheet? (y/n): ").lower() != 'y':
                break

        if made_changes:
            while True:
                try:
                    print("\n Saving file...")
                    wb.save(excel_file)
                    print(f"{Fore.GREEN} Saved successfully.{Style.RESET_ALL}")
                    break
                except PermissionError:
                    print(f"{Fore.RED} File is open. Close it and press Enter.{Style.RESET_ALL}")
                    input()
        else:
            print("\nNo changes to save.")

    def merge_sdt_interactive(self):
        """
        Utility to merge external files into a Master SDT file with Deduplication.
        """
        print(f"\n{Fore.CYAN}--- MERGE SDT FILES UTILITY ---{Style.RESET_ALL}")

        # 1. Select Master
        print(f"\n{Fore.GREEN}Step 1: Select MASTER File (Destination){Style.RESET_ALL}")
        master_file = ui.select_file("Select MASTER Excel File", [("Excel files", "*.xlsx")])
        if not master_file: return

        while True:
            try:
                wb_master = openpyxl.load_workbook(master_file)
                break
            except PermissionError:
                print(f"{Fore.RED} Master File is open. Close it and press Enter.{Style.RESET_ALL}")
                input()

        master_sheets = wb_master.sheetnames
        made_changes = False

        # 2. Loop for Sources
        while True:
            print(f"\n{Fore.GREEN}Step 2: Select SOURCE File to merge into Master{Style.RESET_ALL}")
            source_file = ui.select_file("Select SOURCE Excel File", [("Excel files", "*.xlsx")])
            if not source_file: break

            try:
                wb_source = openpyxl.load_workbook(source_file, data_only=True)
            except Exception as e:
                print(f"{Fore.RED}Error reading source: {e}{Style.RESET_ALL}")
                continue

            # Find common sheets
            common_sheets = [s for s in wb_source.sheetnames if s in master_sheets]
            
            if not common_sheets:
                print(f"{Fore.RED}No matching sheet names found between files.{Style.RESET_ALL}")
                continue

            print(f"\n{Fore.YELLOW}Select Sheet to Merge:{Style.RESET_ALL}")
            selected_sheet = ui.interactive_list_picker(common_sheets, "Available Sheets")
            if not selected_sheet: continue

            print(f"Merging '{selected_sheet}' from Source -> Master...")
            
            count = self._merge_sheet_data(wb_master[selected_sheet], wb_source[selected_sheet])
            print(f"{Fore.GREEN} Merged {count} new unique rows.{Style.RESET_ALL}")
            made_changes = True

            if input("\nMerge another file? (y/n): ").lower() != 'y':
                break

        # 3. Save
        if made_changes:
            while True:
                try:
                    print("\n Saving MASTER file...")
                    wb_master.save(master_file)
                    print(f"{Fore.GREEN} Saved: {master_file}{Style.RESET_ALL}")
                    break
                except PermissionError:
                    print(f"{Fore.RED} Master File is open. Close it and press Enter.{Style.RESET_ALL}")
                    input()

    def _merge_sheet_data(self, ws_master, ws_source):
        """
        Reads data from source, checks against master, appends unique rows.
        Assumes Headers on Row 1, Data starts Row 4.
        """
        # 1. Read Master Data into Set of Tuples (for deduplication)
        # We read starting at row 4
        master_data = []
        seen_rows = set()

        # Read Master
        for row in ws_master.iter_rows(min_row=4, values_only=True):
            # Check if row is effectively empty
            if not any(row): continue
            
            # Convert to string to ensure consistent comparison (1 vs '1')
            row_tuple = tuple(str(x).strip() if x is not None else "" for x in row)
            
            master_data.append(row) # Keep original types for re-writing if needed? 
            # Actually, we just append NEW rows to the bottom of Master. 
            # We don't re-write master rows.
            seen_rows.add(row_tuple)

        # 2. Read Source Data
        new_rows_count = 0
        
        for row in ws_source.iter_rows(min_row=4, values_only=True):
            if not any(row): continue
            
            row_tuple = tuple(str(x).strip() if x is not None else "" for x in row)
            
            if row_tuple not in seen_rows:
                # It's unique! Append to Master
                ws_master.append(row)
                seen_rows.add(row_tuple) # Prevent duplicates within source file too
                new_rows_count += 1
        
        return new_rows_count

    def _clear_rows(self, ws, start_row):
        if ws.max_row >= start_row:
            count = ws.max_row - start_row + 1
            ws.delete_rows(start_row, count)

    def _map_and_copy_data(self, source_ws, dest_ws):
        source_headers = {cell.value: idx for idx, cell in enumerate(source_ws[1], start=1) if cell.value}
        dest_headers = {cell.value: idx for idx, cell in enumerate(dest_ws[1], start=1) if cell.value}

        message_col_idx = source_headers.get('MESSAGE')
        include_nok = True
        if message_col_idx is not None:
            ans = input("Include rows where MESSAGE starts with 'NOK'? (y/n): ").strip().lower()
            include_nok = (ans == 'y')

        for col in ['MESSAGE', 'CONO']:
            source_headers.pop(col, None)
            dest_headers.pop(col, None)

        matched_columns = {
            header: (source_headers[header], dest_headers[header])
            for header in source_headers if header in dest_headers
        }

        if not matched_columns: return 0, 0

        data_rows = list(source_ws.iter_rows(min_row=4, values_only=True))
        filtered_rows = []

        for row in data_rows:
            if message_col_idx is not None and not include_nok:
                val = row[message_col_idx - 1]
                if isinstance(val, str) and val.strip().upper().startswith("NOK"):
                    continue
            filtered_rows.append(row)

        self._clear_rows(dest_ws, 4)

        for row_offset, row in enumerate(filtered_rows, start=4):
            for header, (src_idx, dst_idx) in matched_columns.items():
                val = row[src_idx - 1]
                if val is not None:
                    cell = dest_ws.cell(row=row_offset, column=dst_idx)
                    cell.value = str(val)
                    cell.number_format = '@' 

        return len(matched_columns), len(filtered_rows)
--- END FILE: .\modules\sdt_utils.py ---

--- START FILE: .\modules\sdt_writer.py ---
import pandas as pd
import shutil
import os
import glob
import datetime
from openpyxl import load_workbook
from colorama import Fore, Style

class SDTWriter:
    def __init__(self, output_dir='output'):
        self.output_dir = output_dir
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def _get_versioned_filename(self, base_name):
        """Generates TIMESTAMPED_REV filename."""
        # If the name already looks timestamped (has SURGICAL and date), trust it.
        if "SURGICAL" in base_name and "_" in base_name:
            return base_name

        base_clean = base_name.replace('.xlsx', '')
        today = datetime.datetime.now().strftime("%Y%m%d")
        
        search_pattern = os.path.join(self.output_dir, f"{base_clean}_{today}_rev*.xlsx")
        existing_files = glob.glob(search_pattern)
        
        rev = 1
        if existing_files:
            try:
                revs = [int(f.split('_rev')[-1].split('.')[0]) for f in existing_files]
                if revs:
                    rev = max(revs) + 1
            except:
                pass
        
        final = f"{base_clean}_{today}_rev{rev}.xlsx"
        print(f"   -> Auto-versioned filename generated: {final}")
        return final

    def generate_from_template(self, template_path, legacy_data, rules_df, target_sheets, output_name):
        out_path = os.path.join(self.output_dir, output_name)
        
        # --- LOGIC: CREATE vs APPEND ---
        if os.path.exists(out_path):
            print(f"{Fore.CYAN}   -> Appending to existing surgical file: {output_name}{Style.RESET_ALL}")
            target_path = out_path
            try:
                wb = load_workbook(target_path)
            except PermissionError:
                print(f"{Fore.RED}   ERROR: File is open. Close '{output_name}' and try again!{Style.RESET_ALL}")
                return
        else:
            # New file: version it unless surgical already dictates the name
            if "SURGICAL" in output_name:
                final_name = output_name
            else:
                final_name = self._get_versioned_filename(output_name)
            
            target_path = os.path.join(self.output_dir, final_name)
            print(f"{Fore.GREEN}   -> Creating NEW SDT file: {os.path.basename(target_path)}{Style.RESET_ALL}")
            shutil.copyfile(template_path, target_path)

            try:
                wb = load_workbook(target_path)
            except PermissionError:
                print(f"{Fore.RED}   ERROR: Cannot open new file '{final_name}'. Is it locked?{Style.RESET_ALL}")
                return

        # ------------------------------
        # PROCESS EACH TARGET SHEET
        # ------------------------------
        for sheet_name in target_sheets:
            if sheet_name not in wb.sheetnames:
                print(f"{Fore.YELLOW}   Warning: Sheet '{sheet_name}' not found in template.{Style.RESET_ALL}")
                continue

            print(f"   -> Populating Sheet: {sheet_name}...")

            ws = wb[sheet_name]
            
            # Extract headers (Row 1)
            headers = [cell.value for cell in ws[1]]
            valid_fields = [str(h).strip().upper() for h in headers if h is not None]
            
            # Transform data according to rules
            df_transformed = self._transform_data(legacy_data, rules_df, valid_fields, sheet_name)
            
            # --- CLEAR OLD DATA (rows 4 to end) ---
            if ws.max_row >= 4:
                count = ws.max_row - 3
                if count > 0:
                    print(f"      -> Clearing {count} old rows...")
                    ws.delete_rows(4, count)
            
            # --- WRITE NEW DATA ---
            data_rows = df_transformed.values.tolist()
            start_row = 4
            
            print(f"      -> Writing {len(data_rows)} rows...")
            for i, row in enumerate(data_rows):
                for j, val in enumerate(row):
                    cell = ws.cell(row=start_row + i, column=j + 1)
                    cell.value = str(val) if pd.notna(val) else ""

        # ------------------------------
        # SAVE FILE
        # ------------------------------
        wb.save(target_path)
        print(f"{Fore.GREEN}   -> Saved SDT File: {target_path}{Style.RESET_ALL}")

    def _transform_data(self, df_source, rules_df, valid_fields, sheet_name):
        df_out = pd.DataFrame('', index=df_source.index, columns=valid_fields)
        rule_map = rules_df.set_index('TARGET_FIELD').to_dict('index')
        
        mapped_count = 0
        
        # Pre-calc source map (Upper->Original Case)
        src_map = {c.upper(): c for c in df_source.columns}

        for field in valid_fields:
            if field in rule_map:
                rule = rule_map[field]
                r_type = rule['RULE_TYPE']
                r_src = str(rule['SOURCE_FIELD']).strip().upper() if pd.notna(rule['SOURCE_FIELD']) else ""
                r_val = rule['RULE_VALUE']
                
                try:
                    # CONST
                    if r_type == 'CONST':
                        df_out[field] = r_val
                        mapped_count += 1
                        continue

                    # DIRECT
                    if r_type == 'DIRECT':
                        if r_src in src_map:
                            df_out[field] = df_source[src_map[r_src]]
                            mapped_count += 1
                        else:
                            # Try suffix match for Movex  MCO fields
                            suffix_match = next(
                                (c for c in src_map if c.endswith(r_src) and len(c) == 6),
                                None
                            )
                            if suffix_match:
                                df_out[field] = df_source[src_map[suffix_match]]
                                mapped_count += 1

                    # MAP (lookup placeholder)
                    elif r_type == 'MAP':
                        if r_src in src_map:
                            df_out[field] = df_source[src_map[r_src]]
                            mapped_count += 1

                except Exception:
                    pass
        
        if mapped_count > 0:
            print(f"      -> Mapped {mapped_count} columns.")
        else:
            print(f"{Fore.YELLOW}      [Warning] 0 columns mapped for {sheet_name}.{Style.RESET_ALL}")
            
        return df_out

--- END FILE: .\modules\sdt_writer.py ---

--- START FILE: .\modules\surgical_extractor.py ---
import pandas as pd
import os
import warnings
from colorama import Fore, Style
from modules.extractor import DataExtractor

class SurgicalExtractor:
    def __init__(self):
        self.config_dir = 'config'
        self.staging_dir = 'surgical_staging'
        if not os.path.exists(self.staging_dir):
            os.makedirs(self.staging_dir)
        
    def _load_csv(self, filename):
        path = os.path.join(self.config_dir, filename)
        if not os.path.exists(path): return pd.DataFrame()
        try:
            df = pd.read_csv(path)
            df.columns = [c.upper().strip() for c in df.columns]
            return df
        except: return pd.DataFrame()

    def get_available_objects(self):
        df = self._load_csv('surgical_def.csv')
        if df.empty: return []
        return sorted(df['OBJECT_TYPE'].unique().tolist())

    def perform_extraction(self, object_type, id_list):
        print(f"\n{Fore.CYAN}--- SURGICAL EXTRACTION: {object_type} ---{Style.RESET_ALL}")
        
        # Suppress fragmentation warning
        warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)

        df_def = self._load_csv('surgical_def.csv')
        df_source = self._load_csv('source_map.csv')
        df_mig = self._load_csv('migration_map.csv')
        
        if df_def.empty or df_source.empty or df_mig.empty:
            print(f"{Fore.RED}Missing configuration files.{Style.RESET_ALL}")
            return []

        relevant_sheets = df_def[df_def['OBJECT_TYPE'] == object_type]['MCO_SHEET'].tolist()
        if not relevant_sheets:
            print(f"{Fore.RED}No sheets defined for {object_type}.{Style.RESET_ALL}")
            return []

        tasks = []
        extractor = DataExtractor()
        
        # Normalize Input IDs (String, Strip, Upper)
        # Ensure we compare "clean" strings
        clean_ids = set([str(x).strip().upper().split('.')[0] for x in id_list])

        for sheet in relevant_sheets:
            print(f"   Processing scope: {sheet}...")
            
            row_src = df_source[df_source['MCO_SHEET'] == sheet]
            if row_src.empty:
                print(f"{Fore.YELLOW}      Warning: No source mapping for '{sheet}'. Skipping.{Style.RESET_ALL}")
                continue
            
            source_file = row_src.iloc[0]['SOURCE_FILE']
            raw_keys = str(row_src.iloc[0]['JOIN_KEY'])
            join_keys = [k.strip().upper() for k in raw_keys.split(',')]
            
            row_mig = df_mig[df_mig['MCO_SHEET'] == sheet]
            if row_mig.empty:
                print(f"{Fore.YELLOW}      Warning: No API mapping for '{sheet}'. Skipping.{Style.RESET_ALL}")
                continue
            
            api_name = row_mig.iloc[0]['API_NAME']
            
            try:
                # Resolve Path
                full_path = source_file
                if not os.path.exists(full_path):
                    alt_path = os.path.join('raw_data', source_file)
                    if os.path.exists(alt_path): full_path = alt_path
                    else:
                        print(f"{Fore.RED}      Source file not found: {source_file}{Style.RESET_ALL}")
                        continue

                df_raw = extractor.load_data(full_path, format_type='MOVEX')
                
                valid_keys = [k for k in join_keys if k in df_raw.columns]
                if not valid_keys:
                    print(f"{Fore.RED}      Keys {join_keys} not found. Available: {list(df_raw.columns)[:5]}...{Style.RESET_ALL}")
                    continue

                final_mask = pd.Series([False] * len(df_raw))
                
                for key in valid_keys:
                    # --- ROBUST NORMALIZATION ---
                    # 1. Force to numeric (coerce errors to NaN) to handle "1001" vs 1001
                    # 2. Fill NaN with original (in case it's alphanumeric ID like 'A100')
                    # 3. Convert to string
                    # 4. Remove '.0' if it exists (Fixes the Float vs Int issue)
                    
                    col_series = pd.to_numeric(df_raw[key], errors='coerce').fillna(df_raw[key])
                    col_str = col_series.astype(str).str.strip().str.upper()
                    
                    # Remove trailing .0 from floats (e.g. '44709.0' -> '44709')
                    col_str = col_str.apply(lambda x: x.split('.')[0] if x.replace('.','',1).isdigit() else x)

                    final_mask = final_mask | col_str.isin(clean_ids)

                df_filtered = df_raw[final_mask].copy()
                
                if df_filtered.empty:
                    print(f"{Fore.YELLOW}      No records found for IDs.{Style.RESET_ALL}")
                    # Debug helper: Show what the file actually contains
                    if len(df_raw) > 0:
                        raw_sample = df_raw[valid_keys[0]].head(3).tolist()
                        print(f"      (File contains IDs like: {raw_sample})")
                    continue
                
                # --- COLUMN ALIASING ---
                new_cols = {}
                for col in df_filtered.columns:
                    # 1. Universal 4-char Alias (OPCUNO -> CUNO)
                    if len(col) == 6:
                        short_col = col[2:]
                        if short_col not in df_filtered.columns:
                            new_cols[short_col] = df_filtered[col]

                    # 2. Item Aliases (Warehouse/Facility -> Master)
                    if col.startswith('MB') or col.startswith('M9'):
                        mm_col = 'MM' + col[2:]
                        if mm_col not in df_filtered.columns:
                            new_cols[mm_col] = df_filtered[col]

                    # 3. Customer Aliases (OP -> OK)
                    if col.startswith('OP'):
                        ok_col = 'OK' + col[2:]
                        if ok_col not in df_filtered.columns:
                            new_cols[ok_col] = df_filtered[col]
                    
                    # 4. Supplier Aliases (II/IB -> ID)
                    if col.startswith('II') or col.startswith('IB'):
                        id_col = 'ID' + col[2:]
                        if id_col not in df_filtered.columns:
                            new_cols[id_col] = df_filtered[col]

                if new_cols:
                    df_filtered = pd.concat([df_filtered, pd.DataFrame(new_cols, index=df_filtered.index)], axis=1)

                # Save Staged File
                safe_sheet = "".join([c if c.isalnum() else "_" for c in sheet])
                staged_name = f"STAGED_{api_name}_{safe_sheet}.xlsx"
                staged_path = os.path.join(self.staging_dir, staged_name)
                
                df_filtered.to_excel(staged_path, index=False)
                
                print(f"{Fore.GREEN}      -> Staged {len(df_filtered)} rows to {staged_path}{Style.RESET_ALL}")
                
                tasks.append({
                    'program_name': api_name,
                    'legacy_path': staged_path,
                    'mco_sheet': sheet 
                })

            except Exception as e:
                print(f"{Fore.RED}      Error processing {sheet}: {e}{Style.RESET_ALL}")
                import traceback
                traceback.print_exc()

        return tasks
--- END FILE: .\modules\surgical_extractor.py ---

--- START FILE: .\modules\transform_engine.py ---
import pandas as pd
import numpy as np
import os
from colorama import Fore, Style

# Import hooks (Optional now since we use exec, but good for shared utils)
try:
    import modules.hooks as hooks
except ImportError:
    hooks = None

class TransformEngine:
    def __init__(self, rules_df, lookups):
        self.rules = rules_df
        self.lookups = lookups
        self._map_cache = {}

    def _load_map_file(self, config_str):
        if config_str in self._map_cache: return self._map_cache[config_str]
        try:
            parts = config_str.split('|')
            if len(parts) != 3: return None
            path, key_col, val_col = parts[0].strip(), parts[1].strip(), parts[2].strip()
            
            if not os.path.exists(path):
                if os.path.exists(os.path.join('config', path)): path = os.path.join('config', path)
                elif os.path.exists(os.path.join('raw_data', path)): path = os.path.join('raw_data', path)
                else: return None

            if path.endswith('.csv'): df = pd.read_csv(path)
            else: df = pd.read_excel(path)

            df = df.drop_duplicates(subset=[key_col])
            lookup_dict = pd.Series(df[val_col].values, index=df[key_col].astype(str)).to_dict()
            self._map_cache[config_str] = lookup_dict
            return lookup_dict
        except: return None

    def _execute_python_rule(self, code_snippet, source_val, row_data):
        try:
            local_vars = {'source': source_val, 'row': row_data}
            func_def = f"def user_transform(source, row):\n"
            indented_code = "\n".join([f"    {line}" for line in code_snippet.split('\n')])
            full_code = func_def + indented_code
            
            exec(full_code, {}, local_vars)
            return local_vars['user_transform'](source_val, row_data)
        except Exception:
            return str(source_val)

    def process(self, df_source):
        df_target = pd.DataFrame(index=df_source.index)
        src_map = {c.upper(): c for c in df_source.columns}

        print("   Applying Transformation Rules...")

        for index, rule in self.rules.iterrows():
            target_col = rule['TARGET_FIELD']
            rule_type = rule['RULE_TYPE']
            r_src = str(rule['SOURCE_FIELD']).strip().upper() if pd.notna(rule['SOURCE_FIELD']) else ""
            
            r_val = str(rule['RULE_VALUE']).strip() if pd.notna(rule['RULE_VALUE']) else ""
            # FIX: Float cleanup (20.0 -> 20)
            if r_val.endswith('.0'):
                try: r_val = str(int(float(r_val)))
                except: pass

            try:
                if rule_type == 'DIRECT':
                    if r_src in src_map:
                        df_target[target_col] = df_source[src_map[r_src]]
                    else:
                        match = next((c for c in src_map if c.endswith(r_src) and len(c)==6), None)
                        if match: df_target[target_col] = df_source[src_map[match]]

                elif rule_type == 'CONST':
                    df_target[target_col] = r_val

                elif rule_type == 'MAP':
                    source_series = None
                    if r_src in src_map: source_series = df_source[src_map[r_src]]
                    else:
                        match = next((c for c in src_map if c.endswith(r_src) and len(c)==6), None)
                        if match: source_series = df_source[src_map[match]]
                    
                    if source_series is not None:
                        lookup_dict = self._load_map_file(r_val)
                        if lookup_dict:
                            df_target[target_col] = source_series.astype(str).map(lookup_dict)

                elif rule_type == 'PYTHON':
                    col_name = None
                    if r_src in src_map: col_name = src_map[r_src]
                    else:
                        match = next((c for c in src_map if c.endswith(r_src) and len(c)==6), None)
                        if match: col_name = src_map[match]
                    
                    if col_name:
                        df_target[target_col] = df_source.apply(
                            lambda row: self._execute_python_rule(r_val, row[col_name], row), 
                            axis=1
                        )
                    else:
                        df_target[target_col] = df_source.apply(
                            lambda row: self._execute_python_rule(r_val, None, row), 
                            axis=1
                        )
            except Exception: pass
        
        return df_target
--- END FILE: .\modules\transform_engine.py ---

--- START FILE: .\modules\ui.py ---
import os
import sys
import tkinter as tk
from tkinter import filedialog
from colorama import init, Fore, Style
import pandas as pd

# Initialize colorama
init(autoreset=True)

def print_header(title):
    print("\n" + "="*60); print(f"   {title.upper()}"); print("="*60)

def ensure_folder(path):
    if not os.path.exists(path): os.makedirs(path)

def select_file(prompt, filetypes):
    print(f"\n{Fore.GREEN}--> ACTION REQUIRED: {prompt}{Style.RESET_ALL}")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    f = filedialog.askopenfilename(title=prompt, filetypes=filetypes)
    root.destroy()
    return f

def select_folder(prompt):
    print(f"\n{Fore.GREEN}--> ACTION REQUIRED: {prompt}{Style.RESET_ALL}")
    root = tk.Tk(); root.withdraw(); root.attributes('-topmost', True)
    f = filedialog.askdirectory(title=prompt)
    root.destroy()
    return f

def interactive_list_picker(options, title_prompt, multi=False):
    filtered_indices = list(range(len(options)))
    filter_text = ""
    while True:
        print(f"\n{Fore.CYAN}--- {title_prompt} ---{Style.RESET_ALL}")
        if filter_text: print(f"{Fore.YELLOW}[Filter: '{filter_text}'] (Type 'all' to clear){Style.RESET_ALL}")
        
        limit = 20; count = 0; display_map = {}
        for i in filtered_indices:
            count += 1
            if count > limit:
                print(f"   ... ({len(filtered_indices) - limit} more matches)")
                break
            print(f"   {count}. {options[i]}")
            display_map[count] = i
        
        if multi: print(f"\n{Fore.GREEN}Type numbers (e.g. 1, 3), 'all', or Filter Text.{Style.RESET_ALL}")
        else: print(f"\n{Fore.GREEN}Type a Number, Filter Text, or '0' to Cancel.{Style.RESET_ALL}")
            
        user_input = input(f"{Fore.CYAN}>> Selection: {Style.RESET_ALL}").strip()
        if not user_input: continue
        
        if user_input == '0': return None
        if multi and user_input.lower() == 'all': return [options[i] for i in filtered_indices]

        if multi and ',' in user_input:
            try:
                parts = [int(x.strip()) for x in user_input.split(',')]
                selected = [options[display_map[p]] for p in parts if p in display_map]
                if selected: return selected
            except ValueError: pass

        if user_input.isdigit():
            choice = int(user_input)
            if choice in display_map:
                res = options[display_map[choice]]
                return [res] if multi else res
        elif user_input.lower() in ['clear']: filter_text = ""; filtered_indices = list(range(len(options)))
        else:
            filter_text = user_input
            filtered_indices = [i for i, opt in enumerate(options) if filter_text.lower() in str(opt).lower()]
            if not filtered_indices: print("No matches.")

def get_sheet_selection(file_path):
    try:
        xls = pd.ExcelFile(file_path, engine='openpyxl')
        sheets = xls.sheet_names
        if len(sheets) == 1:
            print(f"   -> Auto-selecting single sheet: '{sheets[0]}'")
            return sheets[0]
        return interactive_list_picker(sheets, f"Select Sheet from {os.path.basename(file_path)}")
    except Exception as e:
        print(f"{Fore.RED}   Warning: Could not read sheets. Defaulting to first.{Style.RESET_ALL}")
        return 0

def select_columns_interactive(df, prompt_text):
    columns = list(df.columns)
    display_limit = min(8, len(columns))
    print(f"\n{Fore.CYAN}   {prompt_text}{Style.RESET_ALL}")
    print(f"   Available Columns (Showing first {display_limit}):")
    for i in range(display_limit): print(f"   {i+1}. {columns[i]}")
    if len(columns) > 8: print(f"   ... (and {len(columns)-8} more)")
    while True:
        user_input = input(f"{Fore.CYAN}   >> Enter Column Number(s) (comma separated): {Style.RESET_ALL}").strip()
        if not user_input: continue
        selected_cols = []
        try:
            indices = [int(x.strip()) - 1 for x in user_input.split(',')]
            for idx in indices:
                if 0 <= idx < len(columns): selected_cols.append(columns[idx])
            if selected_cols: return selected_cols
        except ValueError: print(f"{Fore.RED}Error: Numbers only.{Style.RESET_ALL}")
--- END FILE: .\modules\ui.py ---

--- START FILE: .\modules\validator_analyzer.py ---
import pandas as pd
import numpy as np
from colorama import Fore, Style

class ValidatorAnalyzer:
    def __init__(self, ignore_cols=None):
        if ignore_cols is None:
            self.ignore_cols = ['CONO', 'MESSAGE', 'RGDT', 'LMDT', 'RGTM', 'CHID']
        else:
            self.ignore_cols = ignore_cols

    def _prepare_df(self, df):
        df = df.copy()
        cols_to_drop = [c for c in self.ignore_cols if c in df.columns]
        if cols_to_drop:
            df.drop(columns=cols_to_drop, inplace=True)
        return df

    def _is_valid_predictor(self, series, total_rows):
        unique_count = series.nunique()
        if unique_count <= 1: return False
        if unique_count == total_rows: return False
        if unique_count > 100: return False 
        if unique_count > (total_rows * 0.2): return False
        return True

    def _get_col_safe(self, df, col_name, suffix):
        if col_name in df.columns: return df[col_name]
        suffixed_name = f"{col_name}{suffix}"
        if suffixed_name in df.columns: return df[suffixed_name]
        return pd.Series([np.nan] * len(df))

    def reverse_engineer_rules(self, df_legacy, df_gold, leg_key, m3_key, existing_targets=None):
        if existing_targets is None: existing_targets = []
        print(f"\n{Fore.CYAN}--- Starting Advanced Pattern Recognition ---{Style.RESET_ALL}")
        print(f"   Skipping {len(existing_targets)} already defined fields.")
        
        # 1. Prepare Dataframes
        df_legacy = self._prepare_df(df_legacy)
        df_gold = self._prepare_df(df_gold)
        
        # 2. TYPE SAFETY FIX: Force Join Keys to String to prevent int64 vs object crash
        # Handle Legacy Key
        if isinstance(leg_key, list):
            for k in leg_key: 
                if k in df_legacy.columns: 
                    df_legacy[k] = df_legacy[k].astype(str).str.strip()
        else:
            if leg_key in df_legacy.columns:
                df_legacy[leg_key] = df_legacy[leg_key].astype(str).str.strip()

        # Handle M3 Key
        if isinstance(m3_key, list):
            for k in m3_key: 
                if k in df_gold.columns:
                    df_gold[k] = df_gold[k].astype(str).str.strip()
        else:
            if m3_key in df_gold.columns:
                df_gold[m3_key] = df_gold[m3_key].astype(str).str.strip()

        # Handle list keys for merge
        leg_keys = leg_key if isinstance(leg_key, list) else [leg_key]
        m3_keys = m3_key if isinstance(m3_key, list) else [m3_key]
        
        # 3. Merge
        try:
            merged = pd.merge(df_legacy, df_gold, left_on=leg_keys, right_on=m3_keys, how='inner', suffixes=('_SRC', '_TGT'))
        except Exception as e:
            print(f"{Fore.RED}Merge Error: {e}{Style.RESET_ALL}")
            return pd.DataFrame() # Return empty on failure

        total_rows = len(merged)
        print(f"   -> Analyzing {total_rows} matched rows.")
        
        valid_predictors = []
        for col in df_legacy.columns:
            if self._is_valid_predictor(df_legacy[col], total_rows):
                valid_predictors.append(col)
        
        suggestions = []

        # 4. Analyze Targets
        for m3_col in df_gold.columns:
            if m3_col in m3_keys: continue
            if m3_col in existing_targets: continue
            
            series_m3 = self._get_col_safe(merged, m3_col, '_TGT')
            unique_vals = series_m3.dropna().unique()
            
            # PATTERN 1: CONSTANT
            if len(unique_vals) == 1:
                suggestions.append({
                    'TARGET': m3_col, 'SOURCE': '', 'TYPE': 'CONST',
                    'LOGIC': str(unique_vals[0]), 'CONFIDENCE': 'High'
                })
                continue
            
            # PATTERN 2: DIRECT MAPPING
            match_found = False
            for leg_col in df_legacy.columns:
                series_leg = self._get_col_safe(merged, leg_col, '_SRC')
                s_m3 = series_m3.astype(str).str.strip().replace({'nan':'', '0':'', '0.0':''})
                s_leg = series_leg.astype(str).str.strip().replace({'nan':'', '0':'', '0.0':''})
                
                if (s_m3 != '').sum() < (total_rows * 0.05): continue
                
                if s_m3.equals(s_leg):
                    suggestions.append({
                        'TARGET': m3_col, 'SOURCE': leg_col, 'TYPE': 'DIRECT',
                        'LOGIC': 'Direct Copy', 'CONFIDENCE': 'High'
                    })
                    match_found = True
                    break
            if match_found: continue

            # PATTERN 3: CONDITIONAL LOGIC
            if len(unique_vals) > 50: continue 

            best_predictor = None
            for pred_col in valid_predictors:
                series_pred = self._get_col_safe(merged, pred_col, '_SRC')
                temp_df = pd.DataFrame({'PRED': series_pred, 'TARGET': series_m3})
                
                if temp_df.groupby('PRED')['TARGET'].nunique().mean() == 1.0:
                    best_predictor = pred_col
                    break
            
            if best_predictor:
                series_pred = self._get_col_safe(merged, best_predictor, '_SRC')
                temp_df = pd.DataFrame({'PRED': series_pred, 'TARGET': series_m3})
                example = temp_df.groupby('PRED')['TARGET'].first().head(3).to_dict()
                suggestions.append({
                    'TARGET': m3_col, 'SOURCE': best_predictor, 'TYPE': 'MAP',
                    'LOGIC': f"Derived from {best_predictor}", 
                    'CONFIDENCE': 'Medium'
                })

        return pd.DataFrame(suggestions)
--- END FILE: .\modules\validator_analyzer.py ---

--- START FILE: .\modules\__init__.py ---

--- END FILE: .\modules\__init__.py ---

--- START FILE: .\modules\gui\app.py ---
import customtkinter as ctk
import tkinter as tk
import sys

# Import Hubs
from modules.gui.tab_migration import MigrationHub
from modules.gui.tab_config import ConfigHub
from modules.gui.tab_rules import RulesHub
from modules.gui.tab_utils import UtilitiesHub # <--- Added Import
from modules.gui.utils import TextRedirector

ctk.set_appearance_mode("Dark")
ctk.set_default_color_theme("blue")

class M3MigrationApp(ctk.CTk):
    def __init__(self):
        super().__init__()
        self.title("M3 Data Migration Platform")
        self.geometry("1280x850")
        
        # Main Grid: 2 Columns (Sidebar | Content)
        self.grid_columnconfigure(1, weight=1)
        self.grid_rowconfigure(0, weight=1)

        # --- SIDEBAR (Fixed) ---
        self.sidebar = ctk.CTkFrame(self, width=200, corner_radius=0)
        self.sidebar.grid(row=0, column=0, sticky="nsew")
        ctk.CTkLabel(self.sidebar, text="M3 MIGRATION", font=ctk.CTkFont(size=20, weight="bold")).pack(pady=(30, 20))
        
        ctk.CTkButton(self.sidebar, text="  Run Migration  ", height=40, anchor="w", command=lambda: self.show_frame("MigrateHub")).pack(fill="x", padx=10, pady=5)
        ctk.CTkButton(self.sidebar, text="  Configuration  ", height=40, anchor="w", fg_color="transparent", border_width=1, command=lambda: self.show_frame("ConfigHub")).pack(fill="x", padx=10, pady=5)
        ctk.CTkButton(self.sidebar, text="  Rules & Admin  ", height=40, anchor="w", fg_color="transparent", border_width=1, command=lambda: self.show_frame("RulesHub")).pack(fill="x", padx=10, pady=5)
        # NEW BUTTON
        ctk.CTkButton(self.sidebar, text="  Utilities  ", height=40, anchor="w", fg_color="transparent", border_width=1, command=lambda: self.show_frame("UtilsHub")).pack(fill="x", padx=10, pady=5)

        # --- RIGHT SIDE (Splitter) ---
        self.splitter = tk.PanedWindow(
            self, 
            orient="vertical", 
            bg="#2B2B2B",      
            bd=0, 
            sashwidth=10,       
            sashrelief="raised", 
            sashpad=2,         
            handlepad=5,
            showhandle=False   
        )
        self.splitter.config(bg="#333333") 
        
        self.splitter.grid(row=0, column=1, sticky="nsew", padx=0, pady=0)

        # PANE 1: MAIN CONTENT AREA
        self.main_area = ctk.CTkFrame(self.splitter, corner_radius=0, fg_color="transparent")
        self.splitter.add(self.main_area, stretch="always", height=600) 
        
        self.frames = {}
        self.frames["MigrateHub"] = MigrationHub(self.main_area)
        self.frames["ConfigHub"] = ConfigHub(self.main_area)
        self.frames["RulesHub"] = RulesHub(self.main_area)
        self.frames["UtilsHub"] = UtilitiesHub(self.main_area) # <--- Added Frame

        # PANE 2: CONSOLE AREA
        self.console_frame = ctk.CTkFrame(self.splitter, corner_radius=0, fg_color="#1A1A1A") 
        self.splitter.add(self.console_frame, stretch="never", height=200)

        # Console Widgets
        con_head = ctk.CTkFrame(self.console_frame, height=28, corner_radius=0, fg_color="#333333")
        con_head.pack(fill="x")
        ctk.CTkLabel(con_head, text="  System Log (Drag bar above to resize)", font=ctk.CTkFont(size=11), text_color="#AAAAAA", anchor="w").pack(side="left", fill="x")
        
        self.console = ctk.CTkTextbox(self.console_frame, font=("Consolas", 12))
        self.console.pack(fill="both", expand=True, padx=0, pady=0)
        self.console.configure(state="disabled")
        
        # Redirect Stdout
        sys.stdout = TextRedirector(self.console)
        sys.stderr = TextRedirector(self.console)

        self.show_frame("MigrateHub")

    def show_frame(self, name):
        for f in self.frames.values(): f.pack_forget()
        self.frames[name].pack(fill="both", expand=True)
--- END FILE: .\modules\gui\app.py ---

--- START FILE: .\modules\gui\tab_config.py ---
import customtkinter as ctk
from tkinter import filedialog
import threading
import glob
import os
import pandas as pd
from colorama import Fore, Style

# Logic Imports
from modules.mco_importer import MCOImporter
from modules.audit_manager import AuditManager
from modules.extractor import DataExtractor
from modules.validator_analyzer import ValidatorAnalyzer
from modules.rule_manager import RuleManager
from modules.mco_checker import MCOChecker

class ConfigHub(ctk.CTkTabview):
    def __init__(self, parent):
        super().__init__(parent)
        self.add("Import MCO")
        self.add("Reverse Engineer")
        self.add("Map Editor") # <--- RESTORED TAB
        
        self._build_imp(self.tab("Import MCO"))
        self._build_an(self.tab("Reverse Engineer"))
        self._build_editor(self.tab("Map Editor"))

    def _browse(self, entry):
        f = filedialog.askopenfilename(filetypes=[("Excel", "*.xlsx *.xlsm")])
        if f: entry.delete(0,"end"); entry.insert(0,f)

    # --- TAB 1: IMPORT ---
    def _build_imp(self, frame):
        ctk.CTkLabel(frame, text="Import Specification", font=("Arial", 18, "bold")).pack(anchor="w", pady=10)
        
        self.imp_mco = ctk.CTkEntry(frame, placeholder_text="MCO File Path")
        self.imp_mco.pack(fill="x", pady=5)
        ctk.CTkButton(frame, text="Browse", command=lambda: self._browse_mco_imp()).pack(fill="x", pady=5)
        
        ctk.CTkLabel(frame, text="Select Sheet:").pack(anchor="w")
        self.imp_sheet_var = ctk.StringVar(value="Load File First")
        self.imp_sheet_menu = ctk.CTkOptionMenu(frame, variable=self.imp_sheet_var)
        self.imp_sheet_menu.pack(fill="x", pady=5)
        
        self.imp_api = ctk.CTkEntry(frame, placeholder_text="Target API Name (e.g. MMS200MI)")
        self.imp_api.pack(fill="x", pady=10)
        ctk.CTkButton(frame, text="IMPORT RULES", fg_color="green", height=40, command=self._run_imp).pack(fill="x", pady=10)
        
        ctk.CTkButton(frame, text="VALIDATE MCO HEALTH", fg_color="#AA0000", command=self._run_check).pack(fill="x", pady=5)

    def _browse_mco_imp(self):
        f = filedialog.askopenfilename()
        if f:
            self.imp_mco.delete(0,"end"); self.imp_mco.insert(0,f)
            sheets = MCOImporter().get_sheet_names(f)
            self.imp_sheet_menu.configure(values=sheets)
            if sheets: self.imp_sheet_var.set(sheets[0])

    def _run_check(self):
        path = self.imp_mco.get()
        if not path or not os.path.exists(path):
            print("Please select a valid MCO file first.")
            return
        def _t():
            MCOChecker().check_file(path)
        threading.Thread(target=_t).start()

    def _run_imp(self):
        def _thread():
            path = self.imp_mco.get(); sheet = self.imp_sheet_var.get(); api = self.imp_api.get()
            if not api: api = sheet.split(' ')[0] + "MI"
            
            MCOImporter().run_import_headless(path, sheet, api)
            AuditManager().commit_changes(api)
            print(f"{Fore.GREEN}Import Complete for {api}.{Style.RESET_ALL}")
        threading.Thread(target=_thread).start()

    # --- TAB 2: ANALYZE ---
    def _build_an(self, frame):
        ctk.CTkLabel(frame, text="Incremental Analysis", font=("Arial", 18, "bold")).pack(anchor="w", pady=10)
        
        self.an_leg = ctk.CTkEntry(frame, placeholder_text="Legacy File"); self.an_leg.pack(fill="x", pady=5)
        ctk.CTkButton(frame, text="Browse Legacy", command=lambda: self._browse_leg()).pack(anchor="e")
        self.an_leg_sheet = ctk.CTkOptionMenu(frame, values=["Load File First"]); self.an_leg_sheet.pack(fill="x", pady=5)
        
        self.an_gold = ctk.CTkEntry(frame, placeholder_text="Gold M3 File"); self.an_gold.pack(fill="x", pady=5)
        ctk.CTkButton(frame, text="Browse Gold", command=lambda: self._browse_gold()).pack(anchor="e")
        self.an_gold_sheet = ctk.CTkOptionMenu(frame, values=["Load File First"]); self.an_gold_sheet.pack(fill="x", pady=5)

        self.an_target_var = ctk.StringVar(value="Target Rule Config")
        self.an_target_menu = ctk.CTkOptionMenu(frame, variable=self.an_target_var)
        self.an_target_menu.pack(fill="x", pady=10)
        self._refresh_an_configs()
        
        btn_frame = ctk.CTkFrame(frame, fg_color="transparent"); btn_frame.pack(fill="x", pady=20)
        ctk.CTkButton(btn_frame, text="ANALYZE & MERGE", fg_color="orange", text_color="black", height=40, command=self._run_an).pack(side="left", fill="x", expand=True, padx=5)
        ctk.CTkButton(btn_frame, text="MERGE DRAFT", fg_color="#444", height=40, command=self._run_merge_draft).pack(side="right", fill="x", expand=True, padx=5)

    def _refresh_an_configs(self):
        files = glob.glob('config/rules/*.xlsx')
        names = [os.path.basename(f).replace('.xlsx','') for f in files]
        if names: self.an_target_menu.configure(values=names); self.an_target_var.set(names[0])

    def _browse_leg(self):
        f = filedialog.askopenfilename()
        if f:
            self.an_leg.delete(0,"end"); self.an_leg.insert(0,f)
            sheets = MCOImporter().get_sheet_names(f)
            self.an_leg_sheet.configure(values=sheets); 
            if sheets: self.an_leg_sheet.set(sheets[0])

    def _browse_gold(self):
        f = filedialog.askopenfilename()
        if f:
            self.an_gold.delete(0,"end"); self.an_gold.insert(0,f)
            sheets = MCOImporter().get_sheet_names(f)
            self.an_gold_sheet.configure(values=sheets); 
            if sheets: self.an_gold_sheet.set(sheets[0])

    def _run_an(self):
        def _thread():
            print("\n--- Analysis ---")
            ex = DataExtractor()
            df_l = ex.load_data(self.an_leg.get(), format_type='MOVEX', sheet_name=self.an_leg_sheet.get())
            df_g = ex.load_data(self.an_gold.get(), format_type='M3_SDT', sheet_name=self.an_gold_sheet.get())
            
            key_l = df_l.columns[0]; key_g = df_g.columns[0]
            if 'MMITNO' in df_l.columns: key_l = 'MMITNO'
            if 'ITNO' in df_g.columns: key_g = 'ITNO'
            
            print(f"Keys: {key_l} -> {key_g}")
            sugg = ValidatorAnalyzer().reverse_engineer_rules(df_l, df_g, key_l, key_g)
            if not sugg.empty: 
                RuleManager().merge_draft_to_production(self.an_target_var.get(), sugg)
            else: print("No patterns.")
        threading.Thread(target=_thread).start()

    def _run_merge_draft(self):
        draft = filedialog.askopenfilename(filetypes=[("Excel", "*.xlsx")])
        if not draft: return
        target = self.an_target_var.get()
        def _t():
            print(f"Merging {os.path.basename(draft)} into {target}...")
            RuleManager().merge_draft_file(draft, target)
            print("Merge Complete.")
        threading.Thread(target=_t).start()

    # --- TAB 3: MAP EDITOR ---
    def _build_editor(self, frame):
        top = ctk.CTkFrame(frame, fg_color="transparent"); top.pack(fill="x", pady=5)
        
        self.map_files = {
            "Migration Map (API/SDT)": "migration_map.csv",
            "Source Map (Legacy Files)": "source_map.csv",
            "Surgical Def (Objects)": "surgical_def.csv",
            "Business Units (Scopes)": "business_units.csv" # <--- NEW ADDITION
        }
        
        self.map_var = ctk.StringVar(value=list(self.map_files.keys())[0])
        self.map_menu = ctk.CTkOptionMenu(top, variable=self.map_var, values=list(self.map_files.keys()), command=self._load_map)
        self.map_menu.pack(side="left", fill="x", expand=True)
        
        ctk.CTkButton(top, text="Save", width=80, fg_color="green", command=self._save_map).pack(side="right", padx=5)
        ctk.CTkButton(top, text="Reload", width=80, command=lambda: self._load_map(None)).pack(side="right")

        self.grid_frame = ctk.CTkScrollableFrame(frame, label_text="Config Data")
        self.grid_frame.pack(fill="both", expand=True, pady=10)
        
        ctk.CTkButton(frame, text="+ Add Row", command=self._add_row).pack(fill="x", pady=10)

        self.cells = []
        self.headers = []
        self._load_map(None)

    def _load_map(self, _):
        for widget in self.grid_frame.winfo_children(): widget.destroy()
        self.cells = []
        self.headers = []

        filename = self.map_files[self.map_var.get()]
        path = os.path.join('config', filename)
        
        # Create if missing
        if not os.path.exists(path):
            if "business_units" in filename:
                # Default seed for BU
                df = pd.DataFrame({'UNIT': ['GLOBAL'], 'DESCRIPTION': ['Global Rules']})
                df.to_csv(path, index=False)
            else:
                pd.DataFrame().to_csv(path, index=False)

        try:
            df = pd.read_csv(path).fillna("")
            self.headers = list(df.columns)
            
            for i, h in enumerate(self.headers):
                ctk.CTkLabel(self.grid_frame, text=h, font=("Arial", 12, "bold")).grid(row=0, column=i, padx=5, pady=5, sticky="w")

            for r_idx, row in df.iterrows():
                row_widgets = []
                for c_idx, col in enumerate(self.headers):
                    ent = ctk.CTkEntry(self.grid_frame)
                    ent.insert(0, str(row[col]))
                    ent.grid(row=r_idx+1, column=c_idx, padx=2, pady=2, sticky="ew")
                    row_widgets.append(ent)
                
                btn = ctk.CTkButton(self.grid_frame, text="X", width=30, fg_color="#550000", command=lambda r=r_idx: self._delete_row(r))
                btn.grid(row=r_idx+1, column=len(self.headers), padx=2)
                self.cells.append(row_widgets)
                
        except Exception as e: print(f"Error loading map: {e}")

    def _save_map(self):
        filename = self.map_files[self.map_var.get()]
        path = os.path.join('config', filename)
        data = []
        for row_widgets in self.cells:
            if row_widgets and row_widgets[0].winfo_exists():
                data.append({self.headers[i]: w.get() for i, w in enumerate(row_widgets)})
        pd.DataFrame(data).to_csv(path, index=False)
        print(f"Saved {filename}")
        self._load_map(None)

    def _add_row(self):
        r_idx = len(self.cells) + 1
        row_widgets = []
        for c_idx, _ in enumerate(self.headers):
            ent = ctk.CTkEntry(self.grid_frame)
            ent.grid(row=r_idx, column=c_idx, padx=2, pady=2, sticky="ew")
            row_widgets.append(ent)
        self.cells.append(row_widgets)

    def _delete_row(self, row_idx):
        filename = self.map_files[self.map_var.get()]
        path = os.path.join('config', filename)
        df = pd.read_csv(path)
        df = df.drop(row_idx)
        df.to_csv(path, index=False)
        self._load_map(None)
--- END FILE: .\modules\gui\tab_config.py ---

--- START FILE: .\modules\gui\tab_migration.py ---
import customtkinter as ctk
from tkinter import filedialog
import os
import glob
import threading
import pandas as pd
import datetime
from colorama import Fore, Style

# Logic Imports
from modules.migration_runner import MigrationRunner
from modules.auto_detector import AutoDetector
from modules.surgical_extractor import SurgicalExtractor
from modules.batch_processor import BatchProcessor

class MigrationHub(ctk.CTkTabview):
    def __init__(self, parent):
        super().__init__(parent)
        self.add("Standard")
        self.add("Auto-Detect")  # Renamed from Magic Drop
        self.add("Load by ID")   # Renamed from Surgical
        self.add("Batch")
        
        self._build_standard(self.tab("Standard"))
        self._build_auto_detect(self.tab("Auto-Detect"))
        self._build_load_by_id(self.tab("Load by ID"))
        self._build_batch(self.tab("Batch"))

    def _browse(self, entry):
        f = filedialog.askopenfilename(filetypes=[("Excel", "*.xlsx")])
        if f: entry.delete(0, "end"); entry.insert(0, f)

    def _refresh_configs(self, menu, var):
        files = glob.glob('config/rules/*.xlsx')
        names = [os.path.basename(f).replace('.xlsx','') for f in files] or ["No Rules Found"]
        menu.configure(values=names)
        var.set(names[0])

    # --- TAB 1: STANDARD ---
    def _build_standard(self, frame):
        ctk.CTkLabel(frame, text="Interactive Migration", font=("Arial", 18, "bold")).pack(anchor="w", pady=10)
        self.std_cfg_var = ctk.StringVar(value="Select Config")
        self.std_cfg_menu = ctk.CTkOptionMenu(frame, variable=self.std_cfg_var)
        self.std_cfg_menu.pack(fill="x", pady=5)
        self._refresh_configs(self.std_cfg_menu, self.std_cfg_var)
        
        src_box = ctk.CTkFrame(frame, fg_color="transparent"); src_box.pack(fill="x", pady=5)
        self.std_src = ctk.CTkEntry(src_box, placeholder_text="Legacy Source File")
        self.std_src.pack(side="left", fill="x", expand=True, padx=(0,5))
        ctk.CTkButton(src_box, text="Browse", width=80, command=lambda: self._browse(self.std_src)).pack(side="right")
        
        self.std_scope = ctk.CTkEntry(frame, placeholder_text="Scope (e.g. DIV_US) [Default: GLOBAL]")
        self.std_scope.pack(fill="x", pady=5)
        ctk.CTkButton(frame, text="RUN MIGRATION", fg_color="green", height=40, command=self._run_std).pack(fill="x", pady=20)

    def _run_std(self):
        threading.Thread(target=lambda: MigrationRunner().execute_migration(
            self.std_cfg_var.get(), self.std_src.get(), division=self.std_scope.get().upper() or "GLOBAL", silent=True
        )).start()

    # --- TAB 2: AUTO-DETECT (Was Magic Drop) ---
    def _build_auto_detect(self, frame):
        ctk.CTkLabel(frame, text="Auto-Detect File Load", font=("Arial", 18, "bold")).pack(anchor="w", pady=10)
        mco_box = ctk.CTkFrame(frame, fg_color="transparent"); mco_box.pack(fill="x")
        self.magic_mco = ctk.CTkEntry(mco_box, placeholder_text="MCO Spec Path (for signatures)"); self.magic_mco.pack(side="left", fill="x", expand=True, padx=(0,5))
        ctk.CTkButton(mco_box, text="Browse", width=80, command=lambda: self._browse(self.magic_mco)).pack(side="right")
        
        drop_box = ctk.CTkFrame(frame, fg_color="transparent"); drop_box.pack(fill="x", pady=10)
        self.magic_drop = ctk.CTkEntry(drop_box, placeholder_text="Legacy File to Process"); self.magic_drop.pack(side="left", fill="x", expand=True, padx=(0,5))
        ctk.CTkButton(drop_box, text="Browse", width=80, command=lambda: self._browse(self.magic_drop)).pack(side="right")
        
        ctk.CTkButton(frame, text="DETECT & RUN", fg_color="#550088", hover_color="#330066", height=40, command=self._run_auto_detect).pack(fill="x", pady=20)

    def _run_auto_detect(self):
        def _t():
            det = AutoDetector(self.magic_mco.get()); det.learn_signatures()
            p, sheet, api = det.identify_file(self.magic_drop.get())
            if not p: print(f"{Fore.RED}Detection Failed.{Style.RESET_ALL}"); return
            print(f"Detected: {p} -> {sheet} -> {api}")
            run = MigrationRunner()
            # Unpack 3 values
            map_api, map_sdt, _ = run.resolve_from_map_public(sheet, 'MCO_SHEET')
            
            final_api = map_api if map_api else api
            sdt_path = os.path.join('config/sdt_templates', map_sdt) if map_sdt else None
            
            # Safety Check
            if isinstance(final_api, list): final_api = final_api[0]
            if not final_api or final_api == "Unknown":
                print(f"{Fore.RED}Could not determine API Name.{Style.RESET_ALL}")
                return

            run.execute_migration(final_api, self.magic_drop.get(), auto_sdt=sdt_path, silent=True)
        threading.Thread(target=_t).start()

    # --- TAB 3: LOAD BY ID (Was Surgical) ---
    def _build_load_by_id(self, frame):
        ctk.CTkLabel(frame, text="Delta Load by ID", font=("Arial", 18, "bold")).pack(anchor="w", pady=10)
        self.surg_obj_var = ctk.StringVar(value="Select Object")
        self.surg_menu = ctk.CTkOptionMenu(frame, variable=self.surg_obj_var)
        self.surg_menu.pack(fill="x", pady=10)
        self._refresh_surg_objs()
        
        self.surg_ids = ctk.CTkTextbox(frame, height=80)
        self.surg_ids.pack(fill="x", pady=5)

        self.surg_scope = ctk.CTkEntry(frame, placeholder_text="Scope (Optional)")
        self.surg_scope.pack(fill="x", pady=10)

        ctk.CTkButton(frame, text="Refresh Objects", command=self._refresh_surg_objs).pack(fill="x", pady=5)
        ctk.CTkButton(frame, text="RUN DELTA LOAD", fg_color="orange", text_color="black", height=40, command=self._run_load_by_id).pack(fill="x", pady=10)

    def _refresh_surg_objs(self):
        try:
            objs = SurgicalExtractor().get_available_objects()
            if objs:
                self.surg_menu.configure(values=objs)
                self.surg_obj_var.set(objs[0])
        except: pass

    def _run_load_by_id(self):
        def _t():
            raw_ids = self.surg_ids.get("1.0", "end").strip()
            ids = [x.strip() for x in raw_ids.split(',') if x.strip()]
            if not ids:
                print(f"{Fore.RED}No IDs provided.{Style.RESET_ALL}"); return

            obj_type = self.surg_obj_var.get()
            if not obj_type or obj_type == "Select Object":
                print(f"{Fore.RED}No object selected.{Style.RESET_ALL}"); return

            extractor = SurgicalExtractor()
            tasks = extractor.perform_extraction(obj_type, ids)
            if not tasks:
                print(f"{Fore.RED}No tasks returned.{Style.RESET_ALL}"); return
            
            runner = MigrationRunner()
            
            # Define Master Filename
            base_prog = tasks[0]['program_name']
            ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            master_name = f"LOAD_{base_prog}_SURGICAL_{ts}.xlsx"
            print(f"Target Output: output/{master_name}")

            for t in tasks:
                # Unpack 3 values
                _, _, trans = runner.resolve_from_map_public(t['mco_sheet'], 'MCO_SHEET')
                
                targets = None
                if isinstance(trans, list): targets = [x.strip() for x in trans if x]
                elif isinstance(trans, str): targets = [x.strip() for x in trans.split(',') if x.strip()]
                
                if not targets:
                    print(f"{Fore.YELLOW}    Warning: No TRANSACTION_SHEET defined for {t['mco_sheet']}. Skipping.{Style.RESET_ALL}")
                    continue

                runner.execute_migration(
                    t['program_name'], 
                    t['legacy_path'], 
                    division=self.surg_scope.get().upper() or "GLOBAL", 
                    target_sheets=targets, 
                    silent=True, 
                    output_name_override=master_name
                )
        threading.Thread(target=_t).start()

    # --- TAB 4: BATCH ---
    def _build_batch(self, frame):
        ctk.CTkLabel(frame, text="Batch Processor", font=("Arial", 18, "bold")).pack(anchor="w", pady=10)
        box = ctk.CTkFrame(frame, fg_color="transparent"); box.pack(fill="x")
        self.batch_ent = ctk.CTkEntry(box, placeholder_text="Batch Definition File"); self.batch_ent.pack(side="left", fill="x", expand=True, padx=(0,5))
        ctk.CTkButton(box, text="Browse", width=80, command=lambda: self._browse(self.batch_ent)).pack(side="right")
        ctk.CTkButton(frame, text="EXECUTE BATCH", fg_color="#880000", height=40, command=self._run_batch).pack(fill="x", pady=20)

    def _run_batch(self):
        threading.Thread(target=lambda: BatchProcessor().run_batch_execution(BatchProcessor().load_batch_file(self.batch_ent.get()))).start()
--- END FILE: .\modules\gui\tab_migration.py ---

--- START FILE: .\modules\gui\tab_rules.py ---
import customtkinter as ctk
import pandas as pd
import glob
import os
import inspect
from colorama import Fore, Style
from modules.audit_manager import AuditManager
from modules.ui import select_file
try:
    import modules.hooks as hooks
except ImportError:
    hooks = None

class RulesHub(ctk.CTkTabview):
    def __init__(self, parent):
        super().__init__(parent)
        self.add("Editor")
        self.add("History")
        self.add("Tools")
        
        self.curr_df = None
        self.curr_path = None
        self.curr_idx = None

        self._build_editor(self.tab("Editor"))
        self._build_hist(self.tab("History"))
        self._build_tools(self.tab("Tools"))

    # --- TAB 1: EDITOR ---
    def _build_editor(self, frame):
        top = ctk.CTkFrame(frame, fg_color="transparent"); top.pack(fill="x")
        
        self.ed_var = ctk.StringVar()
        self.ed_menu = ctk.CTkOptionMenu(top, variable=self.ed_var, command=self._load_ed) 
        self.ed_menu.pack(side="left", fill="x", expand=True)
        
        ctk.CTkButton(top, text="Refresh", width=80, command=self._refresh).pack(side="right")
        
        content = ctk.CTkFrame(frame, fg_color="transparent"); content.pack(fill="both", expand=True, pady=10)
        
        # List
        self.ed_list = ctk.CTkScrollableFrame(content, width=250, label_text="Fields")
        self.ed_list.pack(side="left", fill="y", padx=(0, 10))
        
        # Form
        self.ed_form = ctk.CTkFrame(content); self.ed_form.pack(side="right", fill="both", expand=True)
        
        # 1. Target
        ctk.CTkLabel(self.ed_form, text="TARGET FIELD:").pack(anchor="w", padx=10, pady=(10,0))
        self.ed_lbl = ctk.CTkLabel(self.ed_form, text="-", font=("Arial", 20, "bold")); self.ed_lbl.pack(anchor="w", padx=10)
        
        # 2. Context / Scope (NEW)
        ctk.CTkLabel(self.ed_form, text="Context (Scope):").pack(anchor="w", padx=10)
        self.ed_scope = ctk.CTkComboBox(self.ed_form) # Combobox allows typing or selecting
        self.ed_scope.pack(fill="x", padx=10)

        # 3. Type
        ctk.CTkLabel(self.ed_form, text="Rule Type:").pack(anchor="w", padx=10)
        self.ed_type = ctk.CTkOptionMenu(
            self.ed_form, 
            values=["DIRECT", "CONST", "MAP", "PYTHON", "IGNORE", "TODO"],
            command=self._update_state
        )
        self.ed_type.pack(fill="x", padx=10)
        
        # 4. Source
        self.lbl_src = ctk.CTkLabel(self.ed_form, text="Source Field:")
        self.lbl_src.pack(anchor="w", padx=10)
        self.ed_src = ctk.CTkEntry(self.ed_form)
        self.ed_src.pack(fill="x", padx=10)
        
        # 5. Value
        self.lbl_val = ctk.CTkLabel(self.ed_form, text="Value / Logic:")
        self.lbl_val.pack(anchor="w", padx=10)
        
        val_box = ctk.CTkFrame(self.ed_form, fg_color="transparent"); val_box.pack(fill="both", expand=True, padx=10)
        self.ed_val = ctk.CTkTextbox(val_box, height=120, font=("Consolas", 12))
        self.ed_val.pack(side="left", fill="both", expand=True)
        self.btn_map = ctk.CTkButton(val_box, text="...", width=30, command=self._run_map_wizard)
        
        ctk.CTkButton(self.ed_form, text="SAVE CHANGE", fg_color="green", height=40, command=self._save).pack(fill="x", padx=10, pady=20)
        
        self._refresh()

    def _refresh(self):
        # Load Scopes from CSV
        scopes = ["GLOBAL"]
        if os.path.exists("config/business_units.csv"):
            try:
                df_bu = pd.read_csv("config/business_units.csv")
                scopes.extend(df_bu['UNIT'].dropna().unique().tolist())
            except: pass
        self.ed_scope.configure(values=scopes)

        files = glob.glob('config/rules/*.xlsx')
        names = [os.path.basename(f).replace('.xlsx','') for f in files]
        
        if names: 
            self.ed_menu.configure(values=names)
            if not self.ed_var.get() or self.ed_var.get() == "No Rules":
                self.ed_var.set(names[0])
            self.after(100, lambda: self._load_ed(self.ed_var.get()))
        else:
            self.ed_menu.configure(values=["No Rules"])
            self.ed_var.set("No Rules")

    def _load_ed(self, selection):
        if selection == "No Rules":
            self.curr_df = None
            for w in self.ed_list.winfo_children(): w.destroy()
            return

        self.curr_path = f"config/rules/{selection}.xlsx"
        try:
            self.curr_df = pd.read_excel(self.curr_path, sheet_name='Rules')
            self.curr_df.fillna("", inplace=True)
            
            for w in self.ed_list.winfo_children(): w.destroy()
            
            for idx, row in self.curr_df.iterrows():
                col = "transparent"
                if row['RULE_TYPE'] == 'TODO': col = "#550000"
                elif row['RULE_TYPE'] == 'IGNORE': col = "#333333"
                
                btn = ctk.CTkButton(
                    self.ed_list, text=row['TARGET_FIELD'], fg_color=col, 
                    height=24, anchor="w", command=lambda x=idx: self._load_row(x)
                )
                btn.pack(fill="x", pady=1)
        except Exception as e: 
            print(f"Error loading rules: {e}")
            self.curr_df = None

    def _load_row(self, idx):
        if self.curr_df is None: return
        self.curr_idx = idx
        try:
            r = self.curr_df.iloc[idx]
            self.ed_lbl.configure(text=r['TARGET_FIELD'])
            
            self.ed_type.set(r['RULE_TYPE'])
            
            # Load Scope
            scope_val = str(r['SCOPE']) if 'SCOPE' in r else 'GLOBAL'
            self.ed_scope.set(scope_val)

            self.ed_src.delete(0, "end")
            self.ed_src.insert(0, str(r['SOURCE_FIELD']))
            
            self.ed_val.delete("0.0", "end")
            self.ed_val.insert("0.0", str(r['RULE_VALUE']))
            
            self._update_state(r['RULE_TYPE'], insert_template=False)
            
        except Exception as e:
            print(f"Error loading row {idx}: {e}")

    def _on_type_change(self, choice):
        self._update_state(choice, insert_template=True)

    def _update_state(self, choice, insert_template=False):
        # Reset
        self.ed_src.configure(state="normal", fg_color=["#F9F9FA", "#343638"])
        self.ed_val.configure(state="normal", fg_color=["#F9F9FA", "#343638"])
        self.btn_map.pack_forget() 

        if choice == 'CONST':
            self.lbl_src.configure(text="Source Field (Disabled):")
            self.lbl_val.configure(text="Constant Value:")
            self.ed_src.delete(0, "end")
            self.ed_src.configure(state="disabled", fg_color=["#EBEBEB", "#2B2B2B"])
            
        elif choice == 'DIRECT':
            self.lbl_src.configure(text="Source Field:")
            self.lbl_val.configure(text="Value (Disabled):")
            self.ed_val.delete("0.0", "end")
            self.ed_val.configure(state="disabled", fg_color=["#EBEBEB", "#2B2B2B"])
            
        elif choice == 'PYTHON':
            self.lbl_src.configure(text="Source Field (Variable 'source'):")
            self.lbl_val.configure(text="Python Code Body:")
            
            current_text = self.ed_val.get("0.0", "end").strip()
            if insert_template and not current_text:
                template = """# Write logic here.
# Variables: 'source' (value), 'row' (full record)

if source == '10':
    return 'Active'
elif source == '90':
    return 'Inactive'
else:
    return source"""
                self.ed_val.insert("0.0", template)
            
        elif choice == 'MAP':
            self.lbl_src.configure(text="Source Field (Key):")
            self.lbl_val.configure(text="Map Config (File|Key|Val):")
            self.btn_map.pack(side="right", padx=5, fill="y")

        elif choice in ['IGNORE', 'TODO']:
            self.lbl_src.configure(text="Source Field:")
            self.lbl_val.configure(text="Value:")
            self.ed_src.delete(0, "end"); self.ed_src.configure(state="disabled", fg_color=["#EBEBEB", "#2B2B2B"])
            self.ed_val.delete("0.0", "end"); self.ed_val.configure(state="disabled", fg_color=["#EBEBEB", "#2B2B2B"])

    def _run_map_wizard(self):
        f = select_file("Select Map Excel/CSV", [("Data", "*.xlsx *.csv")])
        if f:
            try:
                try: rel_path = os.path.relpath(f)
                except: rel_path = f
                
                new_val = f"{rel_path}|KEY_COL|VAL_COL"
                self.ed_val.delete("0.0", "end")
                self.ed_val.insert("0.0", new_val)
            except:
                self.ed_val.insert("0.0", f"{f}|KEY|VAL")

    def _save(self):
        if self.curr_idx is None or self.curr_df is None: return
        
        rtype = self.ed_type.get()
        src = self.ed_src.get()
        val = self.ed_val.get("0.0", "end").strip()
        scope = self.ed_scope.get() # <--- SAVE SCOPE
        
        if rtype == 'CONST': src = ""
        if rtype == 'DIRECT': val = ""
        if rtype in ['IGNORE', 'TODO']: src = ""; val = ""

        self.curr_df.at[self.curr_idx, 'RULE_TYPE'] = rtype
        self.curr_df.at[self.curr_idx, 'SOURCE_FIELD'] = src
        self.curr_df.at[self.curr_idx, 'RULE_VALUE'] = val
        self.curr_df.at[self.curr_idx, 'SCOPE'] = scope
        
        try:
            with pd.ExcelWriter(self.curr_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as w:
                self.curr_df.to_excel(w, sheet_name='Rules', index=False)
            
            AuditManager().commit_changes(self.ed_var.get())
            print(f"Updated {self.ed_lbl.cget('text')}")
            self._load_ed(self.ed_var.get())
            self._load_row(self.curr_idx)
            
        except Exception as e:
            print(f"Save failed: {e}")

    # --- HISTORY & TOOLS ---
    def _build_hist(self, frame):
        self.hist_var = ctk.StringVar()
        self.hist_menu = ctk.CTkOptionMenu(frame, variable=self.hist_var); self.hist_menu.pack(fill="x", pady=5)
        ctk.CTkButton(frame, text="Load Log", command=self._load_log).pack(pady=5)
        self.hist_box = ctk.CTkTextbox(frame, font=("Consolas", 11)); self.hist_box.pack(fill="both", expand=True)
        files = glob.glob('config/rules/*.xlsx')
        names = [os.path.basename(f).replace('.xlsx','') for f in files]
        if names: self.hist_menu.configure(values=names); self.hist_var.set(names[0])

    def _load_log(self):
        try:
            df = pd.read_excel(f"config/rules/{self.hist_var.get()}.xlsx", sheet_name='_Audit_Log')
            self.hist_box.configure(state="normal"); self.hist_box.delete("0.0", "end")
            self.hist_box.insert("0.0", df.to_string(index=False)); self.hist_box.configure(state="disabled")
        except: pass

    def _build_tools(self, frame):
        ctk.CTkLabel(frame, text="System Tools", font=("Arial", 18, "bold")).pack(pady=20)
        ctk.CTkButton(frame, text="Commit All Excel Edits", command=self._commit).pack(pady=10)
        ctk.CTkButton(frame, text="HARD RESET SYSTEM", fg_color="red", command=lambda: AuditManager().hard_reset()).pack(pady=20)

    def _commit(self):
        files = glob.glob('config/rules/*.xlsx')
        am = AuditManager()
        for f in files: am.commit_changes(os.path.basename(f).replace('.xlsx',''))
        print("All files audited.")
--- END FILE: .\modules\gui\tab_rules.py ---

--- START FILE: .\modules\gui\tab_utils.py ---
import customtkinter as ctk
from tkinter import filedialog
import threading
import openpyxl
import os
from colorama import Fore, Style  # <--- This import was missing
from modules.sdt_utils import SDTUtils

class UtilitiesHub(ctk.CTkTabview):
    def __init__(self, parent):
        super().__init__(parent)
        self.add("Copy Sheet")
        self.add("Merge Files")
        
        self._build_copy(self.tab("Copy Sheet"))
        self._build_merge(self.tab("Merge Files"))

    # --- COPY SHEET TAB ---
    def _build_copy(self, frame):
        ctk.CTkLabel(frame, text="Copy SDT Sheet Data", font=("Arial", 18, "bold")).pack(anchor="w", pady=10)
        
        # File Selection
        self.file_entry = ctk.CTkEntry(frame, placeholder_text="Select SDT File")
        self.file_entry.pack(fill="x", pady=5)
        ctk.CTkButton(frame, text="Browse & Load", command=self._load_file).pack(anchor="e", pady=5)
        
        # Dropdowns
        ctk.CTkLabel(frame, text="Source Sheet:").pack(anchor="w", pady=(10,0))
        self.src_var = ctk.StringVar(value="")
        self.src_menu = ctk.CTkOptionMenu(frame, variable=self.src_var, values=["Load File First"])
        self.src_menu.pack(fill="x")
        
        ctk.CTkLabel(frame, text="Destination Sheet:").pack(anchor="w", pady=(10,0))
        self.dst_var = ctk.StringVar(value="")
        self.dst_menu = ctk.CTkOptionMenu(frame, variable=self.dst_var, values=["Load File First"])
        self.dst_menu.pack(fill="x")
        
        # Option
        self.nok_var = ctk.BooleanVar(value=True)
        ctk.CTkCheckBox(frame, text="Include 'NOK' rows", variable=self.nok_var).pack(anchor="w", pady=15)
        
        ctk.CTkButton(frame, text="COPY DATA", fg_color="orange", text_color="black", height=40, command=self._run_copy).pack(fill="x", pady=10)
        
        self.wb = None
        self.filepath = None

    def _load_file(self):
        f = filedialog.askopenfilename(filetypes=[("Excel", "*.xlsx")])
        if not f: return
        self.file_entry.delete(0,"end"); self.file_entry.insert(0, f)
        self.filepath = f
        
        try:
            self.wb = openpyxl.load_workbook(f)
            sheets = self.wb.sheetnames
            self.src_menu.configure(values=sheets); self.src_var.set(sheets[0])
            self.dst_menu.configure(values=sheets); self.dst_var.set(sheets[0])
            print(f"Loaded {os.path.basename(f)}")
        except Exception as e:
            print(f"Error loading file: {e}")

    def _run_copy(self):
        if not self.wb or not self.filepath: return
        
        src = self.src_var.get()
        dst = self.dst_var.get()
        
        def _t():
            try:
                utils = SDTUtils()
                ws_src = self.wb[src]
                ws_dst = self.wb[dst]
                
                print(f"Copying {src} -> {dst}...")
                
                matched, copied = utils._map_and_copy_data(ws_src, ws_dst)
                self.wb.save(self.filepath)
                print(f"{Fore.GREEN}Success! Copied {copied} rows.{Style.RESET_ALL}")
                
            except Exception as e:
                print(f"{Fore.RED}Error: {e}{Style.RESET_ALL}")
                
        threading.Thread(target=_t).start()

    # --- MERGE FILES TAB ---
    def _build_merge(self, frame):
        ctk.CTkLabel(frame, text="Merge Files (Deduplicate)", font=("Arial", 18, "bold")).pack(anchor="w", pady=10)

        # Master File
        ctk.CTkLabel(frame, text="1. Master File (Destination):").pack(anchor="w")
        self.merge_master_ent = ctk.CTkEntry(frame)
        self.merge_master_ent.pack(fill="x", pady=5)
        ctk.CTkButton(frame, text="Select Master", command=self._sel_master).pack(anchor="e")

        # Source File
        ctk.CTkLabel(frame, text="2. Source File (Data to Add):").pack(anchor="w", pady=(10,0))
        self.merge_src_ent = ctk.CTkEntry(frame)
        self.merge_src_ent.pack(fill="x", pady=5)
        ctk.CTkButton(frame, text="Select Source", command=self._sel_src).pack(anchor="e")

        # Sheet Select
        ctk.CTkLabel(frame, text="Select Common Sheet:").pack(anchor="w", pady=(10,0))
        self.merge_sheet_var = ctk.StringVar(value="Select Files First")
        self.merge_sheet_menu = ctk.CTkOptionMenu(frame, variable=self.merge_sheet_var)
        self.merge_sheet_menu.pack(fill="x", pady=5)
        
        ctk.CTkButton(frame, text="MERGE & SAVE", fg_color="green", height=40, command=self._run_merge).pack(fill="x", pady=20)
        
        self.wb_master = None
        self.path_master = None
        self.path_source = None

    def _sel_master(self):
        f = filedialog.askopenfilename(filetypes=[("Excel", "*.xlsx")])
        if f:
            self.merge_master_ent.delete(0,"end"); self.merge_master_ent.insert(0,f)
            self.path_master = f
            try:
                self.wb_master = openpyxl.load_workbook(f)
                print(f"Master Loaded: {os.path.basename(f)}")
            except Exception as e: print(f"Error: {e}")

    def _sel_src(self):
        f = filedialog.askopenfilename(filetypes=[("Excel", "*.xlsx")])
        if f:
            self.merge_src_ent.delete(0,"end"); self.merge_src_ent.insert(0,f)
            self.path_source = f
            self._update_common_sheets()

    def _update_common_sheets(self):
        if not self.wb_master or not self.path_source: return
        try:
            wb_src = openpyxl.load_workbook(self.path_source, read_only=True)
            src_sheets = wb_src.sheetnames
            master_sheets = self.wb_master.sheetnames
            
            common = [s for s in src_sheets if s in master_sheets]
            if common:
                self.merge_sheet_menu.configure(values=common)
                self.merge_sheet_var.set(common[0])
            else:
                self.merge_sheet_menu.configure(values=["No Common Sheets"])
                self.merge_sheet_var.set("No Common Sheets")
        except Exception as e: print(f"Error reading source: {e}")

    def _run_merge(self):
        if not self.wb_master or not self.path_source: return
        
        sheet = self.merge_sheet_var.get()
        if sheet == "No Common Sheets": return

        def _t():
            try:
                utils = SDTUtils()
                wb_src = openpyxl.load_workbook(self.path_source, data_only=True)
                
                print(f"Merging '{sheet}'...")
                count = utils._merge_sheet_data(self.wb_master[sheet], wb_src[sheet])
                
                print(f"Saving Master...")
                self.wb_master.save(self.path_master)
                print(f"{Fore.GREEN}Success! Added {count} unique rows.{Style.RESET_ALL}")
            except Exception as e:
                # This print now works because Fore/Style are imported
                print(f"Merge Error: {e}")
            
        threading.Thread(target=_t).start()
--- END FILE: .\modules\gui\tab_utils.py ---

--- START FILE: .\modules\gui\utils.py ---
import customtkinter as ctk

class TextRedirector:
    def __init__(self, widget, tag="stdout"):
        self.widget = widget
        self.tag = tag
    def write(self, str):
        try:
            self.widget.configure(state="normal")
            self.widget.insert("end", str, (self.tag,))
            self.widget.see("end")
            self.widget.configure(state="disabled")
            self.widget.update_idletasks()
        except: pass
    def flush(self): pass
--- END FILE: .\modules\gui\utils.py ---

--- START FILE: .\tests\conftest.py ---
import pytest
import pandas as pd
import os
import shutil

# Constants
TEST_CONFIG_DIR = "tests/config_temp"
TEST_DATA_DIR = "tests/data_temp"
TEST_OUTPUT_DIR = "tests/output_temp"
TEST_STAGE_DIR = "tests/surgical_staging"

@pytest.fixture(scope="session", autouse=True)
def setup_test_env():
    # 1. Clean & Create Dirs
    for d in [TEST_CONFIG_DIR, TEST_DATA_DIR, TEST_OUTPUT_DIR, TEST_STAGE_DIR]:
        if os.path.exists(d): shutil.rmtree(d)
        os.makedirs(d)
    
    os.makedirs(f"{TEST_CONFIG_DIR}/rules")
    os.makedirs(f"{TEST_CONFIG_DIR}/sdt_templates")

    # --- DATA GENERATION ---

    # A. Legacy Data
    df_legacy = pd.DataFrame({
        'MMITNO': ['1001', '1002', '1003'],
        'MMITDS': ['Item A', 'Item B', 'Item C'],
        'MMSTAT': ['20', '50', '20']
    })
    df_legacy.to_excel(f"{TEST_DATA_DIR}/test_source.xlsx", index=False)

    # B. SDT Template
    df_sdt = pd.DataFrame(columns=['CONO', 'ITNO', 'ITDS', 'STAT', 'RESP'])
    with pd.ExcelWriter(f"{TEST_CONFIG_DIR}/sdt_templates/TEST_API.xlsx", engine='xlsxwriter') as writer:
        df_sdt.to_excel(writer, sheet_name='AddBasic', startrow=3, header=False, index=False)
        ws = writer.sheets['AddBasic']
        for i, col in enumerate(df_sdt.columns): ws.write(0, i, col)
        
        df_sdt.to_excel(writer, sheet_name='AddWhs', startrow=3, header=False, index=False)
        ws2 = writer.sheets['AddWhs']
        for i, col in enumerate(df_sdt.columns): ws2.write(0, i, col)

    # C. Rule Configs
    df_rules = pd.DataFrame([
        {'TARGET_FIELD': 'ITNO', 'SOURCE_FIELD': 'MMITNO', 'RULE_TYPE': 'DIRECT', 'RULE_VALUE': '', 'SCOPE': 'GLOBAL'},
        {'TARGET_FIELD': 'ITDS', 'SOURCE_FIELD': 'MMITDS', 'RULE_TYPE': 'DIRECT', 'RULE_VALUE': '', 'SCOPE': 'GLOBAL'},
        {'TARGET_FIELD': 'STAT', 'SOURCE_FIELD': '', 'RULE_TYPE': 'CONST', 'RULE_VALUE': '20', 'SCOPE': 'GLOBAL'},
        {'TARGET_FIELD': 'RESP', 'SOURCE_FIELD': '', 'RULE_TYPE': 'CONST', 'RULE_VALUE': 'TESTUSER', 'SCOPE': 'GLOBAL'},
    ])
    df_rules.to_excel(f"{TEST_CONFIG_DIR}/rules/TEST_RULES.xlsx", sheet_name='Rules', index=False)

    df_scoped = pd.DataFrame([
        {'TARGET_FIELD': 'ITNO', 'SOURCE_FIELD': 'MMITNO', 'RULE_TYPE': 'DIRECT', 'RULE_VALUE': '', 'SCOPE': 'GLOBAL'},
        {'TARGET_FIELD': 'STAT', 'SOURCE_FIELD': '', 'RULE_TYPE': 'CONST', 'RULE_VALUE': '10', 'SCOPE': 'GLOBAL'},
        {'TARGET_FIELD': 'STAT', 'SOURCE_FIELD': '', 'RULE_TYPE': 'CONST', 'RULE_VALUE': '90', 'SCOPE': 'DIV_US'},
    ])
    df_scoped.to_excel(f"{TEST_CONFIG_DIR}/rules/TEST_SCOPED.xlsx", sheet_name='Rules', index=False)

    # D. Maps
    pd.DataFrame([{'OBJECT_TYPE': 'ITEM', 'MCO_SHEET': 'Test_Sheet'}]).to_csv(f"{TEST_CONFIG_DIR}/surgical_def.csv", index=False)
    pd.DataFrame([{'MCO_SHEET': 'Test_Sheet', 'SOURCE_FILE': f"{TEST_DATA_DIR}/test_source.xlsx", 'JOIN_KEY': 'MMITNO'}]).to_csv(f"{TEST_CONFIG_DIR}/source_map.csv", index=False)
    pd.DataFrame([{'MCO_SHEET': 'Test_Sheet', 'API_NAME': 'TEST_API', 'SDT_TEMPLATE': 'TEST_API.xlsx', 'TRANSACTION_SHEET': 'AddBasic, AddWhs'}]).to_csv(f"{TEST_CONFIG_DIR}/migration_map.csv", index=False)

    # E. MCO Files
    # 1. Valid MCO
    df_mco_valid = pd.DataFrame({
        'Field name': ['ITNO', 'ITDS', 'STAT'],
        'Data Converison Source': ['MMITNO', 'MMITDS', ''],
        'Customer Required': ['1', '1', '1'],
        'Transformation Rule': ['Direct', 'Direct', 'Const 20']
    })
    with pd.ExcelWriter(f"{TEST_DATA_DIR}/MCO_VALID.xlsx") as writer:
        df_mco_valid.to_excel(writer, sheet_name='Sheet1', startrow=2, index=False)

    # 2. Update MCO (For Merge Test - Valid but different source)
    df_mco_update = pd.DataFrame({
        'Field name': ['ITNO'],
        'Data Converison Source': ['MMITNO_UPDATED'],
        'Customer Required': ['1'],
        'Transformation Rule': ['Direct']
    })
    with pd.ExcelWriter(f"{TEST_DATA_DIR}/MCO_UPDATE.xlsx") as writer:
        df_mco_update.to_excel(writer, sheet_name='Sheet1', startrow=2, index=False)

    # 3. Duplicate Error MCO
    df_mco_bad = pd.DataFrame({
        'Field name': ['ITNO', 'ITNO'],
        'Data Converison Source': ['MMITNO', 'MMITNO_BACKUP'],
        'Customer Required': ['1', '1']
    })
    with pd.ExcelWriter(f"{TEST_DATA_DIR}/MCO_DUPE.xlsx") as writer:
        df_mco_bad.to_excel(writer, sheet_name='Sheet1', startrow=2, index=False)

    yield
--- END FILE: .\tests\conftest.py ---

--- START FILE: .\tests\test_core.py ---
import pytest
import pandas as pd
import os
import sys
import glob
import datetime
from openpyxl import load_workbook

# Add root directory to path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from modules.config_loader import ConfigLoader
from modules.extractor import DataExtractor
from modules.transform_engine import TransformEngine
from modules.sdt_writer import SDTWriter
from modules.validator_analyzer import ValidatorAnalyzer
from modules.surgical_extractor import SurgicalExtractor
from modules.mco_importer import MCOImporter
from modules.migration_runner import MigrationRunner
from modules.batch_processor import BatchProcessor
from modules.mco_checker import MCOChecker
import modules.hooks as hooks

CONF_DIR = "tests/config_temp"
DATA_DIR = "tests/data_temp"
OUT_DIR = "tests/output_temp"
STAGE_DIR = "tests/surgical_staging"

# --- EXISTING TESTS ---

def test_01_extractor_finds_headers():
    ext = DataExtractor()
    df = ext.load_data(f"{DATA_DIR}/test_source.xlsx", format_type='MOVEX', sheet_name=0)
    assert not df.empty and 'MMITNO' in df.columns
    print("[PASS] Extractor loaded data.")

def test_02_config_loader():
    path = f"{CONF_DIR}/rules/TEST_RULES.xlsx"
    df = pd.read_excel(path, sheet_name='Rules')
    assert len(df) == 4
    print("[PASS] Config Loader read rules.")

def test_03_transformation_logic():
    df_source = pd.read_excel(f"{DATA_DIR}/test_source.xlsx")
    df_rules = pd.read_excel(f"{CONF_DIR}/rules/TEST_RULES.xlsx")
    engine = TransformEngine(df_rules, {})
    df_result = engine.process(df_source)
    assert str(df_result.iloc[0]['ITNO']) == '1001'
    val = str(df_result.iloc[1]['STAT']).split('.')[0]
    assert val == '20' 
    print("[PASS] Transform Logic.")

def test_04_sdt_writer_generation():
    writer = SDTWriter(output_dir=OUT_DIR)
    df_rules = pd.DataFrame([
        {'TARGET_FIELD': 'ITNO', 'RULE_TYPE': 'DIRECT', 'SOURCE_FIELD': 'MMITNO', 'RULE_VALUE': ''}
    ])
    template_path = f"{CONF_DIR}/sdt_templates/TEST_API.xlsx"
    writer.generate_from_template(
        template_path, pd.read_excel(f"{DATA_DIR}/test_source.xlsx"), df_rules, ['AddBasic'], "TEST_LOAD.xlsx"
    )
    files = glob.glob(f"{OUT_DIR}/TEST_LOAD_*.xlsx")
    assert len(files) > 0
    print("[PASS] SDT Writer generated file.")

def test_05_data_sanitization():
    writer = SDTWriter(output_dir=OUT_DIR)
    df_dirty = pd.DataFrame({'MMITNO': ['A'], 'M1': [float('nan')]})
    df_rules = pd.DataFrame([{'TARGET_FIELD': 'ITNO', 'RULE_TYPE': 'DIRECT', 'SOURCE_FIELD': 'MMITNO', 'RULE_VALUE': ''}])
    writer.generate_from_template(f"{CONF_DIR}/sdt_templates/TEST_API.xlsx", df_dirty, df_rules, ['AddBasic'], "TEST_DIRTY.xlsx")
    print("[PASS] Data Sanitization.")

def test_06_python_hooks():
    code_snippet = "if source == 1: return 'HOOKED'\nreturn 'NOPE'"
    df_s = pd.DataFrame({'A': [1]})
    df_r = pd.DataFrame([{'TARGET_FIELD':'B', 'RULE_TYPE':'PYTHON', 'RULE_VALUE': code_snippet, 'SOURCE_FIELD':'A'}])
    eng = TransformEngine(df_r, {})
    res = eng.process(df_s)
    assert res.iloc[0]['B'] == 'HOOKED'
    print("[PASS] Python Hooks.")

def test_07_surgical_extraction():
    extractor = SurgicalExtractor()
    extractor.config_dir = CONF_DIR 
    extractor.staging_dir = STAGE_DIR
    tasks = extractor.perform_extraction('ITEM', ['1002'])
    assert len(tasks) == 1
    staged_file = tasks[0]['legacy_path']
    assert os.path.exists(staged_file)
    df = pd.read_excel(staged_file)
    assert str(df.iloc[0]['MMITNO']) == '1002'
    print("[PASS] Surgical Extraction filtered correctly.")

def test_08_mco_import_valid():
    importer = MCOImporter(sdt_folder=f"{CONF_DIR}/sdt_templates")
    importer.run_import_headless(f"{DATA_DIR}/MCO_VALID.xlsx", "Sheet1", "MCO_TEST_API", output_dir=f"{CONF_DIR}/rules")
    rule_path = f"{CONF_DIR}/rules/MCO_TEST_API.xlsx"
    assert os.path.exists(rule_path)
    df = pd.read_excel(rule_path, sheet_name='Rules')
    assert 'ITNO' in df['TARGET_FIELD'].values
    print("[PASS] MCO Import parsed valid file.")

def test_09_mco_merge_behavior():
    importer = MCOImporter(sdt_folder=f"{CONF_DIR}/sdt_templates")
    api_name = "MCO_MERGE_TEST"
    importer.run_import_headless(f"{DATA_DIR}/MCO_VALID.xlsx", "Sheet1", api_name, output_dir=f"{CONF_DIR}/rules")
    importer.run_import_headless(f"{DATA_DIR}/MCO_UPDATE.xlsx", "Sheet1", api_name, output_dir=f"{CONF_DIR}/rules")
    rule_path = f"{CONF_DIR}/rules/{api_name}.xlsx"
    df = pd.read_excel(rule_path, sheet_name='Rules')
    itno_row = df[df['TARGET_FIELD'] == 'ITNO'].iloc[0]
    assert itno_row['SOURCE_FIELD'] == 'MMITNO_UPDATED'
    print("[PASS] MCO Importer merged duplicates.")

def test_10_map_parsing_logic():
    runner = MigrationRunner()
    raw_trans = "AddBasic, AddWhs "
    sheets = [t.strip() for t in raw_trans.split(',') if t.strip()]
    assert len(sheets) == 2
    print("[PASS] Map parsing logic confirmed.")

def test_11_sdt_append_mode():
    writer = SDTWriter(output_dir=OUT_DIR)
    template = f"{CONF_DIR}/sdt_templates/TEST_API.xlsx"
    filename = "TEST_SURGICAL_MERGE.xlsx"
    df1 = pd.DataFrame({'MMITNO': ['A']})
    df_rules = pd.DataFrame([{'TARGET_FIELD': 'ITNO', 'RULE_TYPE': 'DIRECT', 'SOURCE_FIELD': 'MMITNO', 'RULE_VALUE': ''}])
    writer.generate_from_template(template, df1, df_rules, ['AddBasic'], filename)
    writer.generate_from_template(template, df1, df_rules, ['AddWhs'], filename)
    final_path = f"{OUT_DIR}/{filename}"
    wb = load_workbook(final_path)
    assert wb['AddBasic'].cell(row=4, column=2).value == 'A'
    assert wb['AddWhs'].cell(row=4, column=2).value == 'A'
    print("[PASS] SDTWriter append mode works.")

def test_12_surgical_aliasing():
    extractor = SurgicalExtractor()
    extractor.config_dir = CONF_DIR 
    extractor.staging_dir = STAGE_DIR
    df_whs = pd.DataFrame({'MBITNO': ['9999'], 'MBWHLO': ['001']})
    whs_path = f"{DATA_DIR}/test_whs.xlsx"
    df_whs.to_excel(whs_path, index=False)
    pd.DataFrame([{'MCO_SHEET': 'Test_Whs', 'SOURCE_FILE': whs_path, 'JOIN_KEY': 'MBITNO'}]).to_csv(f"{CONF_DIR}/source_map.csv", index=False)
    pd.DataFrame([{'OBJECT_TYPE': 'WHS', 'MCO_SHEET': 'Test_Whs'}]).to_csv(f"{CONF_DIR}/surgical_def.csv", index=False)
    pd.DataFrame([{'MCO_SHEET': 'Test_Whs', 'API_NAME': 'TEST_API', 'SDT_TEMPLATE': 'TEST_API.xlsx', 'TRANSACTION_SHEET': 'AddBasic'}]).to_csv(f"{CONF_DIR}/migration_map.csv", index=False)
    tasks = extractor.perform_extraction('WHS', ['9999'])
    df_staged = pd.read_excel(tasks[0]['legacy_path'])
    assert 'MMITNO' in df_staged.columns
    assert str(df_staged.iloc[0]['MMITNO']) == '9999'
    print("[PASS] Surgical Aliasing (MB->MM) working.")

def test_13_auto_detector():
    from modules.auto_detector import AutoDetector
    mco_path = f"{DATA_DIR}/MCO_VALID.xlsx"
    legacy_path = f"{DATA_DIR}/magic_legacy.xlsx"
    pd.DataFrame({'MMITNO': ['1']}).to_excel(legacy_path, index=False)
    detector = AutoDetector(mco_path)
    detector.learn_signatures()
    prefix, sheet, api = detector.identify_file(legacy_path)
    assert prefix == 'MM'
    print("[PASS] Auto-Detector identified file.")

def test_14_scope_overrides():
    config_loader = ConfigLoader("TEST_SCOPED", rule_dir=f"{CONF_DIR}/rules")
    rules_gl, _ = config_loader.load_config(division_code="GLOBAL")
    assert not rules_gl.empty, "Global rules failed to load"
    val_gl = str(rules_gl[rules_gl['TARGET_FIELD'] == 'STAT'].iloc[0]['RULE_VALUE']).split('.')[0]
    assert val_gl == '10'
    rules_us, _ = config_loader.load_config(division_code="DIV_US")
    val_us = str(rules_us[rules_us['TARGET_FIELD'] == 'STAT'].iloc[0]['RULE_VALUE']).split('.')[0]
    assert val_us == '90'
    print("[PASS] Scope overrides working.")

def test_15_mco_checker():
    checker = MCOChecker()
    # Fix path normalization for Windows
    path = os.path.abspath(f"{DATA_DIR}/MCO_BAD.xlsx")
    issues = checker._analyze_sheet(path, "Sheet1")
    
    # If issues is just the error message "CRITICAL: Cannot read", then test setup failed.
    if issues and "Cannot read" in issues[0]:
        pytest.fail(f"MCOChecker could not read file: {path}")
        
    assert len(issues) > 0
    assert "Duplicate Rule" in issues[0]
    print("[PASS] MCO Checker found duplicates.")

def test_16_batch_processor():
    processor = BatchProcessor()
    processor.output_dir = OUT_DIR
    batch_path = f"{DATA_DIR}/test_batch.xlsx"
    df_batch = pd.DataFrame([{
        'ENABLED': 'Y', 'JOB_ID': '1', 'RULE_CONFIG': 'TEST', 'SOURCE_PATH': 'T', 'SDT_TEMPLATE': 'T', 'TRANSACTIONS': 'T', 'SCOPE': 'G', 'OUTPUT_PREFIX': 'B'
    }])
    df_batch.to_excel(batch_path, index=False)
    df_loaded = processor.load_batch_file(batch_path)
    assert not df_loaded.empty
    print("[PASS] Batch Processor loaded manifest.")

def test_17_mco_deduplication_logic():
    """Verify Importer handles duplicates by taking the LAST one (permissive)."""
    importer = MCOImporter(sdt_folder=f"{CONF_DIR}/sdt_templates")
    api_name = "MCO_DEDUPE_TEST"
    
    # MCO_DUPE has duplicate ITNO rows.
    # The code should MERGE them (deduplicate), not ABORT.
    importer.run_import_headless(f"{DATA_DIR}/MCO_DUPE.xlsx", "Sheet1", api_name, output_dir=f"{CONF_DIR}/rules")
    
    rule_path = f"{CONF_DIR}/rules/{api_name}.xlsx"
    
    # 1. File MUST exist (Change from previous test expectation)
    assert os.path.exists(rule_path), "File should exist; duplicates should be merged."
    
    df = pd.read_excel(rule_path, sheet_name='Rules')
    
    # 2. Should only be 1 row for ITNO
    rows = df[df['TARGET_FIELD'] == 'ITNO']
    assert len(rows) == 1
    
    # 3. Should be the LAST one in the file (MMITNO_BACKUP)
    assert rows.iloc[0]['SOURCE_FIELD'] == 'MMITNO_BACKUP'
    
    print("[PASS] Importer correctly deduplicated rows.")
--- END FILE: .\tests\test_core.py ---

--- START FILE: .\tests\config_temp\migration_map.csv ---
MCO_SHEET,API_NAME,SDT_TEMPLATE,TRANSACTION_SHEET
Test_Whs,TEST_API,TEST_API.xlsx,AddBasic

--- END FILE: .\tests\config_temp\migration_map.csv ---

--- START FILE: .\tests\config_temp\source_map.csv ---
MCO_SHEET,SOURCE_FILE,JOIN_KEY
Test_Whs,tests/data_temp/test_whs.xlsx,MBITNO

--- END FILE: .\tests\config_temp\source_map.csv ---

--- START FILE: .\tests\config_temp\surgical_def.csv ---
OBJECT_TYPE,MCO_SHEET
WHS,Test_Whs

--- END FILE: .\tests\config_temp\surgical_def.csv ---

